[
  {
    "source": "textbook",
    "question": "In Example 2.49, Homer maps the outcomes from Lisa’s sample space S_L = {r, g, t} into a smaller sample space S_H = {R, G}: f(r) = R, f(g) = G, and f(t) = G. Define the inverse image events as follows: f^{-1}({R}) = A1 = {r} and f^{-1}({G}) = A2 = {g, t}. Let A and B be events in Homer’s sample space. (a) Show that f^{-1}(A ∪ B) = f^{-1}(A) ∪ f^{-1}(B). (b) Show that f^{-1}(A ∩ B) = f^{-1}(A) ∩ f^{-1}(B). (c) Show that f^{-1}(A^c) = f^{-1}(A)^c. (d) Show that the results in parts a, b, and c hold for a general mapping f from a sample space S to a set S'.",
    "answer": "Lisa's sample space is S_L = {r, g, t}. Homer maps the elements of S_L to a smaller sample space S_H = {R, G} using the function f. The function f is defined as f(r) = R, f(g) = f(t) = G. It is also given that A and B are two events in S_H.\n\n(a) We prove that f^{-1}(A ∪ B) = f^{-1}(A) ∪ f^{-1}(B).\n\nf^{-1}(A ∪ B) = {x ∈ S_L : f(x) ∈ A ∪ B}\n= {x ∈ S_L : f(x) ∈ A or f(x) ∈ B}\n= {x ∈ S_L : f(x) ∈ A} ∪ {x ∈ S_L : f(x) ∈ B}\n= f^{-1}(A) ∪ f^{-1}(B).\n\nThis proves part (a).\n\n(b) We prove that f^{-1}(A ∩ B) = f^{-1}(A) ∩ f^{-1}(B).\n\nf^{-1}(A ∩ B) = {x ∈ S_L : f(x) ∈ A ∩ B}\n= {x ∈ S_L : f(x) ∈ A and f(x) ∈ B}\n= {x ∈ S_L : f(x) ∈ A} ∩ {x ∈ S_L : f(x) ∈ B}\n= f^{-1}(A) ∩ f^{-1}(B).\n\nThis completes the proof for part (b).\n\n(c) We show that f^{-1}(A^c) = f^{-1}(A)^c.\n\nf^{-1}(A^c) = {x ∈ S_L : f(x) ∈ A^c}\n= {x ∈ S_L : f(x) ∉ A}\n= {x ∈ S_L : f(x) ∈ A}^c\n= f^{-1}(A)^c.\n\nThis proves part (c).\n\n(d) The proofs in parts (a), (b), and (c) do not rely on any specific properties of the sample spaces S_L and S_H or the function f. Therefore, these results hold for any general mapping f from a sample space S to a set S'."
  },

  {
    "source": "textbook",
    "question": "Use Corollary 7 to prove the following:\n\n(a) P[A ∪ B ∪ C] ≤ P[A] + P[B] + P[C].\n\n(b) P[ ⋃_{k=1}^{n} A_k ] ≤ ∑_{k=1}^{n} P[A_k].\n\n(c) P[ ⋂_{k=1}^{n} A_k ] ≥ 1 − ∑_{k=1}^{n} P[A_k^c].\n\nThe second expression is called the union bound.",
    "answer": "(a) We want to prove that\n\nP[A ∪ B ∪ C] ≤ P[A] + P[B] + P[C].\n\nRecall the well-known identity:\n\nP[X ∪ Y] = P[X] + P[Y] − P[X ∩ Y].\n\nSince X ∩ Y ⊇ ∅, we have P[X ∩ Y] ≥ 0. Hence,\n\nP[X ∪ Y] ≤ P[X] + P[Y].\n\nNow let X = A ∪ B and Y = C. Then,\n\nP[(A ∪ B) ∪ C] ≤ P[A ∪ B] + P[C].\n\nUsing again the inequality P[A ∪ B] ≤ P[A] + P[B], we obtain\n\nP[A ∪ B ∪ C] ≤ P[A] + P[B] + P[C].\n\nThis proves part (a).\n\n(b) We prove by mathematical induction that\n\nP[⋃_{k=1}^{n} A_k] ≤ ∑_{k=1}^{n} P[A_k], for n ≥ 2.\n\nBase case n = 2:\nP[A_1 ∪ A_2] ≤ P[A_1] + P[A_2], which follows from part (a).\n\nInduction hypothesis: assume for n = m,\n\nP[⋃_{k=1}^{m} A_k] ≤ ∑_{k=1}^{m} P[A_k].\n\nFor n = m + 1:\n\nP[⋃_{k=1}^{m+1} A_k] = P[(⋃_{k=1}^{m} A_k) ∪ A_{m+1}]\n≤ P[⋃_{k=1}^{m} A_k] + P[A_{m+1}]\n≤ ∑_{k=1}^{m} P[A_k] + P[A_{m+1}]\n= ∑_{k=1}^{m+1} P[A_k].\n\nThus the inequality holds for all n ≥ 2, proving the union bound.\n\n(c) We prove that\n\nP[⋂_{k=1}^{n} A_k] ≥ 1 − ∑_{k=1}^{n} P[A_k^c].\n\nUsing De Morgan’s law:\n\n⋂_{k=1}^{n} A_k = (⋃_{k=1}^{n} A_k^c)^c.\n\nTherefore,\n\nP[⋂_{k=1}^{n} A_k] = 1 − P[⋃_{k=1}^{n} A_k^c].\n\nApplying the union bound from part (b):\n\nP[⋃_{k=1}^{n} A_k^c] ≤ ∑_{k=1}^{n} P[A_k^c].\n\nHence,\n\nP[⋂_{k=1}^{n} A_k] ≥ 1 − ∑_{k=1}^{n} P[A_k^c].\n\nThis completes the proof."
  },

  {
  "source": "textbook",
  "question": "(a) Find P[A|B] if A ∩ B = ∅; if A ⊂ B; if A ⊃ B.\n(b) Show that if P[A|B] > P[A], then P[B|A] > P[B].",
  "answer": "(a)\nWe are asked to find P(A|B) in three different cases, assuming P(B) > 0.\n\nRecall the definition of conditional probability:\nP(A|B) = P(A ∩ B) / P(B).\n\n1) Case 1: A ∩ B = ∅.\nThen A ∩ B = ∅, so P(A ∩ B) = P(∅) = 0. Hence\nP(A|B) = P(A ∩ B) / P(B) = 0 / P(B) = 0.\n\n2) Case 2: A ⊂ B.\nIf A is a subset of B, then A ∩ B = A. Therefore,\nP(A|B) = P(A ∩ B) / P(B) = P(A) / P(B).\n\n3) Case 3: A ⊃ B (equivalently B ⊂ A).\nIf B is a subset of A, then A ∩ B = B. Hence,\nP(A|B) = P(A ∩ B) / P(B) = P(B) / P(B) = 1.\n\nSo the answers are:\n- If A ∩ B = ∅, then P(A|B) = 0.\n- If A ⊂ B, then P(A|B) = P(A) / P(B).\n- If A ⊃ B, then P(A|B) = 1.\n\n(b)\nWe are given that P(A|B) > P(A), and we must show that this implies P(B|A) > P(B).\n\nUsing the definition of conditional probability,\nP(A|B) = P(A ∩ B) / P(B).\n\nThe assumption P(A|B) > P(A) gives\nP(A ∩ B) / P(B) > P(A).\nMultiplying both sides by P(B) (which is positive), we obtain\nP(A ∩ B) > P(A) P(B).\n\nNow consider P(B|A):\nP(B|A) = P(A ∩ B) / P(A).\n\nUsing the inequality P(A ∩ B) > P(A) P(B), divide both sides by P(A) (assuming P(A) > 0):\nP(A ∩ B) / P(A) > P(B).\nThat is,\nP(B|A) > P(B).\n\nThus, if P(A|B) > P(A), then indeed P(B|A) > P(B), as required."
  },

  {
  "source": "textbook",
  "question": "Show that P[A|B] satisfies the axioms of probability.\n(i) 0 ≤ P[A|B] ≤ 1.\n(ii) P[S|B] = 1.\n(iii) If A ∩ C = ∅, then P[A ∪ C | B] = P[A|B] + P[C|B].",
  "answer": "We must show that the conditional probability P(A|B) satisfies the three axioms of probability. Assume throughout that P(B) > 0.\n\nRecall the definition of conditional probability:\nP(A|B) = P(A ∩ B) / P(B).\n\n(i) 0 ≤ P(A|B) ≤ 1.\n\nSince A ∩ B ⊂ B (because every outcome in A ∩ B is also in B), we have\nP(A ∩ B) ≤ P(B).\nDividing both sides by P(B) > 0 gives\nP(A ∩ B) / P(B) ≤ 1,\nthat is,\nP(A|B) ≤ 1.\n\nOn the other hand, P(A ∩ B) ≥ 0 and P(B) > 0, so their ratio is also non-negative:\nP(A ∩ B) / P(B) ≥ 0,\nwhich implies\nP(A|B) ≥ 0.\n\nCombining both inequalities we obtain\n0 ≤ P(A|B) ≤ 1,\nso axiom (i) holds.\n\n(ii) P(S|B) = 1.\n\nLet S be the sample space. Then B ⊂ S, so\nB ∩ S = B.\nTherefore,\nP(S|B) = P(S ∩ B) / P(B) = P(B) / P(B) = 1.\n\nSo axiom (ii) is satisfied.\n\n(iii) If A ∩ C = ∅, then P(A ∪ C | B) = P(A|B) + P(C|B).\n\nAssume A ∩ C = ∅. Then\n(A ∩ C) ∩ B = ∅ ∩ B = ∅,\nso\n(A ∩ B) ∩ (C ∩ B) = ∅.\nThus the events A ∩ B and C ∩ B are disjoint, and we have\nP[(A ∩ B) ∪ (C ∩ B)] = P(A ∩ B) + P(C ∩ B).\n\nNow divide both sides by P(B) > 0:\nP[(A ∩ B) ∪ (C ∩ B)] / P(B) = P(A ∩ B) / P(B) + P(C ∩ B) / P(B).\nBut\n(A ∩ B) ∪ (C ∩ B) = (A ∪ C) ∩ B,\nso the left-hand side is\nP((A ∪ C) ∩ B) / P(B) = P(A ∪ C | B).\nThe right-hand side is\nP(A ∩ B) / P(B) + P(C ∩ B) / P(B) = P(A|B) + P(C|B).\n\nTherefore,\nP(A ∪ C | B) = P(A|B) + P(C|B)\nwhenever A ∩ C = ∅.\n\nThus P(·|B) satisfies all three axioms of probability."
  },

  {
  "source": "textbook",
  "question": "Show that P[A ∩ B ∩ C] = P[A | B ∩ C] P[B | C] P[C].",
  "answer": "We want to prove that for three events A, B, C with positive probabilities,\n\nP[A ∩ B ∩ C] = P[A | B ∩ C] P[B | C] P[C].\n\nLet E = B ∩ C. Then\nA ∩ E = A ∩ (B ∩ C) = A ∩ B ∩ C.\n\nBy the definition of conditional probability,\nP[A ∩ E] = P[A | E] P[E].   (1)\n\nOn the other hand,\nP[E] = P[B ∩ C] = P[B | C] P[C].\n\nSubstituting this into equation (1), we obtain\nP[A ∩ E] = P[A | E] P[E]\n          = P[A | E] P[B | C] P[C].\n\nSince E = B ∩ C, we can rewrite P[A | E] as P[A | B ∩ C]. Also, A ∩ E = A ∩ B ∩ C. Hence,\nP[A ∩ B ∩ C] = P[A | B ∩ C] P[B | C] P[C].\n\nThis completes the proof."
  },

  {
  "source": "textbook",
  "question": "In each lot of 100 items, two items are tested, and the lot is rejected if either of the tested items is found defective.\n(a) Find the probability that a lot with k defective items is accepted.\n(b) Suppose that when the production process malfunctions, 50 out of 100 items are defective. In order to identify when the process is malfunctioning, how many items should be tested so that the probability that one or more items are found defective is at least 99%?",
  "answer": "(a)\nA lot is accepted only if **both** of the two tested items are nondefective.\n\nIn a lot of 100 items with k defective items, there are 100 − k good (nondefective) items.\n\nNumber of ways to choose 2 nondefective items:\nC(100 − k, 2).\n\nTotal number of ways to choose any 2 items from the lot:\nC(100, 2).\n\nTherefore, the probability that the lot is accepted is\n\nP(accepted) = C(100 − k, 2) / C(100, 2)\n            = [(100 − k)(99 − k)/2] / [100·99/2]\n            = (100 − k)(99 − k) / 9900.\n\nSo the required probability is\n\nP(accepted) = (100 − k)(99 − k) / 9900.\n\n(b)\nNow suppose that when the process malfunctions, exactly 50 out of 100 items are defective. We test N items without replacement and want the probability that **at least one** defective item is found to be at least 0.99.\n\nLet E_N be the event \"at least one of the N tested items is defective\". Then\n\nP(E_N) ≥ 0.99.\n\nEquivalently,\n\nP(E_N^c) ≤ 0.01,\n\nwhere E_N^c is the event that **no** defective item is found among the N tested items.\n\nFor E_N^c to occur, all N chosen items must be nondefective. There are 50 nondefective items (since 50 of the 100 are defective).\n\nNumber of ways to choose N nondefective items:\nC(50, N).\n\nTotal number of ways to choose any N items from the 100:\nC(100, N).\n\nHence,\n\nP(E_N^c) = C(50, N) / C(100, N).\n\nWe need the smallest integer N such that\n\nC(50, N) / C(100, N) ≤ 0.01.\n\nEvaluating this expression for increasing N (e.g., by calculator or software), we obtain:\n\nC(50, 6) / C(100, 6) > 0.01,\nC(50, 7) / C(100, 7) ≤ 0.01.\n\nThus, the minimum N satisfying the requirement is\n\nN = 7.\n\nTherefore, **7 items** must be tested to ensure that the probability of finding at least one defective item is at least 99%."
  },

  {
  "source": "textbook",
  "question": "A multiple choice test has 10 questions with 3 choices each. How many ways are there to answer the test? What is the probability that two papers have the same answers?",
  "answer": "Each of the 10 questions has 3 possible choices. Therefore, the total number of different ways to answer the test is\n\n3 × 3 × ... × 3 (10 times) = 3^10.\n\nNow assume that two students answer the test completely at random. We want the probability that their answer sheets are identical.\n\nThe total number of possible answer sheets is 3^10. The total number of ways two students can independently choose answer sheets is\n\n3^10 × 3^10 = 3^20.\n\nFor the two answer sheets to match exactly, both students must choose the same one of the 3^10 possible answer patterns. Thus the number of favorable cases is 3^10.\n\nHence, the probability that the two students have exactly the same answers is\n\n3^10 / 3^20 = 1 / 3^10 = 3^{-10}.\n\nTherefore,\n- Number of ways to answer the test: 3^10.\n- Probability that two papers have the same answers: 3^{-10}."
  },

  {
  "source": "textbook",
  "question": "A student has five different t-shirts and three pairs of jeans (\"brand new,\" \"broken in,\" and \"perfect\").\n(a) How many days can the student dress without repeating the combination of jeans and t-shirt?\n(b) How many days can the student dress without repeating the combination of jeans and t-shirt and without wearing the same t-shirt on two consecutive days?",
  "answer": "(a)\nThe student has 5 different t-shirts and 3 different pairs of jeans. A daily outfit is determined by choosing one t-shirt and one pair of jeans.\n\nNumber of choices for a t-shirt: 5.\nNumber of choices for jeans: 3.\n\nThus, the number of distinct (t-shirt, jeans) combinations is\n\n5 × 3 = 15.\n\nSo the student can dress for 15 days without repeating a jeans–t-shirt combination.\n\n(b)\nNow we require that the student does not repeat a jeans–t-shirt combination and also does not wear the same t-shirt on two consecutive days.\n\nWe already know there are 15 distinct combinations in total. We only need to check whether it is possible to order these 15 combinations so that no t-shirt is used on two consecutive days.\n\nOne possible ordering is:\n(t1, p1), (t2, p1), (t3, p1), (t4, p1), (t5, p1),\n(t1, p2), (t2, p2), (t3, p2), (t4, p2), (t5, p2),\n(t1, p3), (t2, p3), (t3, p3), (t4, p3), (t5, p3).\n\nIn this sequence, every day is a different (t-shirt, jeans) combination, and no t-shirt appears on two consecutive days. Therefore all 15 combinations can be used under the additional constraint.\n\nHence, the student can dress for 15 days without repeating a jeans–t-shirt combination and without wearing the same t-shirt on two consecutive days."
  },

  {
  "source": "textbook",
  "question": "Ordering a \"deluxe\" pizza means you have four choices from 15 available toppings. How many combinations are possible if toppings can be repeated? If they cannot be repeated? Assume that the order in which the toppings are selected does not matter.",
  "answer": "(a)\nWe must choose 4 toppings from 15 available toppings, where repetition is allowed and order does not matter.\n\nLet x_i denote the number of times the i-th topping is used, for i = 1, 2, ..., 15. Then x_1, x_2, ..., x_15 are non-negative integers satisfying\n\nx_1 + x_2 + ... + x_15 = 4.\n\nThe number of non-negative integer solutions of this equation is given by the stars and bars formula:\n\nC(15 + 4 - 1, 4) = C(18, 4) = 3060.\n\nSo, when toppings can be repeated, the number of possible combinations is 3060.\n\n(b)\nIf toppings cannot be repeated, we simply choose 4 distinct toppings out of 15. The number of such combinations is\n\nC(15, 4) = 1365.\n\nTherefore,\n- If repetition is allowed: 3060 combinations.\n- If repetition is not allowed: 1365 combinations."
  },

  {
  "source": "textbook",
  "question": "A lecture room has 60 seats. In how many ways can 45 students occupy the seats in the room?",
  "answer": "Assume all 60 seats are distinguishable and the 45 students are also distinguishable.\n\nLet the students enter one by one and choose seats.\n- The first student has 60 choices.\n- The second student has 59 choices.\n- The third student has 58 choices, and so on.\n- In general, the k-th student has 61 − k choices.\n\nThus, the total number of ways for 45 students to occupy the seats is\n\n60 × 59 × 58 × ... × 17 × 16.\n\nThis product can be written in factorial form as\n\n60! / 15!.\n\nTherefore, the number of ways in which 45 students can occupy 60 seats is\n\n60! / 15!."
  },

  {
  "source": "textbook",
  "question": "A hockey team has 6 forwards, 4 defensemen, and 2 goalies. At any time, 3 forwards, 2 defensemen, and 1 goalie can be on the ice. How many combinations of players can a coach put on the ice?",
  "answer": "We must choose 3 forwards from 6, 2 defensemen from 4, and 1 goalie from 2.\n\nNumber of ways to choose the forwards:\nC(6, 3) = 20.\n\nNumber of ways to choose the defensemen:\nC(4, 2) = 6.\n\nNumber of ways to choose the goalie:\nC(2, 1) = 2.\n\nSince these choices are independent, the total number of possible on-ice lineups is\n\n20 × 6 × 2 = 240.\n\nSo, the coach can put 240 different combinations of players on the ice."
  },

  {
  "source": "textbook",
  "question": "Find the probability that in a class of 28 students exactly four were born in each of the seven days of the week.",
  "answer": "Each of the 28 students can be born on any of the 7 days of the week. Therefore, the total number of possible birth-day assignments for the class is 7^28. Now we count the number of favorable assignments in which exactly 4 students are born on each day. Treat the 28 students as distinct objects to be assigned into 7 categories (Monday to Sunday) with exactly 4 students in each category. The number of ways to permute these 28 labels into 7 groups of size 4 is given by the multinomial count 28! / (4! 4! 4! 4! 4! 4! 4!). Therefore, the desired probability is:\n\nP = [28! / (4!)^7] / 7^28."
  },

  {
  "source": "textbook",
  "question": "Show that (n choose k) = (n choose n−k).",
  "answer": "Using the definition of the binomial coefficient for integers 0 ≤ b ≤ a, we have (a choose b) = a! / (b! (a − b)!). Applying this to (n choose k), we obtain (n choose k) = n! / (k! (n − k)!). Now consider (n choose n − k). By definition we have (n choose n − k) = n! / ((n − k)! (n − (n − k))!) = n! / ((n − k)! k!). Since the expressions n! / (k! (n − k)!) and n! / ((n − k)! k!) are identical, it follows that (n choose k) = (n choose n − k)."
  },

  {
  "source": "textbook",
  "question": "Let N be a geometric random variable with S_N = {1, 2, ...}. (a) Find P[N = k | N ≤ m]. (b) Find the probability that N is odd.",
  "answer": "(a) Let N be geometric with parameter p, so P[N = n] = p(1−p)^{n−1}. First compute P[N ≤ m] = sum_{n=1}^m p(1−p)^{n−1} = 1 − (1−p)^m. Then, for k > m, P[N = k | N ≤ m] = 0. For k ≤ m, P[N = k | N ≤ m] = P[N = k] / P[N ≤ m] = p(1−p)^{k−1} / (1 − (1−p)^m). Thus, P[N = k | N ≤ m] = { p(1−p)^{k−1} / (1 − (1−p)^m) if k ≤ m; 0 if k > m }.  \n\n(b) To find P[N is odd], write the event {N odd} = {1, 3, 5, ...}. Then P[N is odd] = sum_{n=1}^∞ p(1−p)^{2n−2} = p × sum_{n=1}^∞ ((1−p)^2)^{n−1} = p × 1 / (1 − (1−p)^2). Simplifying, 1 − (1−p)^2 = p(2−p), so P[N is odd] = p / (p(2−p)) = 1 / (2−p)."
  },

  {
  "source": "textbook",
  "question": "Let M be a geometric random variable. Show that M satisfies the memoryless property: P[M ≥ k + j | M ≥ j + 1] = P[M ≥ k] for all j, k > 1.",
  "answer": "Let M be a geometric random variable with parameter p. First compute P[M > k] for k > 1: P[M > k] = 1 − P[M ≤ k − 1] = 1 − ∑_{n=1}^{k−1} p(1 − p)^{n−1} = 1 − p ∑_{n=1}^{k−1}(1 − p)^{n−1}. The sum is a geometric series: ∑_{n=1}^{k−1}(1 − p)^{n−1} = (1 − (1 − p)^{k−1})/p. Thus P[M > k] = (1 − p)^k, and therefore P[M ≥ t] = P[M > t − 1] = (1 − p)^{t−1}. Now evaluate the conditional probability for j, k > 1: P[M ≥ k + j | M ≥ j + 1] = P[M ≥ k + j]/P[M ≥ j + 1] = (1 − p)^{k + j − 1}/(1 − p)^j = (1 − p)^{k − 1}. Since (1 − p)^{k − 1} = P[M ≥ k], it follows that P[M ≥ k + j | M ≥ j + 1] = P[M ≥ k], proving the memoryless property."
  },

  {
  "source": "textbook",
  "question": "Find the cdf of the Cauchy random variable which has pdf f_X(x) = (α/π)/(x^2 + α^2), −∞ < x < ∞, where α > 0.",
  "answer": "We are given a Cauchy random variable X with pdf f_X(x) = (α/π)/(x^2 + α^2), −∞ < x < ∞, α > 0. To find the cdf F_X(x) = P(X ≤ x), we integrate the pdf from −∞ to x: F_X(x) = ∫_{−∞}^x (α/π)·1/(t^2 + α^2) dt. Rewrite the integrand: (α/π)·1/(t^2 + α^2) = (1/π)·1/[α(1 + (t/α)^2)]. Substitute u = t/α, du = dt/α: F_X(x) = (1/π) ∫_{−∞}^{x/α} 1/(1 + u^2) du = (1/π)[arctan(u)]_{−∞}^{x/α} = (1/π)[arctan(x/α) + π/2]. Therefore, the cdf is F_X(x) = (1/π)(arctan(x/α) + π/2), −∞ < x < ∞."
  },

  {
  "source": "textbook",
  "question": "A random variable X has pdf f_X(x)=c(1−x^2) for −1≤x≤1 and 0 elsewhere. (a) Find c and plot the pdf. (b) Plot the cdf of X. (c) Find P[X=0], P(0<X<0.5), and P(|X−0.5|<0.25).",
  "answer": "(a) Normalize the pdf: ∫_{−1}^{1} c(1−x^2) dx = 1. Compute the integral: ∫_{−1}^{1} (1−x^2) dx = [x − x^3/3]_{−1}^{1} = 4/3. Thus c·(4/3)=1, so c=3/4. Therefore f_X(x)=(3/4)(1−x^2) for −1≤x≤1.\n(b) The cdf is F_X(x)=0 for x<−1. For −1≤x<1: F_X(x)=∫_{−1}^{x} (3/4)(1−t^2) dt = (3/4)[t − t^3/3]_{−1}^{x} = (3/4)[(x+1) − (x^3+1)/3]. For x≥1: F_X(x)=1.\n(c) P[X=0]=0 since X is continuous. P(0<X<0.5)=∫_{0}^{0.5} (3/4)(1−x^2) dx = 11/32. P(|X−0.5|<0.25)=P(0.25<X<0.75)=∫_{0.25}^{0.75} (3/4)(1−x^2) dx = 35/128."
  },

  {
  "source": "textbook",
  "question": "A random variable X has pdf f_X(x) = c x (1 - x^2) for 0 ≤ x ≤ 1 and 0 elsewhere. (a) Find c and plot the pdf. (b) Plot the cdf of X. (c) Find P[0 < X < 0.5], P[X = 1], and P[0.25 < X < 0.5].",
  "answer": {
    "a": "To determine c, use ∫_0^1 c x (1 - x^2) dx = 1. Compute the integral: ∫_0^1 (x - x^3) dx = [x^2/2 - x^4/4]_0^1 = 1/2 - 1/4 = 1/4. Thus c · (1/4) = 1, giving c = 4. The pdf is f_X(x) = 4x(1 - x^2) for 0 ≤ x ≤ 1.",
    "b": "For x < 0, F_X(x) = 0. For 0 ≤ x < 1: F_X(x) = ∫_0^x 4(t - t^3) dt = 4[t^2/2 - t^4/4]_0^x = 2x^2 - x^4. For x ≥ 1, F_X(x) = 1.",
    "c": {
      "P(0 < X < 0.5)": "∫_0^0.5 4(x - x^3) dx = 4[ x^2/2 - x^4/4 ]_0^0.5 = 7/16.",
      "P(X = 1)": "0 because X is continuous.",
      "P(0.25 < X < 0.5)": "∫_0.25^0.5 4(x - x^3) dx = 4[ x^2/2 - x^4/4 ]_0.25^0.5 ≈ 0.3164."
      }
    }
  },

  {
  "source": "textbook",
  "question": "Carlos and Michael each flip a fair coin twice. Let X be the difference in the number of heads obtained in the two pairs of tosses (Carlos − Michael) and let Y be the sum of the number of heads obtained. (a) Describe the underlying space S of this random experiment and show the mapping from S to S_XY, the range of the pair (X,Y). (b) Find the probabilities for all values of (X,Y). (c) Find P[X + Y = 1] and P[X + Y = 2].",
  "answer": "(a) For each person two coin flips are made, so each has sample space {HH,HT,TH,TT}. The joint sample space is\nS = {(HH,HH),(HH,HT),(HH,TH),(HH,TT),(HT,HH),(HT,HT),(HT,TH),(HT,TT),(TH,HH),(TH,HT),(TH,TH),(TH,TT),(TT,HH),(TT,HT),(TT,TH),(TT,TT)}.\nLet H(·) denote the number of heads in the two flips. For outcome (u,v), Carlos has H(u) heads and Michael has H(v) heads, so\nX = H(u) − H(v),   Y = H(u) + H(v).\nPossible head-counts for each person are 0,1,2, so the pairs (H(u),H(v)) range over {(0,0),(0,1),(0,2),(1,0),(1,1),(1,2),(2,0),(2,1),(2,2)}. Mapping these to (X,Y):\n(0,0) → (0,0)\n(0,1) → (−1,1)\n(0,2) → (−2,2)\n(1,0) → (1,1)\n(1,1) → (0,2)\n(1,2) → (−1,3)\n(2,0) → (2,2)\n(2,1) → (1,3)\n(2,2) → (0,4).\nThus S_XY = {(0,0),(−1,1),(−2,2),(1,1),(0,2),(−1,3),(2,2),(1,3),(0,4)}.\n\n(b) Each of the 16 outcomes in S is equally likely with probability 1/16. Count how many outcomes produce each (H(u),H(v)) pair:\nP(H(u)=0) = 1/4, P(H(u)=1) = 1/2, P(H(u)=2) = 1/4, and similarly for H(v). Using independence,\nP(H(u)=a,H(v)=b) = P(H(u)=a)P(H(v)=b).\nSo we get:\n(0,0): P = (1/4)(1/4) = 1/16 → (X,Y) = (0,0)\n(0,1): P = (1/4)(1/2) = 1/8 → (−1,1)\n(0,2): P = (1/4)(1/4) = 1/16 → (−2,2)\n(1,0): P = (1/2)(1/4) = 1/8 → (1,1)\n(1,1): P = (1/2)(1/2) = 1/4 → (0,2)\n(1,2): P = (1/2)(1/4) = 1/8 → (−1,3)\n(2,0): P = (1/4)(1/4) = 1/16 → (2,2)\n(2,1): P = (1/4)(1/2) = 1/8 → (1,3)\n(2,2): P = (1/4)(1/4) = 1/16 → (0,4).\nTherefore the joint pmf of (X,Y) is\nP(0,0) = 1/16,\nP(−1,1) = 1/8,\nP(−2,2) = 1/16,\nP(1,1) = 1/8,\nP(0,2) = 1/4,\nP(−1,3) = 1/8,\nP(2,2) = 1/16,\nP(1,3) = 1/8,\nP(0,4) = 1/16,\nand 0 for all other pairs.\n\n(c) First, X + Y = 1. For each (H(u),H(v)) = (a,b), X + Y = (a − b) + (a + b) = 2a. So X + Y = 1 would require 2a = 1, which is impossible since a is integer. Hence P[X + Y = 1] = 0.\nNext, X + Y = 2 means 2a = 2, so a = 1 (Carlos gets exactly one head). That corresponds to (H(u),H(v)) = (1,0),(1,1),(1,2). Thus\nP[X + Y = 2] = P(H(u)=1,H(v)=0) + P(H(u)=1,H(v)=1) + P(H(u)=1,H(v)=2)\n= 1/8 + 1/4 + 1/8 = 1/2."
  },

  {
  "source": "textbook",
  "question": "Is the following a valid cdf? Why? F_XY(x, y) = (1 - 1/(x^2 y^2)) for x > 1, y > 1; 0 elsewhere.",
  "answer": "We have a joint cdf F_XY(x, y) = 1 - 1/(x^2 y^2) for x > 1, y > 1, and 0 elsewhere. To check whether it is a valid cdf, consider the marginal cdf of Y: F_Y(y) = P(Y ≤ y) = F_XY(∞, y) = lim_{x→∞} F_XY(x, y) = 1. Thus F_Y(y) = 1 for all y. A valid cdf must satisfy lim_{y→−∞} F_Y(y) = 0, which is not true here. Therefore F_XY is not a valid cdf."
  },

  {
  "source": "textbook",
  "question": "Let F_X(x) and F_Y(y) be valid one-dimensional cdf’s. Show that F_XY(x, y) = F_X(x)F_Y(y) satisfies the properties of a two-dimensional cdf.",
  "answer": "We are given one-dimensional cdfs F_X(x) and F_Y(y), and a joint cdf candidate F_XY(x, y) = F_X(x)F_Y(y). We verify that F_XY satisfies all required properties of a valid 2D cdf. (1) Monotonicity: If x1 ≤ x2 and y1 ≤ y2, then F_X(x1) ≤ F_X(x2) and F_Y(y1) ≤ F_Y(y2). Multiplying yields F_X(x1)F_Y(y1) ≤ F_X(x2)F_Y(y2), so F_XY(x1, y1) ≤ F_XY(x2, y2). (2) Limits at boundaries: F_XY(−∞, y) = F_X(−∞)F_Y(y) = 0, F_XY(x, −∞) = F_X(x)F_Y(−∞) = 0, and F_XY(∞, ∞) = F_X(∞)F_Y(∞) = 1. (3) Correct marginals: F_XY(x, ∞) = F_X(x)F_Y(∞) = F_X(x), and F_XY(∞, y) = F_X(∞)F_Y(y) = F_Y(y). (4) Right continuity: Since F_X and F_Y are right-continuous, lim_{x→a+} F_XY(x, y) = lim_{x→a+} F_X(x)F_Y(y) = F_X(a)F_Y(y) = F_XY(a, y). Similarly for y. (5) Rectangle probability: P(x1 < X ≤ x2, y1 < Y ≤ y2) = [F_X(x2) − F_X(x1)] · [F_Y(y2) − F_Y(y1)] = F_XY(x2, y2) − F_XY(x2, y1) − F_XY(x1, y2) + F_XY(x1, y1). All five properties are satisfied, so F_XY is a valid 2D cdf."
  },

  {
    "source": "textbook",
    "question": "Let X = θ + N be the output of a noisy channel where the input is the parameter θ and N is a zero-mean, unit-variance Gaussian random variable. Suppose that the output is measured n times to obtain the random sample Xi = θ + Ni for i = 1,…,n. (a) Find the maximum likelihood estimator θ̂ML for θ. (b) Find the pdf of θ̂ML. (c) Determine whether θ̂ML is unbiased and consistent.",
    "answer": "(a) The noises Ni are iid N(0,1). Their joint pdf is\nf(N1,…,Nn) = ∏*{i=1}^n (1/√(2π)) exp(−Ni^2/2).\nSince Ni = Xi − θ, the likelihood of θ given the data x1,…,xn is\nL(θ) = f(x1,…,xn | θ) = ∏*{i=1}^n (1/√(2π)) exp(−(xi − θ)^2/2).\nThe log-likelihood is\nℓ(θ) = ln L(θ) = −(n/2) ln(2π) − (1/2)∑*{i=1}^n (xi − θ)^2.\nDifferentiate and set to zero:\n∂ℓ(θ)/∂θ = −∑*{i=1}^n (xi − θ) = 0 ⇒ nθ̂ML = ∑*{i=1}^n xi ⇒ θ̂ML = x̄,\nwhere x̄ is the sample mean.\n\n(b) Each Xi = θ + Ni with Ni ~ N(0,1), so Xi ~ N(θ,1) and the Xi are iid. Therefore\nθ̂ML = x̄ = (1/n)∑*{i=1}^n Xi ~ N(θ,1/n).\nHence the pdf of θ̂ML is\nf_{θ̂ML}(t) = √(n/(2π)) exp(−(n/2)(t − θ)^2),  −∞ < t < ∞.\n\n(c) From (b), E[θ̂ML] = θ and Var(θ̂ML) = 1/n → 0 as n → ∞. Thus θ̂ML is an unbiased estimator of θ and is consistent."
  },

  {
    "source": "textbook",
    "question": "Let X1 ~ U(0,1). Conditional on X1 = x1, let X2 ~ U(0,x1); and conditional on X2 = x2, let X3 ~ U(0,x2). (a) Find the marginal pdfs of X2 and X3. (b) Find the conditional pdf of X3 given X1. (c) Find the conditional pdf of X1 given X3 = z.",
    "answer": "(a) First find the marginal pdf of X2 by integrating out X1 from the joint pdf:\n\nfX1(x1) = 1 for 0 < x1 < 1.\nGiven X1 = x1, fX2|X1=x1(x2) = 1/x1 for 0 < x2 < x1.\nThus\nfX2(x2) = ∫0^1 fX2|X1=x1(x2) fX1(x1) dx1\n        = ∫x2^1 (1/x1) dx1\n        = [log x1]x2^1\n        = −log(x2),   0 < x2 < 1.\n\nNext find the marginal pdf of X3. Given X2 = x2, fX3|X2=x2(x3) = 1/x2 for 0 < x3 < x2. Hence\nfX3(x3) = ∫0^1 fX3|X2=x2(x3) fX2(x2) dx2\n        = ∫x3^1 (1/x2)(−log x2) dx2\n        = −∫x3^1 (log x2)/x2 dx2.\nLet z = log x2, dz = dx2/x2. Then\nfX3(x3) = −∫log x3^0 z dz\n        = ∫0^{log x3} z dz\n        = (log^2 x3)/2,   0 < x3 < 1.\n\n(b) First write the full joint pdf:\n\nfX1,X2,X3(x1,x2,x3) = fX1(x1) fX2|X1=x1(x2) fX3|X2=x2(x3)\n                    = 1 · (1/x1) · (1/x2) I(0 < x3 < x2 < x1 < 1)\n                    = I(0 < x3 < x2 < x1 < 1)/(x1 x2).\n\nIntegrate out x2 to get the joint pdf of (X1,X3):\n\nfX1,X3(x1,x3) = ∫0^1 fX1,X2,X3(x1,x2,x3) dx2\n              = ∫x3^{x1} (1/(x1 x2)) dx2\n              = (1/x1)[log x2]x3^{x1}\n              = (log x1 − log x3)/x1,   0 < x3 < x1 < 1.\n\nSince fX1(x1) = 1 on (0,1), the conditional pdf of X3 given X1 = x1 is\n\nfX3|X1=x1(x3) = fX1,X3(x1,x3)/fX1(x1)\n              = (log x1 − log x3)/x1,   0 < x3 < x1 < 1.\n\n(c) To find the conditional pdf of X1 given X3 = z, use Bayes’ rule:\n\nfX1|X3=z(x1) = fX1,X3(x1,z)/fX3(z)\n            = [(log x1 − log z)/x1] / [(log^2 z)/2]\n            = 2(log x1 − log z)/(x1 log^2 z),   z < x1 < 1.\n\nThus the results are:\n\nfX2(x2) = −log(x2), 0 < x2 < 1;\nfX3(x3) = (log^2 x3)/2, 0 < x3 < 1;\nfX3|X1=x1(x3) = (log x1 − log x3)/x1, 0 < x3 < x1 < 1;\nfX1|X3=z(x1) = 2(log x1 − log z)/(x1 log^2 z), z < x1 < 1."
  },

  {
    "source":"textbook",
    "question":"The amplitudes of two signals X and Y have joint pdf f_{X,Y}(x,y)=e^{-x/2} y e^{-y^2} for x>0,y>0. (a) Find the joint cdf. (b) Find P[X^{1/2}>Y]. (c) Find the marginal pdfs.",
    "answer":"(a) The joint cdf is F_{X,Y}(x,y)=P(X≤x,Y≤y)=∫_0^x∫_0^y e^{-u/2} v e^{-v^2} dv du for x>0,y>0. First compute the inner integral: ∫_0^y v e^{-v^2} dv=−(1/2)e^{-v^2}|_0^y=(1/2)(1−e^{-y^2}). Then the outer integral: ∫*0^x e^{-u/2} du=−2e^{-u/2}|*0^x=2(1−e^{-x/2}). Multiplying gives F*{X,Y}(x,y)=(1−e^{-x/2})(1−e^{-y^2}) for x>0,y>0, and F*{X,Y}(x,y)=0 if x≤0 or y≤0.\n(b) We want P[X^{1/2}>Y]=P(Y<√X)=∫_0^∞∫_0^{√x} e^{-x/2} y e^{-y^2} dy dx. The inner integral is ∫_0^{√x} y e^{-y^2} dy=−(1/2)e^{-y^2}|_0^{√x}=(1/2)(1−e^{-x}). Thus P[X^{1/2}>Y]=(1/2)∫_0^∞ e^{-x/2}(1−e^{-x}) dx=(1/2)(∫_0^∞ e^{-x/2} dx−∫_0^∞ e^{-3x/2} dx)=(1/2)(2−2/3)=2/3.\n(c) The marginal pdf of X is f_X(x)=∫_0^∞ e^{-x/2} y e^{-y^2} dy=e^{-x/2}∫_0^∞ y e^{-y^2} dy=e^{-x/2}·(1/2)=(1/2)e^{-x/2} for x>0, and 0 otherwise. The marginal pdf of Y is f_Y(y)=∫_0^∞ e^{-x/2} y e^{-y^2} dx=y e^{-y^2}∫_0^∞ e^{-x/2} dx=y e^{-y^2}·2=2y e^{-y^2} for y>0, and 0 otherwise."
  },

  {
    "source":"textbook",
    "question":"Show that the maximum likelihood estimator for a uniform random variable that is distributed in the interval [0,θ] is θ̂ = max{X1,X2,…,Xn}.",
    "answer":"Let X1,…,Xn be i.i.d. samples from U[0,θ]. The joint pdf is\nf(x1,…,xn|θ)=∏_{i=1}^n(1/θ)I(0≤xi≤θ)=(1/θ^n)I(0≤x1,…,xn≤θ),\nwhere I(·) is the indicator function. Write the joint pdf in terms of the order statistics x(1)≤…≤x(n):\nL(θ)=f(x1,…,xn|θ)=(1/θ^n)I(x(n)≤θ)I(0≤x(1)).\nThus, as a function of θ,\nL(θ)=0, if θ<x(n),\nL(θ)=(1/θ^n)I(0≤x(1)), if θ≥x(n).\nFor θ≥x(n), L(θ)=(1/θ^n) is decreasing in θ, so the likelihood is maximized at the smallest admissible θ, namely θ=x(n). Therefore, the maximum likelihood estimator is\nθ̂ML=x(n)=max(X1,…,Xn)."
  },

  {
  "source": "textbook",
  "question": "A toddler pulls three volumes of an encyclopedia from a bookshelf and, after being scolded, places them back in random order. What is the probability that the books are in the correct order?",
  "answer": "There are 3! = 6 possible permutations of the three books, all equally likely. Only one of these permutations corresponds to the correct ordering. Therefore, the probability that the books are placed in the correct order is 1/6."
  },

  {
  "source": "textbook",
  "question": "A lot of 100 items contains k defective items. M items are chosen at random and tested. (a) What is the probability that m are found defective? (b) A lot is accepted if 1 or fewer of the M items are defective. What is the probability that the lot is accepted?",
  "answer": {
    "a": "P(m defective among M) = C(k,m) C(100-k, M-m) / C(100,M)",
    "b": "P(accepted) = [C(k,0) C(100-k,M)] / C(100,M) + [C(k,1) C(100-k, M-1)] / C(100,M)"
  }
  },

  {
    "source":"textbook",
    "question":"A park has N raccoons of which eight were previously captured and tagged. Suppose that 20 raccoons are captured. Find the probability that four of these are found to be tagged. Denote this probability, which depends on N, by p(N). Find the value of N that maximizes this probability.",
    "answer":"To get exactly 4 tagged raccoons in a sample of 20, we choose 4 from the 8 tagged and 16 from the N−8 untagged. Thus the probability is hypergeometric: p(N)=C(8,4)C(N−8,16)/C(N,20)=70·C(N−8,16)/C(N,20). To maximize p(N) over integer N, compare the ratio p(N)/p(N−1): p(N)/p(N−1)=[C(N−8,16)/C(N,20)]·[C(N−1,20)/C(N−9,16)]=[(N−8)(N−20)]/[N(N−24)]. Requiring p(N)/p(N−1)≥1 gives (N−8)(N−20)≥N(N−24), i.e. N²−28N+160≥N²−24N, so −4N+160≥0, hence N≤40. Therefore p(N) increases up to N=40 and decreases afterwards, so the maximizing value is N=40."
},

  {
    "source":"textbook",
    "question":"A lot of 50 items has 40 good items and 10 bad items. (a) Suppose we test five samples from the lot, with replacement. Let X be the number of defective items in the sample. Find P[X = k]. (b) Suppose we test five samples from the lot, without replacement. Let Y be the number of defective items in the sample. Find P[Y = k].",
    "answer":"(a) With replacement, each draw is independent with P(bad) = 10/50 = 0.2 and P(good) = 40/50 = 0.8. For k = 0,1,2,3,4,5 the number X of bad items in 5 draws is binomial: P[X = k] = C(5,k)(0.2)^k(0.8)^(5−k). (b) Without replacement, we use the hypergeometric distribution. To get exactly k bad items in the 5 draws, choose k from the 10 bad and 5−k from the 40 good. The total number of ways to choose any 5 items from 50 is C(50,5). Thus, for k = 0,1,2,3,4,5 we have P[Y = k] = [C(10,k) C(40,5−k)] / C(50,5)."
},

  {
    "source":"textbook",
    "question":"How many distinct permutations are there of four red balls, two white balls, and three black balls?",
    "answer":"We have 4 red, 2 white, and 3 black balls, for a total of 4 + 2 + 3 = 9 balls. If all 9 balls were distinct, there would be 9! permutations. But balls of the same color are indistinguishable, so we divide by the factorial of each color count: 4! for the red balls, 2! for the white balls, and 3! for the black balls. Thus the number of distinct permutations is 9! / (4! 3! 2!) = 1260."
},

  {
    "source":"textbook",
    "question":"A fair die is tossed twice. Let the outcome be (X1,X2), where Xi is the number of dots on toss i. Define event A = {X1 ≥ X2} (the number of dots in the first toss is not less than in the second toss) and event B = {X1 = 6}. Find the conditional probabilities P(A|B) and P(B|A).",
    "answer":"There are 36 equally likely ordered outcomes (X1,X2). Event A = {(1,1),(2,1),(2,2),(3,1),(3,2),(3,3),(4,1),(4,2),(4,3),(4,4),(5,1),(5,2),(5,3),(5,4),(5,5),(6,1),(6,2),(6,3),(6,4),(6,5),(6,6)} has 21 outcomes, so P(A) = 21/36 = 7/12. Event B = {(6,1),(6,2),(6,3),(6,4),(6,5),(6,6)} has 6 outcomes, so P(B) = 6/36 = 1/6. The intersection A∩B is the same 6 outcomes as B, so P(A∩B) = 6/36 = 1/6. Hence P(A|B) = P(A∩B)/P(B) = (1/6)/(1/6) = 1, and P(B|A) = P(A∩B)/P(A) = (1/6)/(7/12) = 2/7."
},

{
    "source":"textbook",
    "question":"A number x is selected at random in the interval [-1,2]. Let the events A={x<0}, B={|x-0.5|<0.5}, and C={x>0.75}. Find P[A|B], P[B|C], P[A|C^c], P[B|C^c].",
    "answer":"The sample space is the interval [-1,2] of length 3. Event B is (0,1), so P(B)=1/3. Event C is (0.75,2], so P(C)=5/12 and P(C^c)=7/12. A∩B=∅, so P(A∩B)=0 and P(A|B)=0. B∩C=(0.75,1), length 0.25, so P(B∩C)=1/12 and P(B|C)=(1/12)/(5/12)=1/5. Event A=[-1,0), so A⊂C^c and P(A∩C^c)=P(A)=1/3, giving P(A|C^c)=(1/3)/(7/12)=4/7. Finally, B∩C^c=(0,0.75), length 0.75, hence P(B∩C^c)=1/4 and P(B|C^c)=(1/4)/(7/12)=3/7."
},

{
    "source":"textbook",
    "question":"Consider the integer function f: Z → Z such that f(n) = n^2 − 4 for any integer n ∈ Z. Define the range subsets A = {−4, −3, 5, 12} ⊂ Z and B = {−3, 12} ⊂ Z. Define the pullback or inverse image set f^{-1}(S) as the set of pre-images z ∈ Z of S under the mapping f: f^{-1}(S) = {z ∈ Z : f(z) ∈ S}. (a) Find f^{-1}(A), f^{-1}(B), f^{-1}(A ∪ B), and f^{-1}(A ∩ B). (b) Verify the commutations f^{-1}(A ∪ B) = f^{-1}(A) ∪ f^{-1}(B) and f^{-1}(A ∩ B) = f^{-1}(A) ∩ f^{-1}(B). (c) Is it also true that f(A ∪ B) = f(A) ∪ f(B) and f(A ∩ B) = f(A) ∩ f(B)?",
    "answer":"Solve n^2 − 4 = y for each y appearing in A or B. For y = −4 we need n^2 = 0 so n = 0. For y = −3 we need n^2 = 1 so n = ±1. For y = 5 we need n^2 = 9 so n = ±3. For y = 12 we need n^2 = 16 so n = ±4. Therefore f^{-1}(A) is the set of all integers whose image lies in {−4, −3, 5, 12}, i.e. f^{-1}(A) = {0, −1, 1, −3, 3, −4, 4} = {−4, −3, −1, 0, 1, 3, 4}. For B = {−3, 12} the pre-images are ±1 and ±4, so f^{-1}(B) = {−4, −1, 1, 4}. Since B ⊂ A we have A ∪ B = A, so f^{-1}(A ∪ B) = f^{-1}(A) = {−4, −3, −1, 0, 1, 3, 4}. Also A ∩ B = {−3, 12}, whose pre-images are again ±1 and ±4, hence f^{-1}(A ∩ B) = {−4, −1, 1, 4}. This gives part (a). For part (b), compute unions and intersections of the inverse images: f^{-1}(A) ∪ f^{-1}(B) = {−4, −3, −1, 0, 1, 3, 4} ∪ {−4, −1, 1, 4} = {−4, −3, −1, 0, 1, 3, 4} which equals f^{-1}(A ∪ B). Their intersection is f^{-1}(A) ∩ f^{-1}(B) = {−4, −1, 1, 4}, which equals f^{-1}(A ∩ B). Thus the commutations f^{-1}(A ∪ B) = f^{-1}(A) ∪ f^{-1}(B) and f^{-1}(A ∩ B) = f^{-1}(A) ∩ f^{-1}(B) hold. For part (c), compute direct images. For each a ∈ A, f(a) is: f(−4) = 12, f(−3) = 5, f(5) = 21, f(12) = 140, so f(A) = {5, 12, 21, 140}. For B we have f(−3) = 5 and f(12) = 140, so f(B) = {5, 140}. Since A ∪ B = A, f(A ∪ B) = f(A) = {5, 12, 21, 140} and f(A) ∪ f(B) = {5, 12, 21, 140} ∪ {5, 140} = {5, 12, 21, 140}, so the equality f(A ∪ B) = f(A) ∪ f(B) holds for these sets. Next A ∩ B = {−3, 12}, hence f(A ∩ B) = {5, 140}. On the other hand f(A) ∩ f(B) = {5, 12, 21, 140} ∩ {5, 140} = {5, 140}. Thus f(A ∩ B) = f(A) ∩ f(B) also holds for these particular A and B, even though in general direct images always preserve unions but need not preserve intersections for arbitrary functions and subsets."
},

{
    "source":"textbook",
    "question":"Prove or disprove the following statements.\n(a) A − (A − B) = A ∩ B.\n(b) A ⊂ B if and only if A ∪ B = B.",
    "answer":"(a) We prove set equality by double inclusion.\n(⊆) Let x ∈ A − (A − B). Then x ∈ A and x ∉ (A − B). By definition, A − B = {y : y ∈ A and y ∉ B}. Thus x ∉ (A − B) means ¬(x ∈ A and x ∉ B), which by De Morgan’s law is (x ∉ A or x ∈ B). Since we already know x ∈ A, the disjunction (x ∉ A or x ∈ B) forces x ∈ B. Hence x ∈ A and x ∈ B, so x ∈ A ∩ B.\n(⊇) Let x ∈ A ∩ B. Then x ∈ A and x ∈ B. If x were in A − B, we would have x ∈ A and x ∉ B, contradicting x ∈ B. Hence x ∉ A − B, so x ∈ A and x ∉ (A − B); therefore x ∈ A − (A − B). Thus A − (A − B) = A ∩ B.\n(b) (⇒) Assume A ⊂ B (subset). Take any x ∈ A ∪ B. Then x ∈ A or x ∈ B. If x ∈ A, since A ⊂ B we have x ∈ B. Thus every element of A ∪ B lies in B, so A ∪ B ⊆ B. Conversely, any x ∈ B is clearly in A ∪ B, so B ⊆ A ∪ B. Therefore A ∪ B = B.\n(⇐) Assume A ∪ B = B. Let x ∈ A. Then x ∈ A ⊆ A ∪ B = B, so x ∈ B. Thus every element of A lies in B, hence A ⊂ B. Therefore A ⊂ B if and only if A ∪ B = B."
},

{
    "source":"homework",
    "question":"For all functions f and all sets A and B prove or disprove the following statements.\n(a) f^{-1}(A) ⊂ f^{-1}(B) if A ⊂ B.\n(b) f(A) ∩ f(B) = f(A ∩ B).\n(c) f^{-1}(A − B) = f^{-1}(A) − f^{-1}(B).",
    "answer":"(a) True. Proof: Assume A ⊂ B. Let x ∈ f^{-1}(A). By definition of inverse image, f(x) ∈ A. Since A ⊂ B, we have f(x) ∈ B. Hence x ∈ f^{-1}(B). Thus every element of f^{-1}(A) is in f^{-1}(B), so f^{-1}(A) ⊂ f^{-1}(B).\n(b) False in general. We always have f(A ∩ B) ⊂ f(A) ∩ f(B): If y ∈ f(A ∩ B), then y = f(x) for some x ∈ A ∩ B, so x ∈ A and x ∈ B, and thus y ∈ f(A) and y ∈ f(B), hence y ∈ f(A) ∩ f(B). However, equality can fail. Counterexample: Let f : {1,2} → {a} be defined by f(1) = a and f(2) = a. Take A = {1}, B = {2}. Then f(A) = {a}, f(B) = {a}, so f(A) ∩ f(B) = {a}. But A ∩ B = ∅, so f(A ∩ B) = f(∅) = ∅. Thus f(A) ∩ f(B) ≠ f(A ∩ B).\n(c) True. Proof: By element-wise equivalence. Let x be arbitrary.\nx ∈ f^{-1}(A − B)\n⇔ f(x) ∈ A − B\n⇔ f(x) ∈ A and f(x) ∉ B\n⇔ x ∈ f^{-1}(A) and x ∉ f^{-1}(B)\n⇔ x ∈ f^{-1}(A) − f^{-1}(B).\nHence f^{-1}(A − B) = f^{-1}(A) − f^{-1}(B)."
},

{
    "source":"homework",
    "question":"Problem 4. Let X = {a, b, c}. Find the power set 2^X. How many possible set collections A ⊆ 2^X are there? How many sigma-algebras can we define on X? Produce all of them. Produce a set collection that is not a sigma-algebra and then show how to minimally augment it to make it a sigma-algebra.",
    "answer":"1) Power set.\nX = {a,b,c}. Every subset of X is in 2^X, so\n2^X = {∅, {a}, {b}, {c}, {a,b}, {a,c}, {b,c}, {a,b,c}}.\nThere are 2^3 = 8 subsets.\n\n2) Number of possible set collections A ⊆ 2^X.\nA set collection A is just any subset of the power set 2^X. Since |2^X| = 8, there are 2^8 = 256 possible collections A ⊆ 2^X.\n\n3) Sigma-algebras on X.\nOn a finite set, every sigma-algebra corresponds to a partition of X; the sigma-algebra consists of all unions of sets in the partition. The set {a,b,c} has Bell number B_3 = 5 partitions, so there are 5 sigma-algebras. We list each partition and its sigma-algebra:\n\n(1) Partition {{a,b,c}}:\nΣ₁ = {∅, {a,b,c}}.\n\n(2) Partition {{a}, {b,c}}:\nΣ₂ = {∅, {a}, {b,c}, {a,b,c}}.\n\n(3) Partition {{b}, {a,c}}:\nΣ₃ = {∅, {b}, {a,c}, {a,b,c}}.\n\n(4) Partition {{c}, {a,b}}:\nΣ₄ = {∅, {c}, {a,b}, {a,b,c}}.\n\n(5) Partition {{a}, {b}, {c}}:\nΣ₅ = 2^X = {∅, {a}, {b}, {c}, {a,b}, {a,c}, {b,c}, {a,b,c}}.\n\nThus there are exactly 5 sigma-algebras on X.\n\n4) Example of a collection that is not a sigma-algebra and its minimal augmentation.\nConsider the collection\nC = {∅, {a}, {a,b,c}}.\nThis is not a sigma-algebra because it is not closed under complements in X: the complement of {a} in X is {b,c}, but {b,c} ∉ C.\n\nTo minimally augment C into a sigma-algebra, we must at least add the missing complement {b,c}. The smallest sigma-algebra containing C is therefore\nΣ = {∅, {a}, {b,c}, {a,b,c}}.\nWe check: it contains X, is closed under complements (∅ ↔ X, {a} ↔ {b,c}), and closed under finite/ countable unions, so Σ is a sigma-algebra. This is the minimal augmentation of C (any sigma-algebra containing C must contain {b,c}, and Σ is exactly the sigma-algebra from partition {{a},{b,c}})."
},

{
"source": "homework",
"question": "Problem 5. Let X = {w, x, y, z}. Find the power set 2^X. How many possible set collections A ⊆ 2^X are there? Produce four set collections A ⊂ 2^X that are not sigma-algebras. Show how to minimally augment these four set collections so that they are sigma-algebras.",
"answer": "1) Power set 2^X.\nX = {w, x, y, z} has 4 elements, so its power set has 2^4 = 16 subsets:\n2^X = {∅, {w}, {x}, {y}, {z}, {w,x}, {w,y}, {w,z}, {x,y}, {x,z}, {y,z}, {w,x,y}, {w,x,z}, {w,y,z}, {x,y,z}, {w,x,y,z}}.\n\n2) Number of possible set collections A ⊆ 2^X.\nAny set collection A is just a subset of 2^X. Since |2^X| = 16, the number of possible collections is |P(2^X)| = 2^16 = 65,536.\n\n3) Four collections that are not sigma-algebras and their minimal augmentations.\nRecall: a sigma-algebra Σ on X must\n(i) contain ∅ and X,\n(ii) be closed under complement in X,\n(iii) be closed under countable unions (on a finite set this is the same as closure under finite unions).\n\nExample 1.\nA₁ = {∅, {w}, X}.\nThis is not a sigma-algebra because the complement of {w} is {x,y,z}, which is not in A₁.\nThe smallest sigma-algebra containing A₁ must add this missing complement. Closure under unions and complements then gives\nΣ₁ = {∅, {w}, {x,y,z}, X}.\nThis is a sigma-algebra (it corresponds to the partition {{w}, {x,y,z}}) and is clearly minimal over A₁.\n\nExample 2.\nA₂ = {∅, {w,x}, X}.\nNot a sigma-algebra: the complement of {w,x} is {y,z}, which is not in A₂.\nWe minimally augment by adding {y,z}. The resulting collection\nΣ₂ = {∅, {w,x}, {y,z}, X}\ncontains ∅ and X, is closed under complements ({w,x} ↔ {y,z}), and unions ({w,x} ∪ {y,z} = X). Thus Σ₂ is the minimal sigma-algebra containing A₂ (partition {{w,x}, {y,z}}).\n\nExample 3.\nA₃ = {∅, {w}, {x}, X}.\nNot a sigma-algebra: the complement of {w} is {x,y,z} (missing), the complement of {x} is {w,y,z} (missing), and the union {w} ∪ {x} = {w,x} is not in A₃.\nThe smallest sigma-algebra containing A₃ is generated by the partition {{w}, {x}, {y,z}}. All unions of these blocks are:\n∅,\n{w},\n{x},\n{y,z},\n{w,x},\n{w,y,z},\n{x,y,z},\nX.\nSo the minimal augmentation is\nΣ₃ = {∅, {w}, {x}, {y,z}, {w,x}, {w,y,z}, {x,y,z}, X}.\nThis is a sigma-algebra and any sigma-algebra containing A₃ must contain these sets, so Σ₃ is minimal.\n\nExample 4.\nA₄ = {∅, {w}, {x}, {y}, X}.\nNot a sigma-algebra: the complement of {w} is {x,y,z} (missing), and the union {w} ∪ {x} = {w,x} is not in A₄.\nStarting from A₄, closure under finite unions gives\n{w,x}, {w,y}, {x,y}, and {w,x,y}.\nThen the complement of {w,x,y} is {z}, so {z} must be included. With {w}, {x}, {y}, {z} we now have the partition {{w},{x},{y},{z}}. The sigma-algebra generated by this partition is the full power set 2^X.\nHence the minimal augmentation of A₄ is\nΣ₄ = 2^X (all 16 subsets of X).\n\nSummary: we found the power set 2^X, counted 65,536 possible set collections, and gave four explicit non-sigma-algebra collections A₁–A₄ together with their minimal sigma-algebra extensions Σ₁–Σ₄."
},

{
"source": "textbook",
"question": "A function f : X → Y is onto (surjective) iff for each image element y ∈ Y there is a pre-image element x ∈ X such that y = f(x). A function f is one-to-one (injective) iff distinct pre-images have distinct images: f(x1) ≠ f(x2) if x1 ≠ x2 for all x1, x2 ∈ X. A function is bijective iff it is both injective and surjective (precisely when the inverse point function f^{-1} exists).\n\nSuppose A ⊂ X and B ⊂ Y for f : X → Y. Prove or disprove:\n(a) f(f^{-1}(B)) ⊂ B.\n(b) f(f^{-1}(B)) = B if f is surjective.\n(c) A ⊂ f^{-1}(f(A)).\n(d) A = f^{-1}(f(A)) if f is injective.\n(e) If f : X → Y is bijective, then the induced map F : 2^X → 2^Y defined by F(S) = f(S) is bijective.",
"answer": "(a) We show f(f^{-1}(B)) ⊂ B.\nLet y ∈ f(f^{-1}(B)). Then there exists x ∈ f^{-1}(B) such that y = f(x). By definition of pre-image, x ∈ f^{-1}(B) implies f(x) ∈ B. Hence y ∈ B, and therefore f(f^{-1}(B)) ⊂ B.\n\n(b) Assume f is surjective. We already know from part (a) that f(f^{-1}(B)) ⊂ B.\nTo prove the reverse inclusion, let y ∈ B. Since f is onto, there exists x ∈ X with f(x) = y. Because y ∈ B, we have x ∈ f^{-1}(B), so y = f(x) ∈ f(f^{-1}(B)). Thus B ⊂ f(f^{-1}(B)), and combining both inclusions gives f(f^{-1}(B)) = B.\n\n(c) Show A ⊂ f^{-1}(f(A)).\nTake any a ∈ A. Then by definition of image, f(a) ∈ f(A). Hence a ∈ f^{-1}(f(A)). Since every a ∈ A lies in f^{-1}(f(A)), we obtain A ⊂ f^{-1}(f(A)).\n\n(d) Assume f is injective and prove A = f^{-1}(f(A)).\nFrom part (c) we already know A ⊂ f^{-1}(f(A)). For the opposite inclusion, let x ∈ f^{-1}(f(A)). Then f(x) ∈ f(A), so there exists some a ∈ A with f(x) = f(a). Because f is injective, f(x) = f(a) implies x = a, so x ∈ A. Thus f^{-1}(f(A)) ⊂ A, and together with the previous inclusion we conclude A = f^{-1}(f(A)).\n\n(e) Suppose f : X → Y is bijective. Define F : 2^X → 2^Y by F(S) = f(S) = {f(x) : x ∈ S}.\n\nInjectivity of F: Assume F(S1) = F(S2); that is, f(S1) = f(S2). Since f is injective, applying part (d) with A = S1 and A = S2 gives S1 = f^{-1}(f(S1)) and S2 = f^{-1}(f(S2)). Hence S1 = f^{-1}(f(S1)) = f^{-1}(f(S2)) = S2, so F is injective.\n\nSurjectivity of F: Let C ⊂ Y be arbitrary. Because f is bijective, it has an inverse function f^{-1} : Y → X. Set S = f^{-1}(C) ⊂ X. For any y ∈ C, there exists x = f^{-1}(y) ∈ S with y = f(x), so y ∈ f(S); hence C ⊂ f(S). Conversely, if y ∈ f(S), then y = f(x) for some x ∈ S = f^{-1}(C), which implies y ∈ C. Thus f(S) = C, so every subset C ⊂ Y lies in the image of F and F is surjective.\n\nTherefore, when f : X → Y is bijective, the induced map F : 2^X → 2^Y is also bijective."
},

{
"source": "homework",
"question": "Show that P[A ∩ B ∩ C] = P[A | B ∩ C] P[B | C] P[C].",
"answer": "We use only the definition of conditional probability.\n\nBy definition, for any events E and F with P[F] > 0,\nP[E | F] = P[E ∩ F] / P[F].\n\nFirst apply this to P[A | B ∩ C]:\nP[A | B ∩ C] = P[A ∩ B ∩ C] / P[B ∩ C]. (1)\n\nNext apply it to P[B | C]:\nP[B | C] = P[B ∩ C] / P[C]. (2)\n\nNow multiply the three factors on the right-hand side of the desired equality:\n\nP[A | B ∩ C] P[B | C] P[C]\n= (P[A ∩ B ∩ C] / P[B ∩ C]) · (P[B ∩ C] / P[C]) · P[C]\n= P[A ∩ B ∩ C].\n\nThus we have shown that\nP[A ∩ B ∩ C] = P[A | B ∩ C] P[B | C] P[C]."
},

{
"source": "homework",
"question": "A computer manufacturer uses chips from three sources. Chips from sources A, B, and C are defective with probabilities .005, .001, and .010, respectively. If a randomly selected chip is found to be defective, find the probability that the manufacturer was A; that the manufacturer was C. Assume that the proportions of chips from A, B, and C are 0.5, 0.1, and 0.4, respectively.",
"answer": "Let A, B, C be the events that a randomly chosen chip comes from source A, B, or C, and let D be the event that the chip is defective.\n\nGiven:\nP(A) = 0.5,  P(B) = 0.1,  P(C) = 0.4,\nP(D | A) = 0.005,  P(D | B) = 0.001,  P(D | C) = 0.010.\n\nFirst compute P(D) using the law of total probability:\nP(D) = P(D | A)P(A) + P(D | B)P(B) + P(D | C)P(C)\n     = 0.005·0.5 + 0.001·0.1 + 0.010·0.4\n     = 0.0025 + 0.0001 + 0.004\n     = 0.0066.\n\nNow apply Bayes’ rule.\n\n1) Probability the chip was from A given that it is defective:\nP(A | D) = [P(D | A)P(A)] / P(D)\n         = (0.005·0.5) / 0.0066\n         = 0.0025 / 0.0066\n         = 25/66 ≈ 0.379.\n\n2) Probability the chip was from C given that it is defective:\nP(C | D) = [P(D | C)P(C)] / P(D)\n         = (0.010·0.4) / 0.0066\n         = 0.004 / 0.0066\n         = 20/33 ≈ 0.606.\n\nSo, given that a chip is defective, the probability it came from A is 25/66 (about 37.9%), and the probability it came from C is 20/33 (about 60.6%)."
},

{
"source": "homework",
"question": "Let S = {1, 2, 3, 4} and A = {1, 2}, B = {1, 3}, C = {1, 4}. Assume the outcomes are equiprobable. Are A, B, and C independent events?",
"answer": "Since all 4 outcomes in S are equiprobable, each has probability 1/4.\n\nP(A) = 2/4 = 1/2, P(B) = 2/4 = 1/2, P(C) = 2/4 = 1/2.\n\nPairwise intersections:\nA ∩ B = {1} ⇒ P(A ∩ B) = 1/4 = P(A)P(B) = (1/2)(1/2).\nA ∩ C = {1} ⇒ P(A ∩ C) = 1/4 = P(A)P(C) = (1/2)(1/2).\nB ∩ C = {1} ⇒ P(B ∩ C) = 1/4 = P(B)P(C) = (1/2)(1/2).\n\nThus every pair of events is independent (pairwise independence holds).\n\nNow consider the triple intersection:\nA ∩ B ∩ C = {1} ⇒ P(A ∩ B ∩ C) = 1/4,\nwhile\nP(A)P(B)P(C) = (1/2)(1/2)(1/2) = 1/8.\n\nSince P(A ∩ B ∩ C) ≠ P(A)P(B)P(C), the three events are not independent as a collection.\n\nConclusion: A, B, and C are pairwise independent but not mutually independent."
},

{
"source": "homework",
"question": "Let U be selected at random from the unit interval (i.e. U ~ Uniform(0,1)). Let A = {0 < U < 1/2}, B = {1/4 < U < 3/4}, and C = {1/2 < U < 1}. Are any of these events independent?",
"answer": "Because U is uniformly distributed on (0,1), the probability of an event is just the length of its interval.\n\nFirst compute the probabilities of A, B, C:\n- A = (0, 1/2) has length 1/2, so P(A) = 1/2.\n- B = (1/4, 3/4) has length 1/2, so P(B) = 1/2.\n- C = (1/2, 1) has length 1/2, so P(C) = 1/2.\n\nNow check pairwise intersections.\n\n1) A and B:\nA ∩ B = (1/4, 1/2), which has length 1/4. Thus P(A ∩ B) = 1/4.\nWe have P(A)P(B) = (1/2)(1/2) = 1/4.\nTherefore P(A ∩ B) = P(A)P(B), so A and B are independent.\n\n2) A and C:\nA ∩ C = (0, 1/2) ∩ (1/2, 1) = ∅, so P(A ∩ C) = 0.\nBut P(A)P(C) = (1/2)(1/2) = 1/4 ≠ 0.\nHence A and C are not independent.\n\n3) B and C:\nB ∩ C = (1/2, 3/4), which has length 1/4. Thus P(B ∩ C) = 1/4.\nAnd P(B)P(C) = (1/2)(1/2) = 1/4.\nTherefore P(B ∩ C) = P(B)P(C), so B and C are independent.\n\nFinally, check mutual independence of A, B, and C:\nA ∩ B ∩ C = (0, 1/2) ∩ (1/4, 3/4) ∩ (1/2, 1) = ∅, so P(A ∩ B ∩ C) = 0,\nwhile P(A)P(B)P(C) = (1/2)^3 = 1/8.\nSince these are not equal, A, B, and C are not mutually independent.\n\nSummary:\n- A and B are independent.\n- B and C are independent.\n- A and C are not independent.\n- The three events together are not mutually independent."
},

{
"source": "homework",
"question": "Show that if A and B are independent events, then the pairs (A, B^c), (A^c, B), and (A^c, B^c) are also independent.",
"answer": "Independence of A and B means\n\nP(A ∩ B) = P(A)P(B).\n\nWe must show that\n1) P(A ∩ B^c) = P(A)P(B^c),\n2) P(A^c ∩ B) = P(A^c)P(B),\n3) P(A^c ∩ B^c) = P(A^c)P(B^c).\n\nRecall that for any event E, P(E^c) = 1 − P(E).\n\n1) Independence of A and B^c.\nWrite A as a disjoint union:\nA = (A ∩ B) ∪ (A ∩ B^c).\nHence\nP(A) = P(A ∩ B) + P(A ∩ B^c).\nSo\nP(A ∩ B^c) = P(A) − P(A ∩ B) = P(A) − P(A)P(B) = P(A)(1 − P(B)) = P(A)P(B^c).\nThus A and B^c are independent.\n\n2) Independence of A^c and B.\nSimilarly,\nB = (A ∩ B) ∪ (A^c ∩ B),\nso\nP(B) = P(A ∩ B) + P(A^c ∩ B),\nand therefore\nP(A^c ∩ B) = P(B) − P(A ∩ B) = P(B) − P(A)P(B) = P(B)(1 − P(A)) = P(A^c)P(B).\nThus A^c and B are independent.\n\n3) Independence of A^c and B^c.\nUse complements of the union:\nA^c ∩ B^c = (A ∪ B)^c,\nso\nP(A^c ∩ B^c) = 1 − P(A ∪ B) = 1 − [P(A) + P(B) − P(A ∩ B)].\nUsing independence of A and B,\nP(A ∩ B) = P(A)P(B), hence\nP(A^c ∩ B^c) = 1 − P(A) − P(B) + P(A)P(B)\n= (1 − P(A))(1 − P(B))\n= P(A^c)P(B^c).\nTherefore A^c and B^c are independent.\n\nAll three required pairs are independent whenever A and B are independent."
},

{
"source": "homework",
"question": "Show that events A and B are independent if P[A|B] = P[A|B^c].",
"answer": "Assume 0 < P(B) < 1 so that both conditional probabilities P(A|B) and P(A|B^c) are defined.\n\nWe are given that\n\nP(A|B) = P(A|B^c). (1)\n\nLet this common value be denoted by p. Then by the law of total probability applied to event A and the partition {B, B^c}, we have\n\nP(A) = P(A|B)P(B) + P(A|B^c)P(B^c).\n\nSubstituting P(A|B) = P(A|B^c) = p into this equation gives\n\nP(A) = p P(B) + p P(B^c) = p [P(B) + P(B^c)] = p · 1 = p.\n\nHence\n\nP(A|B) = p = P(A).\n\nNow compute P(A ∩ B):\n\nP(A ∩ B) = P(A|B)P(B) = P(A)P(B).\n\nThis is exactly the defining condition for independence of A and B. Therefore A and B are independent whenever P(A|B) = P(A|B^c) (with P(B) ∈ (0,1))."
},

{
"source": "homework",
"question": "A student needs eight chips of a certain type to build a circuit. It is known that 5% of these chips are defective. How many chips should he buy for there to be a greater than 90% probability of having enough chips for the circuit?",
"answer": "Let each chip be good with probability 0.95 and defective with probability 0.05, independently of the others.\n\nSuppose the student buys n chips. Let Y be the number of good chips. Then Y ~ Binomial(n, 0.95). The student has enough chips if Y ≥ 8. We want the smallest n such that\n\nP(Y ≥ 8) ≥ 0.9.\n\n• For n = 8:\nP(Y ≥ 8) = P(Y = 8) = 0.95^8 ≈ 0.663 < 0.9, so 8 chips are not enough.\n\n• For n = 9:\nP(Y ≥ 8) = P(Y = 8) + P(Y = 9)\n= C(9,8)(0.95)^8(0.05) + (0.95)^9\n≈ 9·0.6634·0.05 + 0.6302\n≈ 0.2985 + 0.6302\n≈ 0.9287 > 0.9.\n\nThus buying 9 chips already gives a probability greater than 90% of having at least 8 good ones, and 8 chips are insufficient. Therefore, the student should buy **9 chips**."
},

{
"source": "homework",
"question": "An information source produces binary triplets {000, 111, 010, 101, 001, 110, 100, 011} with corresponding probabilities {1/4, 1/4, 1/8, 1/8, 1/16, 1/16, 1/16, 1/16}. A binary code assigns a codeword of length −log₂ p_k to triplet k. Let X be the length of the string assigned to the output of the information source.\n(a) Show the mapping from S to S_X, the range of X.\n(b) Find the probabilities for the various values of X.",
"answer": "(a) For each triplet k with probability p_k, X(k) = −log₂ p_k.\n\nThe probabilities take the values 1/4, 1/8, and 1/16, so the corresponding codeword lengths are:\n−log₂(1/4) = 2,\n−log₂(1/8) = 3,\n−log₂(1/16) = 4.\n\nThus the mapping X : S → S_X is\nX(000) = 2,\nX(111) = 2,\nX(010) = 3,\nX(101) = 3,\nX(001) = 4,\nX(110) = 4,\nX(100) = 4,\nX(011) = 4.\n\nHence the range of X is S_X = {2, 3, 4}.\n\n(b) The probability mass function of X is obtained by summing the probabilities of all triplets that give the same length:\n\nP(X = 2) = P(000) + P(111) = 1/4 + 1/4 = 1/2,\nP(X = 3) = P(010) + P(101) = 1/8 + 1/8 = 1/4,\nP(X = 4) = P(001) + P(110) + P(100) + P(011)\n = 1/16 + 1/16 + 1/16 + 1/16 = 4/16 = 1/4.\n\nSo the distribution of X is:\nX = 2 with probability 1/2,\nX = 3 with probability 1/4,\nX = 4 with probability 1/4."
},

{
"source": "homework",
"question": "An urn contains nine $1 bills and one $50 bill. Let the random variable X be the total amount that results when two bills are drawn from the urn without replacement.\n\n(a) Describe the underlying space S of this random experiment and specify the probabilities of its elementary events.\n(b) Show the mapping from S to S_X, the range of X.\n(c) Find the probabilities for the various values of X.",
"answer": "(a) Two bills are drawn without replacement. Since only the denominations matter and the order of drawing does not, there are only two qualitatively different outcomes:\n\n ω₁: both bills are $1.\n ω₂: one bill is $1 and the other is $50.\n\nThe sample space is therefore S = {ω₁, ω₂}.\n\nThere are C(10,2) = 45 equally likely unordered pairs of physical bills.\n- Number of ways to draw two $1 bills: C(9,2) = 36.\n- Number of ways to draw one $1 bill and the $50 bill: C(9,1)·C(1,1) = 9.\n\nHence the probabilities of the elementary events are\nP(ω₁) = 36/45 = 4/5,\nP(ω₂) = 9/45 = 1/5.\n\n(b) The random variable X assigns to each outcome the total dollar amount of the two drawn bills:\n\n X(ω₁) = 1 + 1 = 2,\n X(ω₂) = 1 + 50 = 51.\n\nThus the range of X is S_X = {2, 51}.\n\n(c) The distribution of X is obtained from the probabilities of the corresponding outcomes:\n\nP(X = 2) = P(ω₁) = 4/5,\nP(X = 51) = P(ω₂) = 1/5.\n\nSo X takes the value 2 dollars with probability 4/5 and 51 dollars with probability 1/5."
},

{
"source": "homework",
"question": "A coin is tossed n times. Let the random variable Y be the difference between the number of heads and the number of tails in the n tosses of a coin. Assume P[heads] = p.\n(a) Describe the sample space S.\n(b) Find the probability of the event {Y = 0}.\n(c) Find the probabilities for the other values of Y.",
"answer": "(a) Each outcome is an ordered sequence of n results, each result being H or T. Thus\nS = { (ω1,...,ωn): ωi ∈ {H,T} for i = 1,...,n },\nso |S| = 2^n.\n\nLet X be the number of heads in n tosses. Then X ~ Binomial(n,p), and the number of tails is n − X. By definition,\nY = (#heads) − (#tails) = X − (n − X) = 2X − n.\n\n(b) The event {Y = 0} means #heads = #tails, so X = n/2. This is only possible when n is even. Hence\nP(Y = 0) =\n C(n, n/2) p^{n/2} (1 − p)^{n/2}, if n is even,\n 0, if n is odd.\n\n(c) For a general integer y, we have\nY = y ⇔ 2X − n = y ⇔ X = (n + y)/2.\nThis requires that (n + y)/2 be an integer between 0 and n, i.e. y ∈ {−n, −n+2, ..., n−2, n} and y has the same parity as n.\nFor such y,\nP(Y = y) = P(X = (n + y)/2)\n= C(n, (n + y)/2) p^{(n + y)/2} (1 − p)^{(n − y)/2},\nand for all other integers y, P(Y = y) = 0."
},

{
"source": "homework",
"question": "Let X be a random variable with pmf p_k = c / k^2 for k = 1, 2, ....\n(a) Estimate the value of c numerically (the series converges).\n(b) Find P[X > 4].\n(c) Find P[6 ≤ X ≤ 8].",
"answer": "(a) Since {p_k} is a pmf, we must have\n∑{k=1}^{∞} p_k = 1.\nThus\n∑{k=1}^{∞} c/k^2 = 1 ⇒ c · ∑{k=1}^{∞} 1/k^2 = 1.\nThe series ∑{k=1}^{∞} 1/k^2 is known to equal π^2/6. Hence\nc · (π^2/6) = 1 ⇒ c = 6/π^2 ≈ 0.6079.\n\n(b) For X > 4 we need\nP[X > 4] = ∑{k=5}^{∞} p_k = ∑{k=5}^{∞} c/k^2.\nIt is convenient to write this as\nP[X > 4] = 1 − P[X ≤ 4]\n= 1 − ∑_{k=1}^{4} c/k^2\n= 1 − c (1 + 1/4 + 1/9 + 1/16).\nNow\n1 + 1/4 + 1/9 + 1/16 = 205/144,\nand c = 6/π^2, so\nP[X > 4] = 1 − (6/π^2) · (205/144)\n= 1 − 205/(24π^2) ≈ 0.135.\n\n(c) For 6 ≤ X ≤ 8,\nP[6 ≤ X ≤ 8] = p_6 + p_7 + p_8\n= c (1/6^2 + 1/7^2 + 1/8^2)\n= (6/π^2) (1/36 + 1/49 + 1/64).\nCombine the fractions:\n1/36 + 1/49 + 1/64 = 1801/28224.\nThus\nP[6 ≤ X ≤ 8] = (6/π^2) · 1801/28224\n= 1801/(4704 π^2) ≈ 0.039.\n\nSo:\n(a) c = 6/π^2 ≈ 0.6079.\n(b) P[X > 4] = 1 − 205/(24π^2) ≈ 0.135.\n(c) P[6 ≤ X ≤ 8] = 1801/(4704π^2) ≈ 0.039."
},

{
"source": "homework",
"question": "A modem transmits a +2 voltage signal into a channel. The channel adds to this signal a noise term N that is drawn from the set {0, −1, −2, −3} with respective probabilities {4/10, 3/10, 2/10, 1/10}. Let Y be the output of the channel (so Y = 2 + N).\n(a) Find the pmf of the output Y.\n(b) What is the probability that the output of the channel is equal to the input of the channel?\n(c) What is the probability that the output of the channel is positive?",
"answer": "(a) The output is Y = 2 + N, where N takes values {0, −1, −2, −3}.\n• If N = 0, then Y = 2 with P = 4/10.\n• If N = −1, then Y = 1 with P = 3/10.\n• If N = −2, then Y = 0 with P = 2/10.\n• If N = −3, then Y = −1 with P = 1/10.\nThus the pmf of Y is\nP(Y = 2) = 0.4,\nP(Y = 1) = 0.3,\nP(Y = 0) = 0.2,\nP(Y = −1) = 0.1.\n\n(b) The output equals the input (+2) exactly when N = 0, so\nP(output = input) = P(N = 0) = 4/10 = 0.4.\n\n(c) The output is positive when Y > 0, i.e., Y ∈ {1, 2}. Therefore\nP(Y > 0) = P(Y = 1) + P(Y = 2) = 0.3 + 0.4 = 0.7."
},

{
"source": "homework",
"question": "A modem transmits a +2 voltage signal into a channel. The channel adds to this signal a noise term that is drawn from the set {0, −1, −2, −3} with respective probabilities {4/10, 3/10, 2/10, 1/10}. Let Y be the output of the channel.\n(a) Find the pmf of the output Y of the channel.\n(b) What is the probability that the output of the channel is equal to the input of the channel?\n(c) What is the probability that the output of the channel is positive?",
"answer": "(a) Let N be the noise term. Then Y = 2 + N.\nN takes values {0, −1, −2, −3} with probabilities {4/10, 3/10, 2/10, 1/10}.\nSo Y takes values:\n- Y = 2 when N = 0, with P(Y = 2) = 4/10.\n- Y = 1 when N = −1, with P(Y = 1) = 3/10.\n- Y = 0 when N = −2, with P(Y = 0) = 2/10.\n- Y = −1 when N = −3, with P(Y = −1) = 1/10.\nThus the pmf is\nP(Y = 2) = 0.4,\nP(Y = 1) = 0.3,\nP(Y = 0) = 0.2,\nP(Y = −1) = 0.1.\n\n(b) The output equals the input (+2) iff N = 0. Hence\nP(output = input) = P(N = 0) = 4/10 = 0.4.\n\n(c) The output is positive when Y > 0, i.e., Y ∈ {1, 2}. Therefore\nP(Y > 0) = P(Y = 1) + P(Y = 2) = 0.3 + 0.4 = 0.7."
},

{
"source": "homework",
"question": "Indicate the value of the indicator function for the event A, I_A(ζ), for each ζ in the sample space S. Find the pmf and expected value of I_A.\n(a) S = {1,2,3,4,5} and A = {ζ > 3}.\n(b) S = [0,1] and A = {0.3 < ζ ≤ 0.7}.\n(c) S = {ζ = (x,y): 0 < x < 1, 0 < y < 1} and A = {ζ = (x,y): 0.25 < x + y < 1.25}.\n(d) S = (−∞,∞) and A = {ζ > a}.",
"answer": "Recall that for any event A and outcome ζ,\nI_A(ζ) = 1 if ζ ∈ A, and I_A(ζ) = 0 if ζ ∉ A.\nThe random variable I_A is always Bernoulli with parameter P(A), so\nP(I_A = 1) = P(A), P(I_A = 0) = 1 − P(A), and E[I_A] = P(A).\nWe assume the underlying sample spaces are equipped with the natural (uniform) probability measure where appropriate.\n\n(a) S = {1,2,3,4,5}, A = {ζ > 3} = {4,5}.\nIndicator function:\nI_A(ζ) = 0 for ζ = 1,2,3; I_A(ζ) = 1 for ζ = 4,5.\nWith all five outcomes equiprobable,\nP(A) = 2/5.\nHence the pmf of I_A is\nP(I_A = 1) = 2/5, P(I_A = 0) = 3/5,\nand\nE[I_A] = P(A) = 2/5.\n\n(b) S = [0,1] with ζ uniform on [0,1], A = {0.3 < ζ ≤ 0.7}.\nIndicator function:\nI_A(ζ) = 1 if 0.3 < ζ ≤ 0.7, and I_A(ζ) = 0 otherwise.\nProbability of A is its length:\nP(A) = 0.7 − 0.3 = 0.4.\nThus the pmf of I_A is\nP(I_A = 1) = 0.4, P(I_A = 0) = 0.6,\nand\nE[I_A] = 0.4.\n\n(c) S = {(x,y): 0 < x < 1, 0 < y < 1}, the unit square with uniform (area) measure.\nA = {(x,y): 0.25 < x + y < 1.25}.\nIndicator function:\nI_A(x,y) = 1 if 0.25 < x + y < 1.25, and I_A(x,y) = 0 otherwise.\nTo find P(A), compute the area of A divided by the area of the square (which is 1).\nThe complement of A inside the square consists of two right triangles:\n- Triangle near the origin where x + y ≤ 0.25 has legs 0.25, so area = (1/2)(0.25)^2 = 0.03125.\n- Triangle in the opposite corner where x + y ≥ 1.25 has legs 0.75, so area = (1/2)(0.75)^2 = 0.28125.\nTotal excluded area = 0.03125 + 0.28125 = 0.3125 = 5/16.\nTherefore\nP(A) = 1 − 5/16 = 11/16.\nThus the pmf of I_A is\nP(I_A = 1) = 11/16, P(I_A = 0) = 5/16,\nand\nE[I_A] = 11/16.\n\n(d) S = (−∞,∞), A = {ζ > a}.\nIndicator function:\nI_A(ζ) = 1 if ζ > a, and I_A(ζ) = 0 if ζ ≤ a.\nLet F_ζ be the cdf of ζ, so P(ζ ≤ a) = F_ζ(a).\nThen\nP(A) = P(ζ > a) = 1 − F_ζ(a).\nHence the pmf of I_A is\nP(I_A = 1) = 1 − F_ζ(a), P(I_A = 0) = F_ζ(a),\nand\nE[I_A] = P(A) = 1 − F_ζ(a)."
},

{
"source": "homework",
"question": "Heat must be removed from a system according to how fast it is generated. Suppose the system has eight components each of which is active with probability 0.25, independently of the others. The design of the heat removal system requires finding the probabilities of the following events: (a) None of the systems is active. (b) Exactly one is active. (c) More than four are active. (d) More than two and fewer than six are active.",
"answer": "Let X be the number of active components in the system. Each component is active with probability p = 0.25 independently, so X ~ Binomial(n = 8, p = 0.25). Thus for k = 0,1,...,8 we have P(X = k) = C(8,k) (0.25)^k (0.75)^(8−k).\n\n(a) None of the systems is active means X = 0. Therefore\nP(X = 0) = (0.75)^8 = (3/4)^8 = 6561/65536 ≈ 0.1001.\n\n(b) Exactly one is active means X = 1. Hence\nP(X = 1) = C(8,1)(0.25)(0.75)^7 = 8(0.25)(0.75)^7 = 2187/8192 ≈ 0.2670.\n\n(c) More than four are active means X ≥ 5. Thus\nP(X > 4) = P(X ≥ 5) = Σ_{k=5}^{8} C(8,k)(0.25)^k(0.75)^(8−k)\n= C(8,5)(0.25)^5(0.75)^3 + C(8,6)(0.25)^6(0.75)^2 + C(8,7)(0.25)^7(0.75) + C(8,8)(0.25)^8\n= 189/8192 + 63/16384 + 3/8192 + 1/65536\n= 1789/65536 ≈ 0.0273.\n\n(d) More than two and fewer than six are active means 2 < X < 6, i.e. X = 3,4,5. Therefore\nP(2 < X < 6) = P(X = 3) + P(X = 4) + P(X = 5)\n= C(8,3)(0.25)^3(0.75)^5 + C(8,4)(0.25)^4(0.75)^4 + C(8,5)(0.25)^5(0.75)^3\n= 1701/8192 + 2835/32768 + 189/8192\n= 10395/32768 ≈ 0.3172."
},

{
"source": "homework",
"question": "Let X be a binomial random variable with parameters n and p.\n(a) Show that\n p_X(k+1) / p_X(k) = ((n-k)/(k+1)) * (p/(1-p)),\nwhere p_X(0) = (1-p)^n.\n(b) Show that part (a) implies: (1) P[X = k] is maximum at k_max = ⌊(n+1)p⌋, where ⌊x⌋ denotes the largest integer that is smaller than or equal to x; and (2) when (n+1)p is an integer, then the maximum is achieved at k_max and k_max − 1.",
"answer": "(a) For X ~ Bin(n,p),\n\np_X(k) = P(X = k) = C(n,k) p^k (1-p)^{n-k}, k = 0,1,...,n.\n\nHence\np_X(k+1)\n = C(n,k+1) p^{k+1} (1-p)^{n-k-1}.\nSo\np_X(k+1) / p_X(k)\n = [C(n,k+1)/C(n,k)] * [p^{k+1}/p^k] * [(1-p)^{n-k-1}/(1-p)^{n-k}]\n = [(n-k)/(k+1)] * p * (1-p)^{-1}\n = (n-k)/(k+1) * p/(1-p).\n\nFor k = 0, we have\np_X(0) = C(n,0) p^0 (1-p)^n = (1-p)^n,\nwhich is the stated value.\n\n(b) From (a), the ratio between consecutive probabilities is\n\nR_k := p_X(k+1)/p_X(k) = (n-k)/(k+1) * p/(1-p).\n\nThe pmf increases with k as long as R_k > 1 and decreases once R_k < 1. The boundary is obtained from R_k ≥ 1:\n\n(n-k)p ≥ (k+1)(1-p).\n\nExpanding the right-hand side and simplifying:\nnp - kp ≥ k+1 - (k+1)p\nnp - kp - k - 1 + (k+1)p ≥ 0\nnp + p - (k+1) ≥ 0\nk + 1 ≤ (n+1)p.\n\nThus R_k ≥ 1 exactly when k ≤ (n+1)p − 1, and R_k < 1 when k ≥ (n+1)p. This shows that p_X(k) increases up to the first k where k+1 exceeds (n+1)p, and decreases afterwards.\n\nLet m = ⌊(n+1)p⌋. Then:\n- For k = m−1 we have k+1 = m ≤ (n+1)p, so R_{m−1} ≥ 1 and p_X(m) ≥ p_X(m−1).\n- For k = m we have k+1 = m+1 > (n+1)p, so R_m < 1 and p_X(m+1) < p_X(m).\n\nHence p_X(k) attains its maximum at k = m = ⌊(n+1)p⌋.\n\nIf (n+1)p is not an integer, then m < (n+1)p < m+1, which gives p_X(m) > p_X(m−1) and p_X(m) > p_X(m+1); therefore m is the unique mode.\n\nIf (n+1)p is an integer, say (n+1)p = m, then for k = m−1 we obtain\nR_{m−1} = (n-(m−1))/(m) * p/(1-p) = 1,\nso p_X(m) = p_X(m−1), and for k = m we still have R_m < 1, so the sequence decreases after m. Therefore the maximum value of the pmf is achieved at both k = m−1 and k = m, i.e. at k_max−1 and k_max when k_max = (n+1)p.\n\nThus part (a) implies exactly the statements in part (b)."
},

{
"source": "homework",
"question": "Consider the expression (a + b + c)^n.\n(a) Use the binomial expansion for (a + b) and c to obtain an expression for (a + b + c)^n.\n(b) Now expand all terms of the form (a + b)^k and obtain an expression that involves the multinomial coefficient for M = 3 mutually exclusive events, A1, A2, A3.\n(c) Let p1 = P[A1], p2 = P[A2], p3 = P[A3]. Use the result from part b to show that the multinomial probabilities add to one.",
"answer": "(a) Group a and b together:\n\n(a + b + c)^n = ((a + b) + c)^n.\n\nUsing the binomial theorem with x = (a + b), y = c we get\n\n(a + b + c)^n = sum_{k=0}^n C(n, k) (a + b)^k c^{n-k}. (1)\n\n(b) Now expand each (a + b)^k in (1) using the binomial theorem again:\n\n(a + b)^k = sum_{i=0}^k C(k, i) a^i b^{k-i}.\n\nSubstitute this into (1):\n\n(a + b + c)^n = sum_{k=0}^n C(n, k) c^{n-k} sum_{i=0}^k C(k, i) a^i b^{k-i}.\n\nRe-index by letting\n i = number of a's,\n j = number of b's,\n l = number of c's.\n\nWe have k = i + j and l = n - k = n - i - j, so i, j, l ≥ 0 and i + j + l = n. Using\n\nC(n, k) C(k, i) = C(n, i + j) C(i + j, i) = n! / (i! j! l!),\n\nwe can rewrite the double sum as\n\n(a + b + c)^n = sum_{i,j,l ≥ 0, i+j+l=n} [ n! / (i! j! l!) ] a^i b^j c^l.\n\nThe coefficient\n\nn! / (i! j! l!)\n\nis the multinomial coefficient for M = 3 categories (corresponding to events A1, A2, A3) with counts i, j, l that sum to n.\n\n(c) Let p1 = P[A1], p2 = P[A2], p3 = P[A3]. For mutually exclusive and exhaustive events we have\n\np1 + p2 + p3 = 1.\n\nApply the result of part (b) with a = p1, b = p2, c = p3:\n\n(p1 + p2 + p3)^n = sum_{i+j+l=n} [ n! / (i! j! l!) ] p1^i p2^j p3^l.\n\nBut p1 + p2 + p3 = 1, so the left-hand side is 1^n = 1. Therefore\n\nsum_{i+j+l=n} [ n! / (i! j! l!) ] p1^i p2^j p3^l = 1.\n\nThe expression inside the sum is exactly the multinomial probability\n\nP(N1 = i, N2 = j, N3 = l) = n! / (i! j! l!) p1^i p2^j p3^l,\n\nso this identity shows that the multinomial probabilities over all triples (i, j, l) with i + j + l = n add up to one."
},

{
"source": "homework",
"question": "Let M be a geometric random variable with parameter p on {1,2,...}. Show that M satisfies the memoryless property: P[M ≥ k + j | M ≥ j + 1] = P[M ≥ k] for all integers j,k > 1.",
"answer": "For a geometric random variable M with parameter p (success probability each trial) and support {1,2,...}, we have the tail probability\n\nP(M ≥ n) = (1 − p)^{n−1}, n = 1,2,....\n\nWe want to compute P(M ≥ k + j | M ≥ j + 1) for integers j,k > 1.\n\nBy the definition of conditional probability,\n\nP(M ≥ k + j | M ≥ j + 1) = P(M ≥ k + j ∩ M ≥ j + 1) / P(M ≥ j + 1).\n\nBut the event {M ≥ k + j} is a subset of {M ≥ j + 1} (since k + j ≥ j + 1), so\n\nP(M ≥ k + j ∩ M ≥ j + 1) = P(M ≥ k + j).\n\nTherefore,\n\nP(M ≥ k + j | M ≥ j + 1) = P(M ≥ k + j) / P(M ≥ j + 1).\n\nUsing the tail formula,\n\nP(M ≥ k + j) = (1 − p)^{k + j − 1},\nP(M ≥ j + 1) = (1 − p)^{j}.\n\nHence\n\nP(M ≥ k + j | M ≥ j + 1) = (1 − p)^{k + j − 1} / (1 − p)^{j} = (1 − p)^{k − 1}.\n\nBut (1 − p)^{k − 1} is exactly P(M ≥ k), again from the tail formula. Thus\n\nP(M ≥ k + j | M ≥ j + 1) = P(M ≥ k)\n\nfor all integers j,k > 1. This proves that the geometric random variable M is memoryless."
},

{
"source": "homework",
"question": "Let X be a discrete random variable that assumes only nonnegative integer values and that satisfies the memoryless property. Show that X must be a geometric random variable. Hint: Find an equation that must be satisfied by g(m) = P[X ≥ m].",
"answer": "Since X takes values in {0,1,2,...} and is memoryless, we have for all integers m,n ≥ 0\n\nP[X ≥ m + n | X ≥ m] = P[X ≥ n]. \n\nBy the definition of conditional probability,\n\nP[X ≥ m + n | X ≥ m] = P[X ≥ m + n]/P[X ≥ m].\n\nHence for all m,n ≥ 0,\n\nP[X ≥ m + n] = P[X ≥ m] P[X ≥ n]. (1)\n\nDefine g(k) = P[X ≥ k], k = 0,1,2,... . Then (1) becomes\n\ng(m + n) = g(m) g(n), for all m,n ≥ 0, and g(0) = P[X ≥ 0] = 1.\n\nLet c = g(1) = P[X ≥ 1]. Using the functional equation with n = 1,\n\ng(k + 1) = g(k) g(1) = c g(k).\n\nBy induction on k:\n- For k = 0, g(0) = 1 = c^0.\n- Suppose g(k) = c^k. Then g(k + 1) = c g(k) = c·c^k = c^{k+1}.\n\nThus\n\ng(k) = P[X ≥ k] = c^k, for all k ≥ 0, where 0 ≤ c ≤ 1.\n\nNow obtain the pmf of X. For k ≥ 0,\n\nP[X = k] = P[X ≥ k] − P[X ≥ k + 1] \n= c^k − c^{k+1} \n= (1 − c) c^k.\n\nThis is exactly the pmf of a geometric random variable on {0,1,2,...} with parameter p = 1 − c:\n\nP[X = k] = p (1 − p)^k, k = 0,1,2,....\n\n(If one prefers a geometric supported on {1,2,...}, it is obtained simply by considering Y = X + 1.)\n\nTherefore, any nonnegative integer-valued discrete random variable that satisfies the memoryless property must be geometric."
},

{
"source": "homework",
"question": "Compare the Poisson approximation and the binomial probabilities for k = 0,1,2,3 and (n,p) = (10,0.1), (20,0.05), (100,0.01).",
"answer": "For each case X ~ Bin(n,p) with λ = np = 1. The Poisson approximation uses Y ~ Pois(λ = 1), so\n\nP_Pois(Y = k) = e^{-1} · 1^k / k!.\n\nNumerical values (rounded to 4 d.p.) are:\n\n1) n = 10, p = 0.1 (λ = 1)\n - k = 0: P_Bin = C(10,0) 0.1^0 0.9^{10} ≈ 0.3487,\n P_Pois = e^{-1} ≈ 0.3679.\n - k = 1: P_Bin = C(10,1) 0.1^1 0.9^{9} ≈ 0.3874,\n P_Pois = e^{-1} ≈ 0.3679.\n - k = 2: P_Bin = C(10,2) 0.1^2 0.9^{8} ≈ 0.1937,\n P_Pois = e^{-1}/2 ≈ 0.1839.\n - k = 3: P_Bin = C(10,3) 0.1^3 0.9^{7} ≈ 0.0574,\n P_Pois = e^{-1}/6 ≈ 0.0613.\n\n2) n = 20, p = 0.05 (λ = 1)\n - k = 0: P_Bin ≈ 0.3585, P_Pois ≈ 0.3679.\n - k = 1: P_Bin ≈ 0.3774, P_Pois ≈ 0.3679.\n - k = 2: P_Bin ≈ 0.1887, P_Pois ≈ 0.1839.\n - k = 3: P_Bin ≈ 0.0596, P_Pois ≈ 0.0613.\n\n3) n = 100, p = 0.01 (λ = 1)\n - k = 0: P_Bin ≈ 0.3660, P_Pois ≈ 0.3679.\n - k = 1: P_Bin ≈ 0.3697, P_Pois ≈ 0.3679.\n - k = 2: P_Bin ≈ 0.1849, P_Pois ≈ 0.1839.\n - k = 3: P_Bin ≈ 0.0610, P_Pois ≈ 0.0613.\n\nWe see that as n increases and p decreases with np fixed (=1 here), the binomial probabilities get closer to the Poisson probabilities. The approximation is roughest for n = 10, p = 0.1 and is very good for n = 100, p = 0.01."
},

{
"source": "homework",
"question": "A voltage X is uniformly distributed in the set {−3, −2, −1, 0, 1, 2, 3, 4}.\n(a) Find the mean and variance of X.\n(b) Find the mean and variance of Y = −2X^2 + 3.\n(c) Find the mean and variance of W = cos(πX/8).\n(d) Find the mean and variance of Z = cos^2(πX/8).",
"answer": "(a) X is discrete uniform on eight points {−3, −2, −1, 0, 1, 2, 3, 4}, each with probability 1/8.\n\nE[X] = (1/8)·(−3 − 2 − 1 + 0 + 1 + 2 + 3 + 4) = 4/8 = 1/2.\n\nE[X^2] = (1/8)·(9 + 4 + 1 + 0 + 1 + 4 + 9 + 16) = 44/8 = 11/2.\n\nVar(X) = E[X^2] − (E[X])^2 = 11/2 − (1/2)^2 = 11/2 − 1/4 = 21/4 = 5.25.\n\nSo\nE[X] = 1/2, Var(X) = 21/4.\n\n(b) Y = −2X^2 + 3.\n\nE[Y] = −2E[X^2] + 3 = −2·(11/2) + 3 = −11 + 3 = −8.\n\nTo get Var(Y), first find Var(X^2).\nWe already have E[X^2] = 11/2. Next\nE[X^4] = (1/8)·(81 + 16 + 1 + 0 + 1 + 16 + 81 + 256) = 452/8 = 113/2.\n\nVar(X^2) = E[X^4] − (E[X^2])^2 = 113/2 − (11/2)^2 = 113/2 − 121/4 = (226 − 121)/4 = 105/4.\n\nSince Y = −2X^2 + 3, Var(Y) = (−2)^2 Var(X^2) = 4·(105/4) = 105.\n\nSo\nE[Y] = −8, Var(Y) = 105.\n\n(c) W = cos(πX/8).\nValues of cos(πx/8) for x ∈ {−3, −2, −1, 0, 1, 2, 3, 4} are:\n- For x = ±3: cos(3π/8).\n- For x = ±2: cos(π/4).\n- For x = ±1: cos(π/8).\n- For x = 0: cos 0 = 1.\n- For x = 4: cos(π/2) = 0.\n\nThus\nE[W] = (1/8)[2cos(3π/8) + 2cos(π/4) + 2cos(π/8) + 1 + 0]\n≈ (1/8)(2·0.382683 + 2·0.707107 + 2·0.923880 + 1)\n≈ 0.6284.\n\nFor the variance we need E[W^2] = E[cos^2(πX/8)]. But cos^2(πX/8) is exactly the random variable Z of part (d). From part (d) we will find E[Z] = 1/2, so E[W^2] = 1/2.\n\nTherefore\nVar(W) = E[W^2] − (E[W])^2 = 1/2 − (E[W])^2 ≈ 0.5 − 0.6284^2 ≈ 0.105.\n\nSo (numerically)\nE[W] ≈ 0.628, Var(W) ≈ 0.105.\n\n(d) Z = cos^2(πX/8).\nUsing cos^2 θ = (1 + cos 2θ)/2, we get values:\n- For x = ±1: cos^2(π/8) = (1 + cos(π/4))/2 = (1 + √2/2)/2.\n- For x = ±2: cos^2(π/4) = (1 + cos(π/2))/2 = 1/2.\n- For x = ±3: cos^2(3π/8) = (1 + cos(3π/4))/2 = (1 − √2/2)/2.\n- For x = 0: cos^2 0 = 1.\n- For x = 4: cos^2(π/2) = 0.\n\nThen\nE[Z] = (1/8)[2cos^2(3π/8) + 2cos^2(π/4) + 2cos^2(π/8) + 1 + 0].\nNoting that cos^2(π/8) + cos^2(3π/8) = 1/2( (1 + cos(π/4)) + (1 + cos(3π/4)) ) = 1/2(2) = 1,\nwe get\nE[Z] = (1/8)[2·1 + 2·(1/2) + 1] = (1/8)(2 + 1 + 1) = 4/8 = 1/2.\n\nFor Var(Z) we need E[Z^2] = E[cos^4(πX/8)]. Using cos^4 θ = (3 + 4cos 2θ + cos 4θ)/8 or by squaring the exact cos^2 values,\n- cos^4(π/8) = (3 + 2√2)/8,\n- cos^4(3π/8) = (3 − 2√2)/8,\n- cos^4(π/4) = 1/4,\n- cos^4 0 = 1,\n- cos^4(π/2) = 0.\n\nThus\nE[Z^2] = (1/8)[2cos^4(3π/8) + 2cos^4(π/4) + 2cos^4(π/8) + 1]\n= (1/8)[2·(3−2√2)/8 + 2·(1/4) + 2·(3+2√2)/8 + 1]\n= (1/8)[(3/2) + 1/2 + 1] = (1/8)·3 = 3/8.\n\nTherefore\nVar(Z) = E[Z^2] − (E[Z])^2 = 3/8 − (1/2)^2 = 3/8 − 1/4 = 1/8.\n\nSo\nE[Z] = 1/2, Var(Z) = 1/8."
},

{
"source": "textbook",
"question": "Let X be a Gaussian random variable with mean m and variance σ^2.\n(a) Find P[X ≤ m].\n(b) Find P[|X − m| < kσ], for k = 1,2,3,4,5,6.\n(c) Find the value of k for which Q(k) = P[X > m + kσ] = 10^{−j} for j = 1,2,3,4,5,6.",
"answer": "(a) Standardize X by Z = (X − m)/σ, so Z ~ N(0,1). Then\nP[X ≤ m] = P[Z ≤ 0] = 1/2,\nby symmetry of the standard normal distribution.\n\n(b) Again use Z = (X − m)/σ. Then\nP[|X − m| < kσ] = P(−k < Z < k) = Φ(k) − Φ(−k),\nwhere Φ is the CDF of N(0,1). Since Φ(−k) = 1 − Φ(k),\nP[|X − m| < kσ] = Φ(k) − (1 − Φ(k)) = 2Φ(k) − 1 = 1 − 2Q(k),\nwhere Q(k) = 1 − Φ(k).\nUsing standard normal values:\n• k = 1: Φ(1) ≈ 0.8413 ⇒ P ≈ 2·0.8413 − 1 ≈ 0.683.\n• k = 2: Φ(2) ≈ 0.9772 ⇒ P ≈ 0.955.\n• k = 3: Φ(3) ≈ 0.99865 ⇒ P ≈ 0.997.\n• k = 4: Φ(4) ≈ 0.999968 ⇒ P ≈ 0.99994.\n• k = 5: Φ(5) ≈ 0.9999997 ⇒ P ≈ 0.9999994 (≈ 1 − 6×10^{−7}).\n• k = 6: Φ(6) ≈ 0.9999999990 ⇒ P ≈ 0.999999998.\n\nSo in compact form:\nP[|X − m| < kσ] = 2Φ(k) − 1, and the numerical probabilities for k = 1,…,6 are approximately\n0.6827, 0.9545, 0.9973, 0.99994, 0.9999994, 0.999999998.\n\n(c) Here Q(k) = P[Z > k] = 10^{−j}.\nThus k is the (1 − 10^{−j})–quantile of the standard normal distribution:\nΦ(k) = 1 − 10^{−j} ⇒ k = Φ^{−1}(1 − 10^{−j}).\nUsing standard normal quantiles (rounded to three decimals):\n• j = 1: Q(k) = 0.1 ⇒ k ≈ 1.282.\n• j = 2: Q(k) = 0.01 ⇒ k ≈ 2.326.\n• j = 3: Q(k) = 0.001 ⇒ k ≈ 3.090.\n• j = 4: Q(k) = 0.0001 ⇒ k ≈ 3.719.\n• j = 5: Q(k) = 10^{−5} ⇒ k ≈ 4.265.\n• j = 6: Q(k) = 10^{−6} ⇒ k ≈ 4.753.\n\nSo for each j, the required k is k_j = Φ^{−1}(1 − 10^{−j}), numerically\nk ≈ 1.282, 2.326, 3.090, 3.719, 4.265, 4.753 for j = 1,2,3,4,5,6 respectively."
},

{
"source": "textbook",
"question": "Let X be a Gaussian random variable with mean 2 and variance 4. The reward in a system is given by Y = (X)^+. Find the pdf of Y.",
"answer": "We have X ~ N(μ = 2, σ^2 = 4) so σ = 2 and\nf_X(x) = 1 / (2√(2π)) · exp(−(x − 2)^2 / 8), −∞ < x < ∞.\n\nDefine Y = X^+ = max(X, 0). Thus\n- If X ≤ 0 then Y = 0.\n- If X > 0 then Y = X.\n\nSo the distribution of Y is mixed: it has a point mass at 0 and a continuous density for y > 0.\n\nProbability mass at 0:\nP(Y = 0) = P(X ≤ 0) = Φ((0 − 2)/2) = Φ(−1) = 1 − Φ(1) ≈ 0.1587,\nwhere Φ is the standard normal CDF.\n\nFor y > 0,\nP(y < Y ≤ y + dy) = P(y < X ≤ y + dy),\nso the density of Y on (0, ∞) is the same as that of X:\n\nf_Y(y) = f_X(y) = 1 / (2√(2π)) · exp(−(y − 2)^2 / 8), y > 0.\n\nCollecting this, the pdf / distribution of Y can be written as\n- A point mass at 0 of size P(Y = 0) = Φ(−1) ≈ 0.1587;\n- A continuous density for y > 0 given by\n f_Y(y) = 1 / (2√(2π)) · exp(−(y − 2)^2 / 8), y > 0,\n and f_Y(y) = 0 for y < 0.\n\n(Equivalently, in measure notation,\n f_Y(y) = Φ(−1) δ_0(y) + 1/(2√(2π)) exp(−(y − 2)^2 / 8) · 1_{(0,∞)}(y).)"
},

{
"source": "textbook",
"question": "Let X be the number of successes in n Bernoulli trials where the probability of success is p. Let Y = X/n be the average number of successes per trial. Apply the Chebyshev inequality to the event { |Y − p| > a }. What happens as n → ∞?",
"answer": "First compute the mean and variance of Y.\n\nSince X ~ Binomial(n, p), we have\nE[X] = np, Var(X) = np(1 − p).\n\nDefine Y = X/n. Then\nE[Y] = E[X]/n = np/n = p,\nVar(Y) = Var(X)/n^2 = np(1 − p)/n^2 = p(1 − p)/n.\n\nChebyshev’s inequality states that, for any a > 0,\n\nP(|Y − E[Y]| > a) ≤ Var(Y)/a^2.\n\nHere E[Y] = p, so\n\nP(|Y − p| > a) ≤ Var(Y)/a^2 = \frac{p(1 − p)}{n a^2}.\n\nAs n → ∞, the right-hand side\n\n\frac{p(1 − p)}{n a^2} → 0.\n\nTherefore Chebyshev’s inequality shows that\n\nP(|Y − p| > a) → 0 as n → ∞,\n\ni.e., the sample average Y converges in probability to p (a statement of the Weak Law of Large Numbers for Bernoulli trials)."
},

{
"source": "homework",
"question": "A biased coin is tossed three times.\n(a) Find the entropy of the outcome if the sequence of heads and tails is noted.\n(b) Find the entropy of the outcome if the number of heads is noted.\n(c) Explain the difference between the entropies in parts (a) and (b).",
"answer": "Let the probability of heads be p and of tails be q=1−p.\n\n(a) Let X1,X2,X3 be the outcomes of the three tosses (H or T). They are i.i.d. Bernoulli(p). The entropy of one toss is\nh(p)=−p log2 p − q log2 q.\nSince the tosses are independent,\nH(X1,X2,X3)=H(X1)+H(X2)+H(X3)=3 h(p)=3[−p log2 p − (1−p) log2(1−p)].\n\n(b) Let N be the number of heads in the three tosses. Then N∈{0,1,2,3} with\nP(N=k)=C(3,k) p^k (1−p)^{3−k},\nwhere C(3,k) is the binomial coefficient. The entropy of N is\nH(N)=−∑{k=0}^3 P(N=k) log2 P(N=k)\n=−∑{k=0}^3 [C(3,k) p^k (1−p)^{3−k} log2(C(3,k) p^k (1−p)^{3−k})].\n\n(c) The sequence (X1,X2,X3) contains more detailed information than just the number of heads N, because N is a function of the sequence that forgets the order (e.g., HTT,THT,TTH all give N=1). Hence observing the full sequence cannot give less information than observing only N, so H(N)≤H(X1,X2,X3)=3 h(p). The difference\nH(X1,X2,X3)−H(N)=H((X1,X2,X3) | N)\nrepresents the extra uncertainty about the order of heads and tails given that we know only how many heads occurred."
},

{
"source": "homework",
"question": "A communication channel accepts input I from the set {0,1,2,3,4,5,6}. The channel output is X = I + N (mod 7), where N is equally likely to be +1 or −1.\n(a) Find the entropy of I if all inputs are equiprobable.\n(b) Find the entropy of I given that X = 4.",
"answer": "(a) If all inputs are equiprobable over the 7 symbols, then\nP[I = i] = 1/7, i = 0,1,2,3,4,5,6.\nThe entropy of I is\nH(I) = −∑_{i=0}^6 (1/7) log₂(1/7)\n = −7·(1/7)·log₂(1/7)\n = log₂ 7 bits.\n\n(b) We need the posterior distribution P(I | X = 4).\nThe channel rule is X = I + N (mod 7) with N = +1 or −1, each with probability 1/2, independent of I.\nWe look for all i such that we can get X = 4:\n- If N = +1, X = I + 1 (mod 7) ⇒ I = 3.\n- If N = −1, X = I − 1 (mod 7) ⇒ I = 5.\nThus only I = 3 or I = 5 can produce X = 4.\n\nJoint probabilities:\nP(I = 3, X = 4) = P(I = 3) P(N = +1) = (1/7)(1/2) = 1/14,\nP(I = 5, X = 4) = P(I = 5) P(N = −1) = (1/7)(1/2) = 1/14.\nHence\nP(X = 4) = 1/14 + 1/14 = 1/7.\n\nPosterior probabilities:\nP(I = 3 | X = 4) = (1/14) / (1/7) = 1/2,\nP(I = 5 | X = 4) = (1/14) / (1/7) = 1/2,\nP(I = i | X = 4) = 0 for i ≠ 3,5.\nSo given X = 4 the input is equally likely to be 3 or 5, and the conditional entropy is\nH(I | X = 4) = −(1/2) log₂(1/2) − (1/2) log₂(1/2) = 1 bit."
},

{
"source": "homework",
"question": "Let (X, Y) be the pair of outcomes from two independent tosses of a fair die.\n(a) Find the entropy of X.\n(b) Find the entropy of the pair (X, Y).\n(c) Find the entropy in n independent tosses of a die. Explain why entropy is additive in this case.",
"answer": "(a) For a fair die, X takes values {1,2,3,4,5,6} with probability 1/6 each.\nThe entropy is\nH(X) = −∑{i=1}^6 (1/6) log₂(1/6) = −6·(1/6)·log₂(1/6) = log₂ 6 bits.\n\n(b) The pair (X,Y) has 36 equally likely outcomes (1,1), …, (6,6), each with probability 1/36.\nThus\nH(X,Y) = −∑{(i,j)} (1/36) log₂(1/36)\n = −36·(1/36)·log₂(1/36)\n = log₂ 36\n = log₂(6²)\n = 2 log₂ 6 bits.\nEquivalently, since X and Y are independent and identically distributed,\nH(X,Y) = H(X) + H(Y) = log₂ 6 + log₂ 6 = 2 log₂ 6.\n\n(c) Let X₁, X₂, …, X_n be the outcomes of n independent tosses of the same fair die. Each Xi has entropy H(Xi) = log₂ 6. Because the tosses are independent, the joint pmf factorizes:\nP(X₁=x₁, …, X_n=x_n) = ∏{k=1}^n P(X_k = x_k).\nThen the joint entropy is\nH(X₁,…,X_n) = −∑{x₁,…,x_n} P(x₁,…,x_n) log₂ P(x₁,…,x_n)\n = −∑{x₁,…,x_n} P(x₁,…,x_n) log₂ ∏{k=1}^n P(x_k)\n = −∑{x₁,…,x_n} P(x₁,…,x_n) ∑{k=1}^n log₂ P(x_k)\n = ∑{k=1}^n \big( −∑{x₁,…,x_n} P(x₁,…,x_n) log₂ P(x_k) \big)\n = ∑_{k=1}^n H(X_k) = n log₂ 6.\nThus the entropy is additive in this case because independence makes the log of the joint probability equal to the sum of the logs of the individual probabilities, so the joint entropy becomes the sum of the individual entropies."
},

{
"source": "homework",
"question": "Let X take on values from {1,2,…,K}. Suppose that P[X = K] = p, and let H_Y be the entropy of X given that X is not equal to K. Show that\n\nH_X = −p ln p − (1 − p) ln(1 − p) + (1 − p) H_Y.",
"answer": "Let A be the event {X ≠ K}. Then P(A) = 1 − p and P(X = K) = p. Define a random variable Y that equals X conditioned on A, so Y takes values in {1,…,K−1} with probabilities\n\nq_i = P(Y = i) = P(X = i | X ≠ K), i = 1,…,K−1.\n\nThen for i = 1,…,K−1,\nP(X = i) = P(X = i, A) = P(A) P(X = i | A) = (1 − p) q_i.\n\nAlso, Σ_{i=1}^{K−1} q_i = 1.\n\nThe entropy of X is\nH_X = − Σ_{i=1}^{K−1} P(X = i) ln P(X = i) − P(X = K) ln P(X = K).\nSubstitute the probabilities:\nH_X = − Σ_{i=1}^{K−1} (1 − p) q_i ln[(1 − p) q_i] − p ln p.\n\nExpand the log term:\nln[(1 − p) q_i] = ln(1 − p) + ln q_i.\nThus\nH_X = − Σ_{i=1}^{K−1} (1 − p) q_i [ln(1 − p) + ln q_i] − p ln p\n = − (1 − p) ln(1 − p) Σ_{i=1}^{K−1} q_i − (1 − p) Σ_{i=1}^{K−1} q_i ln q_i − p ln p.\n\nSince Σ_{i=1}^{K−1} q_i = 1, the first sum becomes −(1 − p) ln(1 − p). The second sum is exactly the entropy of Y:\nH_Y = − Σ_{i=1}^{K−1} q_i ln q_i.\nTherefore\nH_X = −p ln p − (1 − p) ln(1 − p) + (1 − p) H_Y,\nwhich is the desired result."
},

{
"source": "homework",
"question": "The random variable X is uniformly distributed on the interval [0,a]. Suppose a is unknown, so we estimate a by the maximum value observed in n independent repetitions of the experiment; that is, we estimate a by Y = max{X1, X2, ..., Xn}. (a) Find P[Y ≤ y]. (b) Find the mean and variance of Y, and explain why Y is a good estimate for a when n is large.",
"answer": "(a) Each Xi is Uniform(0,a), so its CDF is F_X(x) = x/a for 0 ≤ x ≤ a. For the maximum Y = max{X1, ..., Xn},\nP(Y ≤ y) = P(X1 ≤ y, ..., Xn ≤ y) = [F_X(y)]^n = (y/a)^n, 0 ≤ y ≤ a,\nand\nP(Y ≤ y) = 0 for y < 0, P(Y ≤ y) = 1 for y ≥ a.\n\n(b) First find the pdf of Y by differentiating the CDF:\nf_Y(y) = d/dy (y/a)^n = n y^{n-1} / a^n, 0 ≤ y ≤ a.\n\nThen\nE[Y] = ∫_0^a y f_Y(y) dy = ∫_0^a y · (n y^{n-1}/a^n) dy\n = n/a^n ∫_0^a y^n dy\n = n/a^n · a^{n+1}/(n+1)\n = (n/(n+1)) a.\n\nSimilarly,\nE[Y^2] = ∫_0^a y^2 f_Y(y) dy = n/a^n ∫_0^a y^{n+1} dy\n = n/a^n · a^{n+2}/(n+2)\n = (n/(n+2)) a^2.\n\nHence the variance is\nVar(Y) = E[Y^2] − (E[Y])^2\n = a^2 [ n/(n+2) − (n/(n+1))^2 ]\n = a^2 · n / [ (n+2)(n+1)^2 ].\n\nExplanation: E[Y] = (n/(n+1)) a < a, but E[Y] → a as n → ∞, so the estimator becomes asymptotically unbiased. At the same time, Var(Y) = a^2 n / [ (n+2)(n+1)^2 ] → 0 as n → ∞, meaning Y concentrates more and more tightly around a. Therefore, for large n, the maximum Y is a very good estimate of the unknown endpoint a."
},

{
"source": "homework",
"question": "Let X be the difference and let Y be the sum of the number of heads obtained when Carlos and Michael each flip a fair coin twice.\n(a) Describe the underlying space S of this random experiment and show the mapping from S to S_{XY}, the range of the pair (X, Y).\n(b) Find the probabilities for all values of (X, Y).\n(c) Find P[X + Y = 1], P[X + Y = 2].",
"answer": "We denote Carlos’s two flips by (C1, C2) and Michael’s two flips by (M1, M2).\nEach flip is H or T and all 4 flips are independent.\n\n(a) Sample space S and mapping to (X, Y)\n\nCarlos’s outcomes for 2 flips:\n - HH → C = 2 heads\n - HT, TH → C = 1 head\n - TT → C = 0 heads\nMichael’s outcomes similarly give M ∈ {0,1,2}.\n\nThus\nS = {(C1, C2, M1, M2) : each entry ∈ {H, T}},\n|S| = 2^4 = 16.\n\nWe define\n C = number of heads in (C1, C2),\n M = number of heads in (M1, M2),\n X = C − M,\n Y = C + M.\n\nPossible (C, M) pairs (with their probabilities) are\n\nC, M ∈ {0, 1, 2} and\nP(C = 0) = P(C = 2) = 1/4, P(C = 1) = 1/2,\nP(M = 0) = P(M = 2) = 1/4, P(M = 1) = 1/2.\n\nMapping (C, M) → (X, Y):\n (0,0) → (0,0)\n (0,1) → (−1,1)\n (0,2) → (−2,2)\n (1,0) → ( 1,1)\n (1,1) → ( 0,2)\n (1,2) → (−1,3)\n (2,0) → ( 2,2)\n (2,1) → ( 1,3)\n (2,2) → ( 0,4).\n\nSo the range S_{XY} is\nS_{XY} = {(0,0), (−1,1), (−2,2), (1,1), (0,2), (−1,3), (2,2), (1,3), (0,4)}.\n\n(b) Probabilities of all (X, Y)\n\nBecause C and M are independent,\nP(C = c, M = m) = P(C = c) P(M = m).\nUsing the mapping above:\n\n- (X,Y) = (0,0) from (C,M) = (0,0):\n P = (1/4)(1/4) = 1/16.\n- (X,Y) = (−1,1) from (0,1):\n P = (1/4)(1/2) = 1/8.\n- (X,Y) = (−2,2) from (0,2):\n P = (1/4)(1/4) = 1/16.\n- (X,Y) = ( 1,1) from (1,0):\n P = (1/2)(1/4) = 1/8.\n- (X,Y) = (0,2) from (1,1):\n P = (1/2)(1/2) = 1/4.\n- (X,Y) = (−1,3) from (1,2):\n P = (1/2)(1/4) = 1/8.\n- (X,Y) = ( 2,2) from (2,0):\n P = (1/4)(1/4) = 1/16.\n- (X,Y) = ( 1,3) from (2,1):\n P = (1/4)(1/2) = 1/8.\n- (X,Y) = (0,4) from (2,2):\n P = (1/4)(1/4) = 1/16.\n\nThus the joint pmf of (X,Y) is\n (0,0): 1/16,\n (−1,1): 1/8,\n (−2,2): 1/16,\n (1,1): 1/8,\n (0,2): 1/4,\n (−1,3): 1/8,\n (2,2): 1/16,\n (1,3): 1/8,\n (0,4): 1/16.\n\n(c) P[X + Y = 1] and P[X + Y = 2]\n\nNote:\nX + Y = (C − M) + (C + M) = 2C.\n\nSo X + Y can only take values 0, 2, or 4 (since C ∈ {0,1,2}). Therefore\n\nP[X + Y = 1] = 0.\n\nMoreover,\nP[X + Y = 2] = P(2C = 2) = P(C = 1) = 1/2,\nusing the Binomial(2, 1/2) distribution for C.\n\nSo\nP[X + Y = 1] = 0,\nP[X + Y = 2] = 1/2."
},

{
"source": "textbook",
"question": "Let X and Y be independent random variables. Express the probabilities of the events (a) {a < X ≤ b} ∩ {Y > d}, (b) {a < X ≤ b} ∩ {c ≤ Y < d}, and (c) {|X| < a} ∩ {c ≤ Y ≤ d} in terms of F_X(x) and F_Y(y).",
"answer": "(Independence means P(A∩B) = P(A)P(B), and the CDFs are F_X(x)=P(X≤x), F_Y(y)=P(Y≤y).)\n\n(a) \nP({a < X ≤ b} ∩ {Y > d}) \n= P(a < X ≤ b) · P(Y > d) \n= [F_X(b) − F_X(a)] · [1 − F_Y(d)].\n\n(b) \nP({a < X ≤ b} ∩ {c ≤ Y < d}) \n= P(a < X ≤ b) · P(c ≤ Y < d) \n= [F_X(b) − F_X(a)] · [F_Y(d) − F_Y(c)],\n(where the strict/ non-strict endpoints differ only on possible point masses; for continuous Y this is exact).\n\n(c) \n{|X| < a} is {−a < X < a}. Thus\nP(|X| < a) = P(−a < X < a) = F_X(a) − F_X(−a),\nagain up to possible point masses at ±a. Also\nP(c ≤ Y ≤ d) = F_Y(d) − F_Y(c). \nBy independence,\nP({|X| < a} ∩ {c ≤ Y ≤ d}) \n= [F_X(a) − F_X(−a)] · [F_Y(d) − F_Y(c)]."
},

{
"source": "textbook",
"question": "Find E[|X−Y|] if X and Y are independent exponential random variables with parameters λ1=1 and λ2=2, respectively.",
"answer": {
"I": "Since X,Y ≥ 0 and are independent exponentials, we can express |X−Y| using X,Y and their minimum.",
"M": "For nonnegative variables, |X−Y| = X + Y − 2·min(X,Y). So\nE[|X−Y|] = E[X+Y] − 2E[min(X,Y)].\nHere X~Exp(λ1=1), Y~Exp(λ2=2).\nE[X] = 1/λ1 = 1,\nE[Y] = 1/λ2 = 1/2, so E[X+Y] = 3/2.\nFor independent exponentials, Z = min(X,Y) is exponential with rate λ1+λ2, hence\nE[min(X,Y)] = 1/(λ1+λ2) = 1/(1+2) = 1/3.",
"A": "E[|X−Y|] = 3/2 − 2·(1/3) = 3/2 − 2/3 = (9−4)/6 = 5/6.",
"C": "E[|X−Y|] = 5/6."
}
},

{
"source": "textbook",
"question": "The output of a channel is Y = X + N, where the input X and the noise N are independent, zero-mean random variables. (a) Find the correlation coefficient between the input X and the output Y. (b) Suppose we estimate the input X by a linear function g(Y) = aY. Find the value of a that minimizes the mean-squared error E[(X − aY)^2]. (c) Express the resulting mean-square error in terms of σ_X/σ_N.",
"answer": {
"a": {
"I": "We need ρ_XY = Cov(X,Y)/(σ_X σ_Y) for Y = X + N, with X and N zero-mean and independent.",
"M": "Cov(X,Y) = Cov(X, X+N) = Cov(X,X) + Cov(X,N) = Var(X) + 0 = σ_X^2.\nVar(Y) = Var(X+N) = Var(X) + Var(N) = σ_X^2 + σ_N^2, so σ_Y = √(σ_X^2 + σ_N^2).\nThus\nρ_XY = Cov(X,Y)/(σ_X σ_Y) = σ_X^2 / (σ_X √(σ_X^2 + σ_N^2)).",
"C": "ρ_XY = σ_X / √(σ_X^2 + σ_N^2) = 1 / √(1 + σ_N^2/σ_X^2)."
},
"b": {
"I": "We want the linear MMSE estimator aY for X; the optimal coefficient is a* = Cov(X,Y)/Var(Y).",
"M": "From part (a), Cov(X,Y) = σ_X^2 and Var(Y) = σ_X^2 + σ_N^2.\nTherefore\n a* = Cov(X,Y)/Var(Y) = σ_X^2 / (σ_X^2 + σ_N^2).",
"C": "a* = σ_X^2 / (σ_X^2 + σ_N^2)."
},
"c": {
"I": "The minimum MSE for the linear estimator is Var(X) − Cov(X,Y)^2/Var(Y). Express this in terms of r = σ_X/σ_N.",
"M": "MMSE = σ_X^2 − (σ_X^4)/(σ_X^2 + σ_N^2)\n= σ_X^2 (1 − σ_X^2/(σ_X^2 + σ_N^2))\n= σ_X^2 σ_N^2 / (σ_X^2 + σ_N^2).\nLet r = σ_X/σ_N. Then σ_X^2 = r^2 σ_N^2, so\nMMSE = (r^2 σ_N^2 · σ_N^2)/(r^2 σ_N^2 + σ_N^2)\n= σ_N^2 · r^2/(r^2 + 1).",
"C": "The resulting minimum mean-square error is\nE[(X − a*Y)^2] = σ_X^2 σ_N^2 / (σ_X^2 + σ_N^2)\n= σ_N^2 · (σ_X/σ_N)^2 / \u001b[0m( (σ_X/σ_N)^2 + 1 )."
}
}
},

{
"source": "textbook",
"question": "The number X of goals the Bulldogs score against the Flames has a geometric distribution with mean 2; the number of goals Y that the Flames score against the Bulldogs is also geometrically distributed but with mean 4. (a) Find the pmf of Z = X − Y. Assume X and Y are independent. (b) What is the probability that the Bulldogs beat the Flames? Tie the Flames? (c) Find E[Z].",
"answer": "First identify the geometric parameters. For a geometric r.v. on {1,2,...} we have E[N]=1/p where p is the success probability. From E[X]=2 and E[Y]=4 we get p_X=1/2 and p_Y=1/4. Thus P(X=k)=(1/2)(1/2)^{k-1},k=1,2,... and P(Y=k)=(1/4)(3/4)^{k-1},k=1,2,... with X,Y independent.\n\n(a) We want the pmf of Z=X−Y. For any integer z,\nP(Z=z)=∑x P(X=x,Y=x−z)=∑x P(X=x)P(Y=x−z),\nwith constraints x≥1 and x−z≥1.\nCase z≥0: then x≥1+z, so\nP(Z=z)=∑{x=1+z}^∞(1/2)(1/2)^{x-1}(1/4)(3/4)^{x-z-1}\n= (1/8)(1/2)^z∑{j=0}^∞(1/2·3/4)^j\n= (1/8)(1/2)^z·1/(1−3/8)\n= (1/5)(1/2)^z, z=0,1,2,...\nCase z<0: write z=−k,k>0. Then x≥1, so\nP(Z=−k)=∑{x=1}^∞(1/2)(1/2)^{x+k-1}(1/4)(3/4)^{x-1}\n= (1/8)(3/4)^k∑{j=0}^∞(1/2·3/4)^j\n= (1/5)(3/4)^k, k=1,2,...\nSo the pmf of Z is\nP(Z=z)=\n{ (1/5)(1/2)^z, z=0,1,2,...;\n (1/5)(3/4)^{-z}, z=−1,−2,... }.\n\n(b) The Bulldogs beat the Flames when X>Y, i.e. Z>0. They tie when X=Y, i.e. Z=0.\nP(Bulldogs win)=P(Z>0)=∑{z=1}^∞(1/5)(1/2)^z=(1/5)·(1/2)/(1−1/2)=1/5.\nP(tie)=P(Z=0)=(1/5)(1/2)^0=1/5.\n(Consequently P(Flames win)=1−1/5−1/5=3/5, which also equals ∑{k=1}^∞(1/5)(3/4)^k.)\n\n(c) E[Z]=E[X−Y]=E[X]−E[Y]=2−4=−2. So on average the Flames score 2 more goals than the Bulldogs."
},

{
"source": "textbook",
"question": "Passengers arrive at an airport taxi stand every minute according to a Bernoulli random variable (each minute there is a passenger with probability p, independently of other minutes). A taxi will not leave until it has two passengers.\n(a) Find the pmf of the time T when the taxi has two passengers.\n(b) Find the pmf for the time that the first customer waits.",
"answer": "(a) Let T be the number of minutes from time 0 until the instant when the taxi has collected its second passenger. Each minute is a Bernoulli trial with success probability p (a passenger arrives). The taxi leaves when 2 successes have occurred, so T has a negative–binomial distribution with r = 2 successes:\n\nP(T = k) = C(k−1, 2−1) p^2 (1−p)^{k−2} = (k−1) p^2 (1−p)^{k−2}, k = 2,3,4,....\n\n(b) Let W be the time the first customer waits from their arrival until the taxi has two passengers (i.e., until the next passenger arrives). Once the first passenger has arrived, the future arrivals in subsequent minutes are again i.i.d. Bernoulli(p), independent of the past. Therefore the first customer's waiting time is the number of minutes until the next success and is geometric with parameter p:\n\nP(W = w) = (1−p)^{w−1} p, w = 1,2,3,...."
},

{
"source": "textbook",
"question": "Let X and Y be independent random variables that are uniformly distributed in the interval [0,1]. Find the pdf of Z = XY.",
"answer": "For 0<z<1,\nF_Z(z)=P(XY≤z)=∫∫_{xy≤z,0≤x,y≤1}dxdy.\nSplit the region: when 0≤x≤z, we can take 0≤y≤1; when z<x≤1, we have 0≤y≤z/x. Thus\nF_Z(z)=∫_0^z1dx+∫_z^1(z/x)dx=z+z∫_z^1(1/x)dx=z+z(ln1−lnz)=z−zlnz.\nDifferentiate to get the pdf:\n\nf_Z(z)=d/dzF_Z(z)=1−(lnz+1)=−lnz, 0<z<1.\n\nSince Z∈[0,1], f_Z(z)=0 for z≤0 or z≥1.\n\nTherefore the pdf is\n\nf_Z(z)=\n −lnz, 0<z<1,\n 0, otherwise."
},

{
"source": "textbook",
"question": "Let X1, X2, and X3 be independent and uniformly distributed in [−1,1]. (a) Find the cdf and pdf of Y = X1 + X2. (b) Find the cdf of Z = Y + X3.",
"answer": "For each Xi we have pdf f_X(x) = 1/2 for −1 ≤ x ≤ 1 and 0 otherwise.\n\n(a) Y = X1 + X2 is the sum of two i.i.d. U[−1,1] variables.\nThe pdf of Y is the convolution of f_X with itself:\n\nf_Y(y) = ∫_{−∞}^{∞} f_X(x) f_X(y − x) dx\n = 1/4 · length( [−1,1] ∩ [y − 1, y + 1] )\n = { (y + 2)/4, −2 ≤ y ≤ 0;\n (2 − y)/4, 0 < y ≤ 2;\n 0, otherwise }.\n\nEquivalently, f_Y(y) = (1/4)(2 − |y|) for |y| ≤ 2 and 0 otherwise.\n\nIntegrating gives the cdf F_Y(y):\n\nF_Y(y) = P(Y ≤ y) =\n 0, y ≤ −2;\n (y + 2)^2 / 8, −2 < y ≤ 0;\n 1 − (2 − y)^2 / 8, 0 < y ≤ 2;\n 1, y > 2.\n\n(b) Z = Y + X3 is the sum of three i.i.d. U[−1,1] variables. Let Ui = (Xi + 1)/2 ~ U[0,1]; then\n\nS = U1 + U2 + U3 and Z = 2S − 3.\n\nThe cdf of S (Irwin–Hall distribution with n = 3) is\n\nF_S(s) =\n 0, s ≤ 0;\n s^3 / 6, 0 < s ≤ 1;\n (−2s^3 + 9s^2 − 9s + 3)/6, 1 < s ≤ 2;\n (s^3 − 9s^2 + 27s − 21)/6, 2 < s ≤ 3;\n 1, s > 3.\n\nSince Z = 2S − 3, S = (z + 3)/2 and F_Z(z) = P(Z ≤ z) = F_S((z + 3)/2). Mapping the breakpoints s = 0,1,2,3 gives z = −3, −1, 1, 3. Therefore the cdf of Z is\n\nF_Z(z) =\n 0, z ≤ −3;\n (z + 3)^3 / 48, −3 < z ≤ −1;\n 1/2 + (3z)/8 − z^3/24, −1 < z ≤ 1;\n z^3/48 − 3z^2/16 + 9z/16 + 7/16, 1 < z ≤ 3;\n 1, z > 3.\n\n(These pieces match continuously at z = −3, −1, 1, 3 and give the full cdf of Z on [−3,3].)"
},

{
"source": "textbook",
"question": "Signals X and Y are independent. X is exponentially distributed with mean 1 and Y is exponentially distributed with mean 1. (a) Find the cdf of Z = |X − Y|. (b) Use the result of part a to find E[Z].",
"answer": "Since X and Y are independent Exp(1), the difference D = X − Y has the Laplace (double exponential) distribution with pdf\n\nf_D(d) = 1/2 · e^{−|d|}, −∞ < d < ∞.\n\nBecause Z = |X − Y| = |D| and Z ≥ 0, its pdf is\n\nf_Z(z) = f_D(z) + f_D(−z) = (1/2)e^{−z} + (1/2)e^{−z} = e^{−z}, z ≥ 0,\n\nand f_Z(z) = 0 for z < 0.\n\nTherefore the cdf of Z is\n\nF_Z(z) = P(Z ≤ z) =\n 0, z ≤ 0,\n 1 − e^{−z}, z > 0.\n\nThis is the cdf of an Exp(1) random variable, so the mean of Z is\n\nE[Z] = 1."
},

{
"source": "textbook",
"question": "Let X and Y be jointly Gaussian random variables with E[Y] = 0, σ₁ = 1, σ₂ = 2, and E[X|Y] = Y/4 + 1. Find the joint pdf of X and Y.",
"answer": "For a bivariate normal (X, Y) with means μ₁, μ₂, standard deviations σ₁, σ₂ and correlation ρ,\nE[X|Y = y] = μ₁ + ρ (σ₁/σ₂)(y − μ₂).\nWe are given E[Y] = μ₂ = 0, σ₁ = 1, σ₂ = 2 and\nE[X|Y = y] = y/4 + 1.\nMatching terms,\nμ₁ = 1,\nρ (σ₁/σ₂) = 1/4.\nSince σ₁/σ₂ = 1/2, we get ρ (1/2) = 1/4 ⇒ ρ = 1/2.\nThus the parameters of the bivariate normal are\nμ₁ = 1, μ₂ = 0, σ₁ = 1, σ₂ = 2, ρ = 1/2.\nThe joint pdf of a bivariate normal is\nf_{X,Y}(x,y) = 1 / (2π σ₁ σ₂ √(1 − ρ²)) · exp{ −1 / [2(1 − ρ²)] [ (x − μ₁)²/σ₁² − 2ρ(x − μ₁)(y − μ₂)/(σ₁σ₂) + (y − μ₂)²/σ₂² ] }.\nSubstituting σ₁ = 1, σ₂ = 2, μ₁ = 1, μ₂ = 0, ρ = 1/2:\n1 − ρ² = 1 − (1/2)² = 3/4, so √(1 − ρ²) = √3/2,\nσ₁σ₂ = 2.\nHence the normalizing constant is\n2π σ₁ σ₂ √(1 − ρ²) = 2π · 2 · (√3/2) = 2π√3.\nInside the exponent,\n−1 / [2(1 − ρ²)] = −1 / (2 · 3/4) = −2/3,\nand the quadratic form becomes\n(x − 1)²/1² − 2·(1/2)(x − 1)y/(1 · 2) + y²/2² = (x − 1)² − (1/2)(x − 1)y + y²/4.\nTherefore the joint pdf of X and Y is\nf_{X,Y}(x,y) = 1/(2π√3) · exp{ −(2/3) [ (x − 1)² − (1/2)(x − 1)y + y²/4 ] }, for all real x, y."
},

{
"source": "textbook",
"question": "Let h(x, y) be a joint Gaussian pdf for zero-mean, unit-variance Gaussian random variables with correlation coefficient ρ₁. Let g(x, y) be a joint Gaussian pdf for zero-mean, unit-variance Gaussian random variables with correlation coefficient ρ₂ ≠ ρ₁. Suppose the random variables X and Y have joint pdf\n\nf_{X,Y}(x, y) = {h(x, y) + g(x, y)}/2.\n\n(a) Find the marginal pdf for X and for Y.\n(b) Explain why X and Y are not jointly Gaussian random variables.",
"answer": "(a) For each of the bivariate Gaussian pdfs h and g we are told that the marginals are zero-mean, unit-variance Gaussians. Denote the standard normal pdf by\nϕ(t) = (1/√(2π)) e^{−t²/2}.\nThus\n∫{−∞}^{∞} h(x, y) dy = ϕ(x), ∫{−∞}^{∞} g(x, y) dy = ϕ(x),\nand similarly in the other coordinate\n∫{−∞}^{∞} h(x, y) dx = ϕ(y), ∫{−∞}^{∞} g(x, y) dx = ϕ(y).\nTherefore the marginal pdf of X is\nf_X(x) = ∫{−∞}^{∞} f{X,Y}(x, y) dy\n = ∫{−∞}^{∞} [h(x, y) + g(x, y)]/2 dy\n = (1/2)ϕ(x) + (1/2)ϕ(x)\n = ϕ(x),\nso X ∼ N(0, 1). By symmetry the marginal pdf of Y is\nf_Y(y) = ∫{−∞}^{∞} f_{X,Y}(x, y) dx\n = (1/2)ϕ(y) + (1/2)ϕ(y)\n = ϕ(y),\nso Y ∼ N(0, 1) as well.\n\n(b) If (X, Y) were jointly Gaussian, then any linear combination aX + bY would have a univariate Gaussian distribution. Under the given model, (X, Y) is a mixture of two different bivariate Gaussians: with probability 1/2 it has joint pdf h (correlation ρ₁) and with probability 1/2 it has joint pdf g (correlation ρ₂ ≠ ρ₁).\n\nConsider the sum U = X + Y. Conditional on the first component (with pdf h), U is Gaussian with mean 0 and variance\nVar(U | h) = Var(X + Y | h) = 2 + 2ρ₁.\nConditional on the second component (with pdf g), U is Gaussian with mean 0 and variance\nVar(U | g) = 2 + 2ρ₂.\nSince ρ₁ ≠ ρ₂, these variances are different. Hence the unconditional pdf of U is the mixture\nf_U(u) = (1/2) ϕ_{σ₁²}(u) + (1/2) ϕ_{σ₂²}(u),\nwhere ϕ_{σ²} is the N(0, σ²) density and σ₁² = 2 + 2ρ₁, σ₂² = 2 + 2ρ₂ with σ₁² ≠ σ₂². A mixture of two Gaussian densities with different variances is not itself Gaussian. Therefore U is not Gaussian.\n\nSince there exists a linear combination of X and Y (namely X + Y) that is not Gaussian, (X, Y) cannot be jointly Gaussian, even though each marginal is N(0, 1)."
},

{
"source": "textbook",
"question": "Use conditional expectation to show that for X and Y zero-mean, jointly Gaussian random variables,\nE[X^2 Y^2] = E[X^2]E[Y^2] + 2(E[XY])^2.",
"answer": "Let X and Y be zero-mean, jointly Gaussian.\n\nWrite\nσ_X^2 = E[X^2], σ_Y^2 = E[Y^2], c = E[XY].\n\nBecause (X, Y) is jointly Gaussian, the conditional distribution of X given Y is Gaussian with\n\n E[X | Y] = aY, where a = c / σ_Y^2,\n Var(X | Y) = σ_X^2 − a^2 σ_Y^2 = σ_X^2 − c^2 / σ_Y^2.\n\nHence\n E[X^2 | Y] = Var(X | Y) + (E[X | Y])^2\n = σ_X^2 − c^2 / σ_Y^2 + a^2 Y^2\n = σ_X^2 − c^2 / σ_Y^2 + (c^2 / σ_Y^4) Y^2.\n\nNow use conditional expectation:\n\n E[X^2 Y^2] = E[ E[X^2 Y^2 | Y] ]\n = E[ Y^2 E[X^2 | Y] ]\n = E[ Y^2 ( σ_X^2 − c^2 / σ_Y^2 + (c^2 / σ_Y^4) Y^2 ) ].\n\nCompute the expectations term by term:\n\n1. E[Y^2] = σ_Y^2.\n2. For zero-mean Gaussian Y, E[Y^4] = 3σ_Y^4.\n\nThus\n\n E[X^2 Y^2]\n = σ_X^2 E[Y^2] − (c^2 / σ_Y^2) E[Y^2] + (c^2 / σ_Y^4) E[Y^4]\n = σ_X^2 σ_Y^2 − c^2 + (c^2 / σ_Y^4) · 3σ_Y^4\n = σ_X^2 σ_Y^2 − c^2 + 3c^2\n = σ_X^2 σ_Y^2 + 2c^2.\n\nFinally, recalling σ_X^2 = E[X^2], σ_Y^2 = E[Y^2], and c = E[XY], we obtain\n\n E[X^2 Y^2] = E[X^2] E[Y^2] + 2 (E[XY])^2,\n\nas required."
},

{
"source": "textbook",
"question": "Let Z = 3X − 7Y, where X and Y are independent random variables. (a) Find the characteristic function of Z. (b) Find the mean and variance of Z by taking derivatives of the characteristic function found in part a.",
"answer": "(a) Let ϕ_X(t) and ϕ_Y(t) be the characteristic functions of X and Y, respectively. For independent variables and a linear combination Z = aX + bY we have\nϕ_Z(t) = ϕ_X(a t) ϕ_Y(b t).\nHere a = 3 and b = −7, so\n ϕ_Z(t) = ϕ_X(3t) ϕ_Y(−7t).\n\n(b) Recall that for any r.v. W with characteristic function ϕ_W(t):\n E[W] = (1/i) ϕ'_W(0), E[W^2] = −ϕ''_W(0), Var(W) = E[W^2] − (E[W])^2.\nLet μ_X = E[X], μ_Y = E[Y], σ_X^2 = Var(X), σ_Y^2 = Var(Y).\n\nFirst derivative of ϕ_Z(t):\n ϕ'_Z(t) = 3 ϕ'_X(3t) ϕ_Y(−7t) − 7 ϕ_X(3t) ϕ'_Y(−7t).\nEvaluated at t = 0, using ϕ_X(0) = ϕ_Y(0) = 1,\n ϕ'_Z(0) = 3 ϕ'_X(0) − 7 ϕ'_Y(0) = 3 i μ_X − 7 i μ_Y = i(3μ_X − 7μ_Y).\nHence\n E[Z] = (1/i) ϕ'_Z(0) = 3μ_X − 7μ_Y.\n\nSecond derivative of ϕ_Z(t):\n ϕ''_Z(t) = 9 ϕ''_X(3t) ϕ_Y(−7t) + 49 ϕ_X(3t) ϕ''_Y(−7t) − 42 ϕ'_X(3t) ϕ'_Y(−7t).\nAt t = 0, using ϕ''_X(0) = −E[X^2], ϕ''_Y(0) = −E[Y^2], ϕ'_X(0) = iμ_X, ϕ'_Y(0) = iμ_Y,\n ϕ''_Z(0) = −9E[X^2] − 49E[Y^2] + 42 μ_X μ_Y.\nThus\n E[Z^2] = −ϕ''_Z(0) = 9E[X^2] + 49E[Y^2] − 42 μ_X μ_Y.\n\nTherefore\n Var(Z) = E[Z^2] − (E[Z])^2\n = [9E[X^2] + 49E[Y^2] − 42 μ_X μ_Y] − (3μ_X − 7μ_Y)^2\n = 9(E[X^2] − μ_X^2) + 49(E[Y^2] − μ_Y^2)\n = 9σ_X^2 + 49σ_Y^2.\n\nSo,\n E[Z] = 3E[X] − 7E[Y],\n Var(Z) = 9 Var(X) + 49 Var(Y)."
},

{
"source": "textbook",
"question": "Use the fact that E[(tX + Y)^2] ≥ 0 for all real t to prove the Cauchy–Schwarz inequality (E[XY])^2 ≤ E[X^2]E[Y^2].",
"answer": "Define the function\nf(t) = E[(tX + Y)^2], for t ∈ ℝ.\n\nExpanding the square inside the expectation gives a quadratic in t:\n\nf(t) = E[t^2X^2 + 2tXY + Y^2]\n = t^2 E[X^2] + 2t E[XY] + E[Y^2].\n\nBy assumption, (tX + Y)^2 ≥ 0 almost surely, so its expectation is non-negative for all t:\n\nf(t) = t^2 E[X^2] + 2t E[XY] + E[Y^2] ≥ 0 for all t ∈ ℝ.\n\nNow view f(t) as a quadratic polynomial in t:\n\na t^2 + b t + c with a = E[X^2], b = 2E[XY], c = E[Y^2].\n\nA quadratic at^2 + bt + c is non-negative for all real t if and only if its discriminant is non-positive:\n\nb^2 − 4ac ≤ 0.\n\nSo here we must have\n\n(2E[XY])^2 − 4E[X^2]E[Y^2] ≤ 0.\n\nDividing by 4 yields\n\n(E[XY])^2 ≤ E[X^2]E[Y^2].\n\nThis is exactly the Cauchy–Schwarz inequality for random variables X and Y."
}

]
