"""
æ¦‚ç‡è®º QA ç³»ç»Ÿ - Gradio å‰ç«¯ç•Œé¢
æ•´åˆ RAG + GRPO + æ¨¡å‹å¯¹æ¯”åŠŸèƒ½
"""
import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import json
import time
from pathlib import Path
import sys

# æ·»åŠ è·¯å¾„
sys.path.append('.')
from src.vector_database import VectorDatabase

class ProbabilityQASystem:
    """æ¦‚ç‡è®º QA ç³»ç»Ÿ"""

    def __init__(self):
        self.tokenizer = None
        self.base_model = None
        self.grpo_model = None
        self.vector_db = None
        self.is_loaded = False

    def load_models(self, progress=gr.Progress()):
        """åŠ è½½æ¨¡å‹å’Œç³»ç»Ÿç»„ä»¶"""
        if self.is_loaded:
            return "âœ… ç³»ç»Ÿå·²åŠ è½½"

        progress(0, desc="åˆå§‹åŒ–ä¸­...")

        # 1. åŠ è½½ tokenizer
        progress(0.2, desc="åŠ è½½ Tokenizer...")
        model_name = "Qwen/Qwen2.5-Math-7B-Instruct"
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side='left'
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token

        # 2. åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆ4-bitï¼‰
        progress(0.4, desc="åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆ4-bité‡åŒ–ï¼‰...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True
        )

        self.base_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        )

        # 3. åŠ è½½ GRPO æ¨¡å‹
        progress(0.6, desc="åŠ è½½ GRPO å¾®è°ƒæ¨¡å‹...")
        grpo_path = "outputs/grpo/final_model"
        if Path(grpo_path).exists():
            self.grpo_model = PeftModel.from_pretrained(self.base_model, grpo_path)
        else:
            return "âŒ GRPO æ¨¡å‹æœªæ‰¾åˆ°"

        # 4. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        progress(0.8, desc="åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
        self.vector_db = VectorDatabase(
            db_path="./data/chroma_db",
            embedding_model="BAAI/bge-base-en-v1.5"
        )
        self.vector_db.initialize()

        progress(1.0, desc="åŠ è½½å®Œæˆï¼")
        self.is_loaded = True

        db_info = self.vector_db.get_collection_info()
        return f"""âœ… **ç³»ç»ŸåŠ è½½æˆåŠŸï¼**

ğŸ“Š **ç³»ç»Ÿä¿¡æ¯**:
- åŸºç¡€æ¨¡å‹: Qwen2.5-Math-7B-Instruct (4-bit)
- å¾®è°ƒæ¨¡å‹: GRPO (å¼ºåŒ–å­¦ä¹ å¯¹é½)
- çŸ¥è¯†åº“: {db_info.get('count', 0)} ä¸ªæ–‡æ¡£ç‰‡æ®µ
- åµŒå…¥æ¨¡å‹: BGE-base-en-v1.5

ğŸ‰ ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†ï¼"""

    def retrieve_context(self, query, top_k=3):
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        if not self.vector_db:
            return [], []

        results = self.vector_db.search(query, n_results=top_k)
        contexts = [r['text'] for r in results]
        return contexts, results

    def generate_answer(self, model, query, contexts=None, max_length=512):
        """ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»º prompt
        if contexts:
            context_text = "\n\n".join([f"å‚è€ƒ {i+1}: {ctx}" for i, ctx in enumerate(contexts)])
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¦‚ç‡è®ºå­¦ä¹ åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜ã€‚

å‚è€ƒèµ„æ–™:
{context_text}

é—®é¢˜: {query}

è¦æ±‚:
1. åŸºäºå‚è€ƒèµ„æ–™æä¾›å‡†ç¡®çš„æ•°å­¦ç­”æ¡ˆ
2. åŒ…å«å¿…è¦çš„å…¬å¼æ¨å¯¼å’Œè¯æ˜æ­¥éª¤
3. ä½¿ç”¨ä¸¥è°¨çš„æ•°å­¦è¯­è¨€å’Œç¬¦å·

ç­”æ¡ˆ:"""
        else:
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¦‚ç‡è®ºå­¦ä¹ åŠ©æ‰‹ã€‚

é—®é¢˜: {query}

è¦æ±‚:
1. æä¾›å‡†ç¡®çš„æ•°å­¦å®šä¹‰å’Œè§£ç­”
2. åŒ…å«æ¸…æ™°çš„æ¨å¯¼è¿‡ç¨‹å’Œè¯æ˜æ­¥éª¤
3. ä½¿ç”¨æ ‡å‡†çš„æ•°å­¦ç¬¦å·å’Œä¸¥è°¨çš„è¡¨è¾¾

ç­”æ¡ˆ:"""

        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–ç­”æ¡ˆéƒ¨åˆ†
        if "ç­”æ¡ˆ:" in generated_text:
            answer = generated_text.split("ç­”æ¡ˆ:")[-1].strip()
        else:
            answer = generated_text[len(prompt):].strip()

        return answer

    def answer_question(self, question, use_rag=True, progress=gr.Progress()):
        """å›ç­”é—®é¢˜ï¼ˆä¸»ç•Œé¢ï¼‰"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆç‚¹å‡»'åŠ è½½ç³»ç»Ÿ'æŒ‰é’®åˆå§‹åŒ–ç³»ç»Ÿ", "", ""

        if not question.strip():
            return "âš ï¸ è¯·è¾“å…¥é—®é¢˜", "", ""

        start_time = time.time()

        # æ£€ç´¢ä¸Šä¸‹æ–‡
        contexts = []
        retrieval_info = ""
        if use_rag:
            progress(0.3, desc="æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
            contexts, results = self.retrieve_context(question, top_k=3)

            retrieval_info = "### ğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£\n\n"
            for i, (ctx, result) in enumerate(zip(contexts, results), 1):
                source = result.get('metadata', {}).get('source', 'unknown')
                distance = result.get('distance', 0)
                relevance = max(0, (1 - distance) * 100)
                retrieval_info += f"**[{i}] ç›¸å…³åº¦: {relevance:.1f}%** | æ¥æº: {source}\n\n"
                retrieval_info += f"```\n{ctx[:300]}...\n```\n\n"

        # ç”Ÿæˆç­”æ¡ˆ
        progress(0.7, desc="ç”Ÿæˆç­”æ¡ˆä¸­...")
        answer = self.generate_answer(
            self.grpo_model,
            question,
            contexts if use_rag else None
        )

        elapsed_time = time.time() - start_time

        # æ ¼å¼åŒ–è¾“å‡º
        answer_text = f"""### ğŸ’¡ å›ç­”

{answer}

---
â±ï¸ ç”Ÿæˆæ—¶é—´: {elapsed_time:.2f}ç§’ | {"ğŸ” ä½¿ç”¨ RAG å¢å¼º" if use_rag else "ğŸš€ ç›´æ¥ç”Ÿæˆ"}
"""

        return answer_text, retrieval_info, f"âœ… å›ç­”å®Œæˆï¼ˆè€—æ—¶ {elapsed_time:.2f}ç§’ï¼‰"

    def compare_models(self, question, progress=gr.Progress()):
        """å¯¹æ¯”ä¸åŒæ¨¡å‹"""
        if not self.is_loaded:
            return "âŒ è¯·å…ˆåŠ è½½ç³»ç»Ÿ", "", "", ""

        if not question.strip():
            return "âš ï¸ è¯·è¾“å…¥é—®é¢˜", "", "", ""

        progress(0.2, desc="æ£€ç´¢æ–‡æ¡£...")
        contexts, _ = self.retrieve_context(question, top_k=3)

        # 1. Base Model (æ—  RAG)
        progress(0.3, desc="Base Model ç”Ÿæˆä¸­...")
        base_answer = self.generate_answer(self.base_model, question, None, max_length=256)

        # 2. GRPO Model (æ—  RAG)
        progress(0.5, desc="GRPO Model ç”Ÿæˆä¸­...")
        grpo_no_rag = self.generate_answer(self.grpo_model, question, None, max_length=256)

        # 3. GRPO + RAG
        progress(0.7, desc="GRPO + RAG ç”Ÿæˆä¸­...")
        grpo_with_rag = self.generate_answer(self.grpo_model, question, contexts, max_length=256)

        # æ ¼å¼åŒ–è¾“å‡º
        base_output = f"""### ğŸ”µ Base Modelï¼ˆæœªå¾®è°ƒï¼‰

{base_answer}
"""

        grpo_output = f"""### ğŸŸ¢ GRPO Modelï¼ˆå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼‰

{grpo_no_rag}
"""

        rag_output = f"""### ğŸŒŸ GRPO + RAGï¼ˆæœ€ä½³é…ç½®ï¼‰

{grpo_with_rag}
"""

        progress(1.0, desc="å¯¹æ¯”å®Œæˆ!")
        return base_output, grpo_output, rag_output, "âœ… ä¸‰ä¸ªæ¨¡å‹å¯¹æ¯”å®Œæˆ"

    def get_system_stats(self):
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_loaded:
            return "ç³»ç»ŸæœªåŠ è½½"

        # è·å–æ•°æ®åº“ä¿¡æ¯
        db_info = self.vector_db.get_collection_info()

        # åŠ è½½è¯„ä¼°ç»“æœ
        eval_file = "evaluation_results.json"
        if Path(eval_file).exists():
            with open(eval_file, 'r', encoding='utf-8') as f:
                eval_results = json.load(f)

            stats_text = f"""## ğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

### æ¨¡å‹é…ç½®
- **åŸºç¡€æ¨¡å‹**: Qwen2.5-Math-7B-Instruct
- **é‡åŒ–æ–¹å¼**: 4-bit NF4
- **å¾®è°ƒæ–¹æ³•**: LoRA (Rank 16) + GRPO
- **è®­ç»ƒæ•°æ®**: 81 ä¸ªæ¦‚ç‡è®º QA å¯¹
- **è®­ç»ƒæ—¶é•¿**: 3.5 å°æ—¶

### çŸ¥è¯†åº“ä¿¡æ¯
- **æ–‡æ¡£æ•°é‡**: {db_info.get('count', 0)} ä¸ªç‰‡æ®µ
- **åµŒå…¥ç»´åº¦**: {db_info.get('embedding_dim', 'N/A')}
- **åµŒå…¥æ¨¡å‹**: BGE-base-en-v1.5

### æ¨¡å‹æ€§èƒ½ï¼ˆæµ‹è¯•é›†è¯„ä¼°ï¼‰

| æ¨¡å‹ | Average Reward | vs Base æå‡ |
|------|---------------|--------------|
"""
            for result in eval_results:
                model_name = result['model_name']
                avg_reward = result['avg_reward']
                base_reward = eval_results[0]['avg_reward']
                improvement = ((avg_reward - base_reward) / base_reward * 100) if model_name != 'Base Model' else 0

                if model_name == 'Base Model':
                    stats_text += f"| {model_name} | {avg_reward:.4f} | - |\n"
                else:
                    stats_text += f"| {model_name} | {avg_reward:.4f} | **+{improvement:.1f}%** |\n"

            stats_text += f"""
### å…³é”®å‘ç°
- âœ… **SVD åˆå§‹åŒ–** ç›¸æ¯”éšæœºåˆå§‹åŒ–æå‡ **19.5%**
- âœ… **GRPO å¯¹é½** å®ç° **55.5%** æ•´ä½“æ€§èƒ½æå‡
- âœ… **å®Œæ•´ Pipeline** è¾¾åˆ°æœ€ä½³æ•ˆæœ

### æŠ€æœ¯æ ˆ
- ğŸ”§ Framework: PyTorch + Transformers + PEFT
- ğŸ—„ï¸ Vector DB: ChromaDB
- ğŸ¨ Frontend: Gradio
- ğŸ“Š Visualization: Matplotlib
"""
        else:
            stats_text = f"""## ğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

### çŸ¥è¯†åº“ä¿¡æ¯
- **æ–‡æ¡£æ•°é‡**: {db_info.get('count', 0)} ä¸ªç‰‡æ®µ
- **åµŒå…¥æ¨¡å‹**: BGE-base-en-v1.5

### æ¨¡å‹é…ç½®
- **åŸºç¡€æ¨¡å‹**: Qwen2.5-Math-7B-Instruct (4-bit)
- **å¾®è°ƒæ¨¡å‹**: GRPO (å¼ºåŒ–å­¦ä¹ å¯¹é½)

_æ›´å¤šç»Ÿè®¡ä¿¡æ¯å°†åœ¨è¯„ä¼°å®Œæˆåæ˜¾ç¤º_
"""

        return stats_text


# åˆå§‹åŒ–ç³»ç»Ÿ
qa_system = ProbabilityQASystem()

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="æ¦‚ç‡è®ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ") as demo:

    # é¡µé¢æ ‡é¢˜
    gr.Markdown("""
    <div class="main-header">
        <h1>ğŸ“ æ¦‚ç‡è®ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ</h1>
        <p>åŸºäº RAG + GRPO çš„æ™ºèƒ½æ•°å­¦å­¦ä¹ åŠ©æ‰‹</p>
        <p style="font-size: 14px; opacity: 0.9;">
            Qwen2.5-Math-7B + SVD-LoRA + GRPO + ChromaDB
        </p>
    </div>
    """)

    # ç³»ç»ŸçŠ¶æ€
    with gr.Row():
        load_btn = gr.Button("ğŸš€ åŠ è½½ç³»ç»Ÿ", variant="primary", size="lg")
        status_text = gr.Textbox(
            label="ç³»ç»ŸçŠ¶æ€",
            value="â³ ç‚¹å‡»'åŠ è½½ç³»ç»Ÿ'æŒ‰é’®å¼€å§‹åˆå§‹åŒ–...",
            interactive=False,
            lines=8
        )

    # ä¸»è¦åŠŸèƒ½æ ‡ç­¾é¡µ
    with gr.Tabs():
        # Tab 1: ä¸»é—®ç­”ç³»ç»Ÿ
        with gr.Tab("ğŸ’¬ æ™ºèƒ½é—®ç­”"):
            gr.Markdown("### å‘æˆ‘æé—®ä»»ä½•æ¦‚ç‡è®ºç›¸å…³çš„é—®é¢˜ï¼")

            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="ğŸ“ æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æ¡ä»¶æ¦‚ç‡ï¼Ÿä¸­å¿ƒæé™å®šç†çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                        lines=3
                    )
                    use_rag_checkbox = gr.Checkbox(
                        label="ğŸ” ä½¿ç”¨ RAG å¢å¼ºï¼ˆæ¨èï¼‰",
                        value=True,
                        info="å¯ç”¨åä¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥è¾…åŠ©å›ç­”"
                    )
                    submit_btn = gr.Button("ğŸš€ è·å–ç­”æ¡ˆ", variant="primary", size="lg")

                with gr.Column(scale=1):
                    qa_status = gr.Textbox(label="çŠ¶æ€", interactive=False, lines=2)

            answer_output = gr.Markdown(label="ğŸ’¡ å›ç­”")
            retrieval_output = gr.Markdown(label="ğŸ“š æ£€ç´¢åˆ°çš„æ–‡æ¡£")

        # Tab 2: æ¨¡å‹å¯¹æ¯”
        with gr.Tab("ğŸ“Š æ¨¡å‹å¯¹æ¯”"):
            gr.Markdown("""
            ### å¯¹æ¯”ä¸åŒæ¨¡å‹çš„å›ç­”æ•ˆæœ
            åŒæ—¶ç”Ÿæˆ Base Modelã€GRPO Model å’Œ GRPO+RAG çš„ç­”æ¡ˆï¼Œç›´è§‚æ¯”è¾ƒæ€§èƒ½å·®å¼‚ã€‚
            """)

            compare_question = gr.Textbox(
                label="ğŸ“ æµ‹è¯•é—®é¢˜",
                placeholder="è¾“å…¥é—®é¢˜æ¥å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è¡¨ç°...",
                lines=2
            )
            compare_btn = gr.Button("ğŸ”¬ å¼€å§‹å¯¹æ¯”", variant="primary", size="lg")
            compare_status = gr.Textbox(label="å¯¹æ¯”çŠ¶æ€", interactive=False, lines=1)

            with gr.Row():
                with gr.Column():
                    base_output = gr.Markdown(label="Base Model")
                with gr.Column():
                    grpo_output = gr.Markdown(label="GRPO Model")
                with gr.Column():
                    rag_output = gr.Markdown(label="GRPO + RAG")

        # Tab 3: ç³»ç»Ÿä¿¡æ¯
        with gr.Tab("ğŸ“ˆ ç³»ç»Ÿç»Ÿè®¡"):
            gr.Markdown("### æŸ¥çœ‹ç³»ç»Ÿé…ç½®å’Œæ€§èƒ½æŒ‡æ ‡")
            refresh_stats_btn = gr.Button("ğŸ”„ åˆ·æ–°ç»Ÿè®¡", variant="secondary")
            stats_output = gr.Markdown(value="ç‚¹å‡»'åˆ·æ–°ç»Ÿè®¡'æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯...")

        # Tab 4: å…³äº
        with gr.Tab("â„¹ï¸ å…³äº"):
            gr.Markdown("""
            ## ğŸ“ æ¦‚ç‡è®ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ

            ### ç³»ç»Ÿæ¶æ„

            ```
            ç”¨æˆ·é—®é¢˜ â†’ RAGæ£€ç´¢ â†’ ä¸Šä¸‹æ–‡å¢å¼º â†’ GRPOæ¨¡å‹ â†’ é«˜è´¨é‡ç­”æ¡ˆ
                â†“           â†“              â†“            â†“
            ChromaDB   BGEåµŒå…¥      Qwen2.5-Math   LoRAå¾®è°ƒ
            ```

            ### æ ¸å¿ƒæŠ€æœ¯

            1. **RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)**
               - ä½¿ç”¨ ChromaDB å‘é‡æ•°æ®åº“
               - BGE-base-en-v1.5 åµŒå…¥æ¨¡å‹
               - è¯­ä¹‰æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ

            2. **SVD-LoRA åˆå§‹åŒ–**
               - SVDåˆ†è§£æå–ä½ç§©ç»“æ„
               - æ™ºèƒ½åˆå§‹åŒ– LoRA æƒé‡
               - ç›¸æ¯”éšæœºåˆå§‹åŒ–æå‡ 19.5%

            3. **GRPO (Group Relative Policy Optimization)**
               - å¼ºåŒ–å­¦ä¹ å¯¹é½
               - å¯å‘å¼å¥–åŠ±æ¨¡å‹
               - æ•´ä½“æ€§èƒ½æå‡ 55.5%

            ### è®­ç»ƒæ•°æ®
            - 81 ä¸ªé«˜è´¨é‡æ¦‚ç‡è®º QA å¯¹
            - æ¶µç›–æµ‹åº¦è®ºã€éšæœºè¿‡ç¨‹ã€æ¦‚ç‡åŸºç¡€
            - è®­ç»ƒæ—¶é•¿ï¼š3.5 å°æ—¶

            ### æ€§èƒ½æŒ‡æ ‡
            | æ¨¡å‹ | æµ‹è¯•é›†æ€§èƒ½ | æå‡ |
            |------|-----------|------|
            | Base Model | 0.231 | - |
            | SFT Random | 0.253 | +9.5% |
            | SFT SVD | 0.302 | +30.8% |
            | **GRPO** | **0.359** | **+55.5%** |

            ### å¼€å‘å›¢é˜Ÿ
            - ğŸ« è¯¾ç¨‹ï¼šEE510 æ¦‚ç‡è®º
            - ğŸ“… å­¦æœŸï¼šSpring 2025
            - ğŸ”§ æŠ€æœ¯æ ˆï¼šPyTorch, Transformers, PEFT, Gradio, ChromaDB

            ---

            <div style="text-align: center; padding: 20px; color: #666;">
                <p>ğŸ’¡ <strong>æç¤º</strong>: ä½¿ç”¨ RAG å¢å¼ºå¯ä»¥è·å¾—æ›´å‡†ç¡®ã€æ›´æœ‰ä¾æ®çš„ç­”æ¡ˆ</p>
                <p>âš¡ <strong>æ€§èƒ½</strong>: é¦–æ¬¡åŠ è½½éœ€è¦ 1-2 åˆ†é’Ÿï¼Œåç»­æŸ¥è¯¢ 2-5 ç§’</p>
            </div>
            """)

    # ç¤ºä¾‹é—®é¢˜
    gr.Examples(
        examples=[
            "ä»€ä¹ˆæ˜¯æ¡ä»¶æ¦‚ç‡ï¼Ÿ",
            "ä¸­å¿ƒæé™å®šç†çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "è§£é‡Šé©¬å°”å¯å¤«æ€§è´¨",
            "ä»€ä¹ˆæ˜¯Ïƒä»£æ•°ï¼Ÿ",
            "å¸ƒæœ—è¿åŠ¨æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
            "é…çš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
        ],
        inputs=question_input,
        label="ğŸ’¡ ç¤ºä¾‹é—®é¢˜"
    )

    # äº‹ä»¶ç»‘å®š
    load_btn.click(
        fn=qa_system.load_models,
        outputs=[status_text]
    )

    submit_btn.click(
        fn=qa_system.answer_question,
        inputs=[question_input, use_rag_checkbox],
        outputs=[answer_output, retrieval_output, qa_status]
    )

    compare_btn.click(
        fn=qa_system.compare_models,
        inputs=[compare_question],
        outputs=[base_output, grpo_output, rag_output, compare_status]
    )

    refresh_stats_btn.click(
        fn=qa_system.get_system_stats,
        outputs=[stats_output]
    )

if __name__ == "__main__":
    print("="*80)
    print("ğŸš€ å¯åŠ¨æ¦‚ç‡è®ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
    print("="*80)
    print("\nğŸ“ ç³»ç»ŸåŠŸèƒ½:")
    print("  1. ğŸ’¬ æ™ºèƒ½é—®ç­” - RAGå¢å¼ºçš„æ¦‚ç‡è®ºé—®ç­”")
    print("  2. ğŸ“Š æ¨¡å‹å¯¹æ¯” - å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æ€§èƒ½")
    print("  3. ğŸ“ˆ ç³»ç»Ÿç»Ÿè®¡ - æŸ¥çœ‹è®­ç»ƒå’Œè¯„ä¼°æ•°æ®")
    print("\nâš ï¸  é¦–æ¬¡ä½¿ç”¨è¯·å…ˆç‚¹å‡»'åŠ è½½ç³»ç»Ÿ'æŒ‰é’®åˆå§‹åŒ–")
    print("\nğŸŒ è®¿é—®åœ°å€: http://127.0.0.1:7860")
    print("="*80)

    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        show_error=True
    )
