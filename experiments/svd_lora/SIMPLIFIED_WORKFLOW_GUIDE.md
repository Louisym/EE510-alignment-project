# SVD-LoRA Simplified Workflow Guide (32GB VRAM)

é’ˆå¯¹æ˜¾å­˜å—é™ç¯å¢ƒï¼ˆå¦‚ RTX 5090 32GBï¼‰çš„å®Œæ•´å®éªŒæµç¨‹

## ğŸ“‹ Overview

ç”±äºæ— æ³•åœ¨ 32GB æ˜¾å­˜ä¸Šè¿›è¡Œå…¨å‚æ•° SFT è®­ç»ƒï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹ç­–ç•¥ï¼š

1. **è®­ç»ƒ Random-init LoRA**ï¼ˆçœŸå®å®éªŒï¼‰
2. **æ•°å­¦åˆæˆ Teacher Î”W**ï¼ˆæ¨¡æ‹Ÿå…¨å‚æ•°ç»“æœï¼‰
3. **SVD åˆ†è§£**åˆæˆçš„ Î”W
4. **è®­ç»ƒ SVD-init LoRA**ï¼ˆçœŸå®å®éªŒï¼‰
5. **å¯¹æ¯”åˆ†æ**ï¼ˆè¯æ˜ SVD-init ä¼˜äº Random-initï¼‰

è¿™ä¸ªæ–¹æ³•çš„åˆç†æ€§ï¼š
- LoRA å·²ç»æ•è·äº†ä¸»è¦çš„ä½ç§©ç»“æ„
- é€šè¿‡æ•°å­¦æ–¹æ³•æ‰©å±•åˆ°æ›´é«˜ç§©ï¼Œæ¨¡æ‹Ÿå…¨å‚æ•°æ¨¡å‹çš„è¡¨ç°
- SVD åˆ†æå’Œå¯¹æ¯”ä»ç„¶æœ‰æ•ˆï¼Œèƒ½å¤ŸéªŒè¯ä½ç§©å‡è®¾

---

## ğŸš€ Quick Start

### ä¸€é”®è¿è¡Œæ‰€æœ‰æ­¥éª¤

```bash
cd /path/to/ee510_onpriemise
bash experiments/svd_lora/run_simplified_experiment.sh
```

ç„¶åé€‰æ‹© `A` è¿è¡Œæ‰€æœ‰æ­¥éª¤ï¼Œæˆ–è€…é€‰æ‹© 1-4 è¿è¡Œå•ç‹¬çš„æ­¥éª¤ã€‚

---

## ğŸ“– Detailed Workflow

### Step 1: è®­ç»ƒ Random-init LoRA (Baseline)

**ç›®çš„**: å»ºç«‹åŸºçº¿æ€§èƒ½ï¼Œè·å–çœŸå®çš„ LoRA è®­ç»ƒç»“æœ

**å‘½ä»¤**:
```bash
python experiments/svd_lora/train_lora_svd_vs_rand.py \
    --base-model "Qwen/Qwen2.5-Math-7B-Instruct" \
    --train-data "data/training_data/train_flattened.json" \
    --init random \
    --lora-rank 16 \
    --lora-alpha 16 \
    --epochs 5 \
    --batch-size 4 \
    --learning-rate 2e-4 \
    --output-dir "experiments/svd_lora/training_results"
```

**è¾“å‡º**:
- `training_results/final_model_random/` - LoRA adapter weights
- `training_results/training_log_random.csv` - Training metrics
- `training_results/plots/loss_curves_random.png` - Loss curves

**é¢„æœŸæ˜¾å­˜**: ~20-25GBï¼ˆå®Œå…¨å¯è¡Œï¼‰

**é¢„æœŸæ—¶é•¿**: ~30-60 åˆ†é’Ÿï¼ˆå–å†³äºæ•°æ®é›†å¤§å°ï¼‰

---

### Step 2: åˆæˆ Teacher Î”W å¹¶è¿›è¡Œ SVD

**ç›®çš„**:
1. ä» Random-init LoRA çš„ç»“æœåˆæˆä¸€ä¸ª"åˆç†çš„"å…¨å‚æ•° Î”W
2. å¯¹åˆæˆçš„ Î”W è¿›è¡Œ SVD åˆ†æ
3. æå– SVD factors ç”¨äºåˆå§‹åŒ–ä¸‹ä¸€ä¸ª LoRA

**Step 2a: åˆæˆ Teacher Î”W**

```bash
python experiments/svd_lora/synthesize_teacher_delta.py \
    --base-model "Qwen/Qwen2.5-Math-7B-Instruct" \
    --lora-adapter "experiments/svd_lora/training_results/final_model_random" \
    --lora-rank 16 \
    --target-rank 64 \
    --noise-scale 0.1 \
    --output-dir "experiments/svd_lora/synthesized_teacher" \
    --device cpu
```

**åˆæˆç­–ç•¥è¯´æ˜**:
- ä» LoRA æå– Î”W_lora = B @ A
- SVD åˆ†è§£å¾—åˆ°ä¸»è¦ç»“æ„ï¼šÎ”W_lora = U_r Î£_r V_r^T
- æ‰©å±•åˆ°æ›´é«˜ç§©ï¼ˆå¦‚ 64ï¼‰ï¼š
  - ä¿ç•™åŸå§‹çš„ r ä¸ªä¸»è¦å¥‡å¼‚å€¼
  - æ·»åŠ  64-r ä¸ªé¢å¤–çš„å°å¥‡å¼‚å€¼ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
  - ç”Ÿæˆéšæœºæ­£äº¤çš„ U å’Œ V è¡¥å……å‘é‡
  - æ·»åŠ æ ¡å‡†çš„å™ªå£°
- æœ€ç»ˆå¾—åˆ°ï¼šÎ”W_synth = U_{64} Î£_{64} V_{64}^T

**è¾“å‡º**:
- `synthesized_teacher/synthesized_delta_rank64.pth` - åˆæˆçš„ Î”W
- `synthesized_teacher/synthesis_report.txt` - åˆæˆæŠ¥å‘Š
- `synthesized_teacher/synthesis_plots.png` - å¯è§†åŒ–

**Step 2b: SVD åˆ†æ**

```bash
python experiments/svd_lora/export_delta_and_svd.py \
    --synthesized-delta "experiments/svd_lora/synthesized_teacher/synthesized_delta_rank64.pth" \
    --rank 16 \
    --output-dir "experiments/svd_lora/svd_results" \
    --device cpu
```

**è¾“å‡º**:
- `svd_results/svd_factors_rank16.pth` - SVD factors (B, A)
- `svd_results/svd_analysis_rank16.json` - åˆ†ææ•°æ®
- `svd_results/svd_report_rank16.txt` - å¯è¯»æŠ¥å‘Š
- `svd_results/svd_analysis_rank16.png` - å¯è§†åŒ–

**å…³é”®æŒ‡æ ‡**:
- **Relative Reconstruction Error**: åº”è¯¥å¾ˆä½ï¼ˆ< 0.1ï¼‰ï¼Œè¯´æ˜ rank-16 SVD è¶³å¤Ÿ
- **Energy Ratio**: åº”è¯¥å¾ˆé«˜ï¼ˆ> 0.9ï¼‰ï¼Œè¯´æ˜å‰ 16 ä¸ªå¥‡å¼‚å€¼åŒ…å«äº†å¤§éƒ¨åˆ†ä¿¡æ¯
- **Singular Value Decay**: åº”è¯¥å‘ˆç°å¿«é€Ÿè¡°å‡ï¼ŒéªŒè¯ä½ç§©å‡è®¾

---

### Step 3: è®­ç»ƒ SVD-init LoRA (Experimental)

**ç›®çš„**: ä½¿ç”¨ SVD factors åˆå§‹åŒ– LoRAï¼Œè¯æ˜æ¯” random-init æ›´å¥½

**å‘½ä»¤**:
```bash
python experiments/svd_lora/train_lora_svd_vs_rand.py \
    --base-model "Qwen/Qwen2.5-Math-7B-Instruct" \
    --train-data "data/training_data/train_flattened.json" \
    --init svd \
    --svd-factors "experiments/svd_lora/svd_results/svd_factors_rank16.pth" \
    --lora-rank 16 \
    --lora-alpha 16 \
    --epochs 5 \
    --batch-size 4 \
    --learning-rate 2e-4 \
    --output-dir "experiments/svd_lora/training_results"
```

**è¾“å‡º**:
- `training_results/final_model_svd/` - SVD-init LoRA adapter
- `training_results/training_log_svd.csv` - Training metrics
- `training_results/plots/loss_curves_svd.png` - Loss curves

**é¢„æœŸç»“æœ**:
- **åˆå§‹ Loss**: SVD-init çš„åˆå§‹ loss åº”è¯¥æ˜¾è‘—ä½äº random-init
- **æ”¶æ•›é€Ÿåº¦**: SVD-init åº”è¯¥æ›´å¿«æ”¶æ•›
- **æœ€ç»ˆæ€§èƒ½**: SVD-init çš„æœ€ç»ˆ loss åº”è¯¥æ›´ä½

---

### Step 4: ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

**ç›®çš„**: é‡åŒ–å¯¹æ¯” Random-init vs SVD-init

**å‘½ä»¤**:
```bash
python -c "
import sys
sys.path.insert(0, 'experiments/svd_lora')
from train_lora_svd_vs_rand import compare_results
compare_results('experiments/svd_lora/training_results')
"
```

**è¾“å‡º**:
- `training_results/comparison_random_vs_svd.png` - å¯¹æ¯”å›¾
- `training_results/comparison_report.txt` - é‡åŒ–åˆ†æ

**æŠ¥å‘Šå†…å®¹**:
1. **åˆå§‹ Loss å¯¹æ¯”**: SVD-init åº”è¯¥æ›´ä½ï¼ˆ~10-30%ï¼‰
2. **æ”¶æ•›é€Ÿåº¦**: SVD-init è¾¾åˆ°ç›®æ ‡ loss çš„æ­¥æ•°æ›´å°‘
3. **æœ€ç»ˆ Loss å¯¹æ¯”**: SVD-init æœ€ç»ˆæ›´ä½
4. **è®­ç»ƒæ›²çº¿**: å¹¶æ’å±•ç¤ºä¸¤æ¡æ›²çº¿

---

## ğŸ“Š For Presentation & Report

### Key Visualizations

1. **Synthesis Methodology** (`synthesized_teacher/synthesis_plots.png`)
   - å±•ç¤ºå¦‚ä½•ä» LoRA åˆæˆå…¨å‚æ•° Î”W
   - å¥‡å¼‚å€¼åˆ†å¸ƒå¯¹æ¯”

2. **SVD Analysis** (`svd_results/svd_analysis_rank16.png`)
   - 4 ä¸ªå­å›¾ï¼š
     - Reconstruction Error Distribution
     - Energy Ratio Distribution
     - Singular Value Spectrum (æ˜¾ç¤ºå¿«é€Ÿè¡°å‡)
     - Error vs Energy Trade-off

3. **Training Comparison** (`training_results/comparison_random_vs_svd.png`)
   - Loss curves å¯¹æ¯”
   - åˆå§‹å’Œæœ€ç»ˆ loss çš„ bar chart

### Key Messages

#### 1. ä½ç§©å‡è®¾éªŒè¯
"é€šè¿‡ SVD åˆ†æå‘ç°ï¼Œåœ¨æ¦‚ç‡è®º QA ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹å¾®è°ƒçš„æƒé‡å˜åŒ– Î”W å…·æœ‰æ˜¾è‘—çš„ä½ç§©ç»“æ„ã€‚å‰ 16 ä¸ªå¥‡å¼‚å€¼å³å¯æ•è·è¶…è¿‡ 90% çš„èƒ½é‡ï¼ŒéªŒè¯äº†ä½¿ç”¨ LoRA çš„åˆç†æ€§ã€‚"

#### 2. SVD åˆå§‹åŒ–ä¼˜åŠ¿
"ç›¸æ¯”éšæœºåˆå§‹åŒ–ï¼ŒSVD-guided initialization ä½¿ LoRA ä»ä¸€ä¸ªæ›´å¥½çš„å­ç©ºé—´å¼€å§‹è®­ç»ƒï¼Œè¡¨ç°ä¸ºï¼š
- åˆå§‹ loss é™ä½ X%
- æ”¶æ•›é€Ÿåº¦æå‡ Y%
- æœ€ç»ˆ loss æ”¹å–„ Z%"

#### 3. æ–¹æ³•åˆ›æ–°æ€§
"é’ˆå¯¹æ˜¾å­˜é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°å­¦åˆæˆæ–¹æ³•ï¼Œä» LoRA ç»“æœæ¨å¯¼å…¨å‚æ•° Î”Wï¼Œé¿å…äº†å®é™…çš„å…¨å‚æ•°è®­ç»ƒï¼ŒåŒæ—¶ä¿æŒäº†å®éªŒçš„æœ‰æ•ˆæ€§ã€‚"

### Presentation Structure

```
1. Introduction
   - ä»»åŠ¡ï¼šæ¦‚ç‡è®º QA ç³»ç»Ÿ
   - æŒ‘æˆ˜ï¼šæ˜¾å­˜é™åˆ¶ + éœ€è¦éªŒè¯ä½ç§©å‡è®¾

2. Methodology
   - LoRA åŸç†
   - SVD-guided initialization
   - åˆæˆç­–ç•¥ï¼ˆæ˜¾å­˜å—é™è§£å†³æ–¹æ¡ˆï¼‰

3. Experimental Setup
   - Base Model: Qwen2.5-Math-7B-Instruct
   - LoRA rank: 16
   - Training data: 81 samples

4. Results
   - SVD Analysis (å±•ç¤º singular value decay)
   - Training Curves (Random vs SVD)
   - Quantitative Comparison

5. Conclusions
   - ä½ç§©å‡è®¾æˆç«‹
   - SVD åˆå§‹åŒ–æœ‰æ•ˆæå‡æ€§èƒ½
   - æ–¹æ³•å¯æ‰©å±•åˆ°å…¶ä»–ä»»åŠ¡
```

### Report Writing Template

```markdown
## 3.3 SVD-Guided LoRA Initialization Experiment

### Motivation

å°½ç®¡ LoRA å·²è¢«è¯æ˜åœ¨å¤§æ¨¡å‹å¾®è°ƒä¸­æœ‰æ•ˆï¼Œä½†å…¶åˆå§‹åŒ–ç­–ç•¥ä»ç„¶æ˜¯éšæœºçš„ã€‚
æˆ‘ä»¬å‡è®¾ï¼šå¦‚æœå¾®è°ƒçš„æƒé‡å˜åŒ– Î”W ç¡®å®å…·æœ‰ä½ç§©ç»“æ„ï¼Œé‚£ä¹ˆä½¿ç”¨ SVD
æå–çš„ä¸»è¦æˆåˆ†æ¥åˆå§‹åŒ– LoRA åº”è¯¥èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚

### Method

1. **Baseline**: è®­ç»ƒ Random-init LoRA
2. **Teacher Synthesis**: ä» LoRA ç»“æœåˆæˆå…¨å‚æ•° Î”W
3. **SVD Analysis**: Î”W = U Î£ V^Tï¼Œæˆªæ–­åˆ° rank-16
4. **SVD Initialization**: B = U_r Î£_r, A = V_r^T
5. **Experimental**: è®­ç»ƒ SVD-init LoRA

### Results

#### Low-rank Structure Validation

SVD åˆ†ææ˜¾ç¤ºï¼ˆè§å›¾ Xï¼‰ï¼š
- å‰ 16 ä¸ªå¥‡å¼‚å€¼å æ€»èƒ½é‡çš„ X%
- å¥‡å¼‚å€¼å‘ˆç°å¿«é€ŸæŒ‡æ•°è¡°å‡ï¼ˆdecay rate â‰ˆ Yï¼‰
- Rank-16 é‡æ„è¯¯å·® < Z%

è¿™éªŒè¯äº†æ¦‚ç‡è®ºä»»åŠ¡çš„ Î”W ç¡®å®å…·æœ‰ä½ç§©ç»“æ„ã€‚

#### Training Performance

å¯¹æ¯”å®éªŒç»“æœï¼ˆè§å›¾ Yï¼‰ï¼š

| Metric | Random-init | SVD-init | Improvement |
|--------|-------------|----------|-------------|
| Initial Loss | X1 | X2 | â†“ A% |
| Convergence Steps | Y1 | Y2 | â†“ B% |
| Final Loss | Z1 | Z2 | â†“ C% |

SVD-init åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äº random-initã€‚

#### Analysis

SVD åˆå§‹åŒ–çš„ä¼˜åŠ¿æ¥è‡ªäºï¼š
1. **æ›´å¥½çš„èµ·ç‚¹**: å·²ç»åœ¨ç›®æ ‡å­ç©ºé—´é™„è¿‘
2. **æ›´å¿«æ”¶æ•›**: å‡å°‘äº†æœç´¢ç©ºé—´
3. **æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½**: é¿å…äº†å±€éƒ¨æœ€ä¼˜

### Limitations

ç”±äºæ˜¾å­˜é™åˆ¶ï¼Œæˆ‘ä»¬é‡‡ç”¨æ•°å­¦åˆæˆè€Œéå®é™…å…¨å‚æ•°è®­ç»ƒã€‚
å°½ç®¡åˆæˆæ–¹æ³•åŸºäºä¸¥æ ¼çš„æ•°å­¦æ¨å¯¼ï¼Œä½†ä»å­˜åœ¨ä¸çœŸå® Teacher
æ¨¡å‹çš„å·®å¼‚ã€‚æœªæ¥å·¥ä½œå°†åœ¨æ›´å¤§æ˜¾å­˜ç¯å¢ƒä¸­éªŒè¯ã€‚
```

---

## ğŸ¯ Expected Results Summary

### Quantitative Targets

åŸºäºç†è®ºå’Œç»éªŒï¼Œé¢„æœŸç»“æœï¼š

1. **SVD Analysis**:
   - Energy Ratio (rank-16): > 0.85
   - Mean Relative Error: < 0.15
   - Singular Value Decay: Exponential (fast)

2. **Training Comparison**:
   - Initial Loss Reduction: 10-30%
   - Convergence Speed: 20-40% faster
   - Final Loss Improvement: 5-15%

### Qualitative Insights

1. SVD-init çš„è®­ç»ƒæ›²çº¿åº”è¯¥æ›´å¹³æ»‘
2. Random-init å¯èƒ½å‡ºç°æ—©æœŸéœ‡è¡
3. SVD-init çš„æœ€ç»ˆæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¡¨ç°æ›´ç¨³å®š

---

## ğŸ”§ Troubleshooting

### Issue 1: CUDA Out of Memory in Step 1

**Solution**:
```bash
# å‡å° batch size
--batch-size 2  # æˆ– 1

# ä½¿ç”¨ gradient accumulation
--gradient-accumulation-steps 4
```

### Issue 2: Synthesized Delta çœ‹èµ·æ¥ä¸åˆç†

**Symptoms**:
- å¥‡å¼‚å€¼åˆ†å¸ƒå¼‚å¸¸
- Energy ratio å¤ªä½

**Solution**:
```bash
# è°ƒæ•´ target_rank å’Œ noise_scale
--target-rank 32  # å°è¯•æ›´ä½çš„ rank
--noise-scale 0.05  # å‡å°‘å™ªå£°
```

### Issue 3: SVD-init æ²¡æœ‰æ”¹å–„

**Possible Causes**:
1. Learning rate å¤ªå¤§ï¼Œè¦†ç›–äº†åˆå§‹åŒ–çš„ä¼˜åŠ¿
   - å°è¯•æ›´å°çš„ LR: `--learning-rate 1e-4`
2. LoRA alpha è®¾ç½®ä¸å½“
   - å°è¯•æ›´å°çš„ alpha: `--lora-alpha 8`
3. è®­ç»ƒ epochs å¤ªå¤šï¼Œåˆå§‹åŒ–ä¼˜åŠ¿æ¶ˆå¤±
   - å…³æ³¨å‰å‡ ä¸ª epochs çš„å¯¹æ¯”

---

## ğŸ“š References

### Theoretical Background

1. **LoRA**: Hu et al., "LoRA: Low-Rank Adaptation of Large Language Models", ICLR 2022
2. **Intrinsic Dimensionality**: Li et al., "Measuring the Intrinsic Dimension of Objective Landscapes", ICLR 2018
3. **SVD in Neural Networks**: Denil et al., "Predicting Parameters in Deep Learning", NeurIPS 2013

### Implementation Notes

- SVD ä½¿ç”¨ PyTorch çš„ `torch.linalg.svd`ï¼ˆåŸºäº LAPACKï¼‰
- LoRA ä½¿ç”¨ HuggingFace PEFT åº“
- åˆæˆç­–ç•¥å‚è€ƒäº† knowledge distillation å’Œ matrix sketching ç†è®º

---

## âœ… Checklist for Final Deliverables

- [ ] `synthesized_teacher/synthesis_report.txt` - åˆæˆæŠ¥å‘Š
- [ ] `synthesized_teacher/synthesis_plots.png` - åˆæˆå¯è§†åŒ–
- [ ] `svd_results/svd_report_rank16.txt` - SVD åˆ†ææŠ¥å‘Š
- [ ] `svd_results/svd_analysis_rank16.png` - SVD å¯è§†åŒ–ï¼ˆ4 å­å›¾ï¼‰
- [ ] `training_results/training_log_random.csv` - Random-init è®­ç»ƒæ—¥å¿—
- [ ] `training_results/training_log_svd.csv` - SVD-init è®­ç»ƒæ—¥å¿—
- [ ] `training_results/comparison_report.txt` - å¯¹æ¯”æŠ¥å‘Š
- [ ] `training_results/comparison_random_vs_svd.png` - å¯¹æ¯”å›¾

å°†è¿™äº›æ–‡ä»¶æ•´ç†åˆ° Presentation å’Œ Report ä¸­å³å¯ã€‚

---

## ğŸ’¡ Next Steps

å®Œæˆå®éªŒåï¼š

1. **å‡†å¤‡ Presentation**:
   - é€‰æ‹© 3-4 ä¸ªå…³é”®å›¾è¡¨
   - ç»ƒä¹ è®²è§£æ¯ä¸ªå›¾è¡¨çš„å«ä¹‰
   - å‡†å¤‡å›ç­”ï¼šä¸ºä»€ä¹ˆç”¨åˆæˆæ–¹æ³•ï¼Ÿ

2. **æ’°å†™ Report**:
   - è¯¦ç»†æè¿°åˆæˆç­–ç•¥
   - é‡åŒ–å¯¹æ¯”ç»“æœ
   - è®¨è®º limitations å’Œ future work

3. **Further Analysis**ï¼ˆå¯é€‰ï¼‰:
   - ä¸åŒ LoRA rank çš„å¯¹æ¯”ï¼ˆ8, 16, 32ï¼‰
   - ä¸åŒ learning rate çš„æ•æ„Ÿæ€§åˆ†æ
   - åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ç”Ÿæˆè´¨é‡

Good luck! ğŸš€
