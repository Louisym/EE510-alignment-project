"""
LoRA Training: SVD-init vs Random-init Comparison

å¯¹æ¯”å®éªŒï¼š
  - Student-random: ä¼ ç»Ÿ LoRAï¼ˆAéšæœºï¼ŒB=0ï¼‰
  - Student-SVD: SVD åˆå§‹åŒ–çš„ LoRAï¼ˆä» Teacher çš„ Î”W æå–ï¼‰

This script enables direct comparison of convergence speed and final performance.
"""

import os
import sys
import torch
import argparse
import json
import pandas as pd
from pathlib import Path
from typing import Dict, Optional
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    TrainerCallback
)
from peft import (
    LoraConfig,
    get_peft_model,
    PeftModel
)
from peft.tuners.lora import LoraLayer

from training.sft.data_loader import create_dataloaders


class SVDLoRAInitializer:
    """SVD-guided LoRA initialization"""

    def __init__(self, svd_factors_path: str, lora_rank: int):
        """
        åˆå§‹åŒ–

        Args:
            svd_factors_path: SVD factors æ–‡ä»¶è·¯å¾„
            lora_rank: LoRA rank
        """
        self.svd_factors = torch.load(svd_factors_path, map_location='cpu')
        self.lora_rank = lora_rank
        print(f"âœ“ Loaded SVD factors from: {svd_factors_path}")
        print(f"  Total layers: {len(self.svd_factors)}")

    def initialize_lora_weights(self, model: PeftModel):
        """
        å°† SVD çš„ B,A å†™å…¥ LoRA å‚æ•°

        Args:
            model: PEFT LoRA æ¨¡å‹
        """
        print("\nğŸ”§ Initializing LoRA weights with SVD factors...")

        initialized_count = 0
        skipped_count = 0

        # éå†æ‰€æœ‰æ¨¡å—
        for name, module in model.named_modules():
            # æ£€æŸ¥æ˜¯å¦åœ¨ SVD factors ä¸­
            if name not in self.svd_factors:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA å±‚
            # PEFT ä¼šå°†åŸå§‹ Linear åŒ…è£…ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å®é™…çš„ LoRA adapter
            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                factors = self.svd_factors[name]
                B = factors['B']  # [d_out, r]
                A = factors['A']  # [r, d_in]

                # æ£€æŸ¥ç»´åº¦
                lora_A_weight = module.lora_A['default'].weight
                lora_B_weight = module.lora_B['default'].weight

                if lora_A_weight.shape != A.shape:
                    print(f"  âš  Shape mismatch for {name}: "
                          f"expected {lora_A_weight.shape}, got {A.shape}")
                    skipped_count += 1
                    continue

                if lora_B_weight.shape != B.shape:
                    print(f"  âš  Shape mismatch for {name}: "
                          f"expected {lora_B_weight.shape}, got {B.shape}")
                    skipped_count += 1
                    continue

                # å†™å…¥æƒé‡
                with torch.no_grad():
                    lora_A_weight.copy_(A.to(lora_A_weight.device))
                    lora_B_weight.copy_(B.to(lora_B_weight.device))

                initialized_count += 1

        print(f"âœ“ Initialized {initialized_count} layers")
        if skipped_count > 0:
            print(f"  âš  Skipped {skipped_count} layers due to shape mismatch")


class ComparisonCallback(TrainerCallback):
    """è®°å½•è¯¦ç»†çš„è®­ç»ƒæŒ‡æ ‡ç”¨äºå¯¹æ¯”"""

    def __init__(self, output_dir: str, init_method: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.init_method = init_method
        self.log_file = self.output_dir / f"training_log_{init_method}.csv"

        # åˆå§‹åŒ–æ—¥å¿—
        self.logs = []

        print(f"ğŸ“Š Logging to: {self.log_file}")

    def on_log(self, args, state, control, logs=None, **kwargs):
        """è®°å½•æ¯æ¬¡ log"""
        if logs is None:
            return

        # æ·»åŠ æ—¶é—´æˆ³å’Œåˆå§‹åŒ–æ–¹æ³•
        log_entry = {
            'step': state.global_step,
            'epoch': state.epoch,
            'init_method': self.init_method,
            **logs
        }
        self.logs.append(log_entry)

        # å®šæœŸä¿å­˜
        if state.global_step % 10 == 0:
            self.save_logs()

    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶ä¿å­˜"""
        self.save_logs()
        print(f"âœ“ Training log saved to: {self.log_file}")

    def save_logs(self):
        """ä¿å­˜æ—¥å¿—åˆ° CSV"""
        if self.logs:
            df = pd.DataFrame(self.logs)
            df.to_csv(self.log_file, index=False)


def create_lora_model(
    base_model_path: str,
    lora_rank: int,
    lora_alpha: int,
    target_modules: list,
    init_method: str = "random",
    svd_factors_path: Optional[str] = None,
    device: str = "auto"
) -> PeftModel:
    """
    åˆ›å»º LoRA æ¨¡å‹

    Args:
        base_model_path: Base æ¨¡å‹è·¯å¾„
        lora_rank: LoRA rank
        lora_alpha: LoRA alphaï¼ˆæ¨èè®¾ä¸º rankï¼Œä½¿ç¼©æ”¾å› å­ä¸º1ï¼‰
        target_modules: ç›®æ ‡æ¨¡å—åˆ—è¡¨
        init_method: åˆå§‹åŒ–æ–¹æ³• ("random" æˆ– "svd")
        svd_factors_path: SVD factors æ–‡ä»¶è·¯å¾„ï¼ˆinit_method="svd" æ—¶éœ€è¦ï¼‰
        device: è®¾å¤‡

    Returns:
        LoRA æ¨¡å‹
    """
    print(f"\n{'='*70}")
    print(f"ğŸš€ Creating LoRA Model (init={init_method})")
    print(f"{'='*70}")

    # åŠ è½½ base æ¨¡å‹
    print(f"\nLoading base model from: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map=device,
        trust_remote_code=True
    )
    print("âœ“ Base model loaded")

    # é…ç½® LoRA
    lora_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,  # è®¾ä¸º rankï¼Œä½¿ Î±/r = 1
        lora_dropout=0.05,
        target_modules=target_modules,
        task_type="CAUSAL_LM",
        bias="none",
    )

    print(f"\nLoRA config:")
    print(f"  rank: {lora_rank}")
    print(f"  alpha: {lora_alpha}")
    print(f"  alpha/rank: {lora_alpha/lora_rank}")
    print(f"  target_modules: {target_modules}")

    # åº”ç”¨ LoRA
    lora_model = get_peft_model(base_model, lora_config)
    lora_model.print_trainable_parameters()

    # SVD åˆå§‹åŒ–
    if init_method == "svd":
        if svd_factors_path is None:
            raise ValueError("svd_factors_path is required for init_method='svd'")

        initializer = SVDLoRAInitializer(svd_factors_path, lora_rank)
        initializer.initialize_lora_weights(lora_model)

    print("\nâœ“ LoRA model created")

    return lora_model


def train_lora(
    model: PeftModel,
    tokenizer,
    train_loader,
    val_loader,
    output_dir: str,
    init_method: str,
    num_epochs: int = 3,
    learning_rate: float = 2e-4,
    batch_size: int = 4,
    gradient_accumulation_steps: int = 4,
    logging_steps: int = 10,
    save_steps: int = 100
):
    """
    è®­ç»ƒ LoRA æ¨¡å‹

    Args:
        model: LoRA æ¨¡å‹
        tokenizer: Tokenizer
        train_loader: è®­ç»ƒæ•°æ®
        val_loader: éªŒè¯æ•°æ®
        output_dir: è¾“å‡ºç›®å½•
        init_method: åˆå§‹åŒ–æ–¹æ³•
        num_epochs: Epoch æ•°
        learning_rate: å­¦ä¹ ç‡
        batch_size: Batch size
        gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        logging_steps: æ—¥å¿—æ­¥æ•°
        save_steps: ä¿å­˜æ­¥æ•°
    """
    print(f"\n{'='*70}")
    print(f"ğŸ‹ï¸ Training LoRA Model (init={init_method})")
    print(f"{'='*70}")

    # è®­ç»ƒå‚æ•°
    # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯é›†
    has_eval = val_loader is not None and val_loader.dataset is not None and len(val_loader.dataset) > 0

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=learning_rate,
        fp16=True,
        logging_steps=logging_steps,
        save_steps=save_steps,
        save_total_limit=3,
        eval_strategy="steps" if has_eval else "no",  # åªæœ‰æœ‰éªŒè¯é›†æ—¶æ‰è¯„ä¼°
        eval_steps=save_steps if has_eval else None,
        load_best_model_at_end=False,
        report_to="none",
        remove_unused_columns=False,
        group_by_length=True,
    )

    # åˆ›å»ºå›è°ƒ
    comparison_callback = ComparisonCallback(output_dir, init_method)

    # åˆ›å»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_loader.dataset,
        eval_dataset=val_loader.dataset if val_loader else None,
        tokenizer=tokenizer,
        callbacks=[comparison_callback]
    )

    # è®­ç»ƒ
    print(f"\nStarting training...")
    start_time = time.time()

    trainer.train()

    elapsed_time = time.time() - start_time
    print(f"\nâœ“ Training completed in {elapsed_time:.2f} seconds")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(output_dir, f"final_model_{init_method}")
    trainer.save_model(final_model_path)
    print(f"âœ“ Final model saved to: {final_model_path}")

    return trainer, comparison_callback


def compare_results(output_dir: str, methods: list = ["random", "svd"]):
    """
    å¯¹æ¯”ä¸¤ç§åˆå§‹åŒ–æ–¹æ³•çš„ç»“æœ

    Args:
        output_dir: è¾“å‡ºç›®å½•
        methods: åˆå§‹åŒ–æ–¹æ³•åˆ—è¡¨
    """
    import matplotlib.pyplot as plt
    import seaborn as sns

    sns.set_style("whitegrid")

    print(f"\n{'='*70}")
    print("ğŸ“Š Generating Comparison Plots...")
    print(f"{'='*70}")

    output_dir = Path(output_dir)

    # åŠ è½½æ—¥å¿—
    logs = {}
    for method in methods:
        log_file = output_dir / f"training_log_{method}.csv"
        if log_file.exists():
            logs[method] = pd.read_csv(log_file)
            print(f"âœ“ Loaded log for {method}: {len(logs[method])} entries")
        else:
            print(f"âš  Log file not found for {method}: {log_file}")

    if len(logs) == 0:
        print("âŒ No log files found")
        return

    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # 1. è®­ç»ƒæŸå¤±å¯¹æ¯”
    ax = axes[0, 0]
    for method, df in logs.items():
        if 'loss' in df.columns:
            ax.plot(df['step'], df['loss'], label=f'{method}-init', linewidth=2, marker='o', markersize=3)
    ax.set_xlabel('Training Steps', fontsize=12)
    ax.set_ylabel('Training Loss', fontsize=12)
    ax.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    # 2. éªŒè¯æŸå¤±å¯¹æ¯”
    ax = axes[0, 1]
    for method, df in logs.items():
        eval_df = df[df['eval_loss'].notna()]
        if len(eval_df) > 0:
            ax.plot(eval_df['step'], eval_df['eval_loss'],
                   label=f'{method}-init', linewidth=2, marker='s', markersize=5)
    ax.set_xlabel('Training Steps', fontsize=12)
    ax.set_ylabel('Validation Loss', fontsize=12)
    ax.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    # 3. å­¦ä¹ ç‡å¯¹æ¯”
    ax = axes[1, 0]
    for method, df in logs.items():
        if 'learning_rate' in df.columns:
            ax.plot(df['step'], df['learning_rate'], label=f'{method}-init', linewidth=2)
    ax.set_xlabel('Training Steps', fontsize=12)
    ax.set_ylabel('Learning Rate', fontsize=12)
    ax.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

    # 4. æœ€ç»ˆæ€§èƒ½å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
    ax = axes[1, 1]
    final_metrics = {}
    for method, df in logs.items():
        # è·å–æœ€å10æ­¥çš„å¹³å‡æŸå¤±
        final_loss = df['loss'].tail(10).mean()
        final_metrics[method] = final_loss

    methods_list = list(final_metrics.keys())
    values_list = list(final_metrics.values())
    colors = ['#3498db', '#2ecc71'][:len(methods_list)]

    bars = ax.bar(methods_list, values_list, color=colors, alpha=0.7, edgecolor='black')
    ax.set_ylabel('Final Training Loss (last 10 steps avg)', fontsize=12)
    ax.set_title('Final Performance Comparison', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')

    # åœ¨æŸ±å­ä¸Šæ ‡æ³¨æ•°å€¼
    for bar, value in zip(bars, values_list):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{value:.4f}',
               ha='center', va='bottom', fontsize=11, fontweight='bold')

    plt.tight_layout()

    # ä¿å­˜å›¾è¡¨
    comparison_plot = output_dir / "comparison_random_vs_svd.png"
    plt.savefig(comparison_plot, dpi=300, bbox_inches='tight')
    print(f"\nâœ“ Comparison plot saved to: {comparison_plot}")

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report_path = output_dir / "comparison_report.txt"
    report_lines = []
    report_lines.append("="*70)
    report_lines.append("LoRA Initialization Comparison Report")
    report_lines.append("="*70)
    report_lines.append("")

    for method, df in logs.items():
        report_lines.append(f"{method.upper()}-init:")
        report_lines.append(f"  Initial loss: {df['loss'].iloc[0]:.4f}")
        report_lines.append(f"  Final loss: {df['loss'].iloc[-1]:.4f}")
        report_lines.append(f"  Best loss: {df['loss'].min():.4f}")
        improvement = (df['loss'].iloc[0] - df['loss'].iloc[-1]) / df['loss'].iloc[0] * 100
        report_lines.append(f"  Improvement: {improvement:.2f}%")
        report_lines.append("")

    # è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿
    if len(logs) == 2:
        methods_list = list(logs.keys())
        final_losses = [logs[m]['loss'].iloc[-1] for m in methods_list]
        if 'svd' in methods_list and 'random' in methods_list:
            svd_idx = methods_list.index('svd')
            rand_idx = methods_list.index('random')
            advantage = (final_losses[rand_idx] - final_losses[svd_idx]) / final_losses[rand_idx] * 100
            report_lines.append(f"SVD-init advantage over Random-init: {advantage:.2f}%")

    report_lines.append("")
    report_lines.append("="*70)

    report_text = "\n".join(report_lines)

    with open(report_path, 'w') as f:
        f.write(report_text)

    print(f"âœ“ Comparison report saved to: {report_path}")
    print("\n" + report_text)


def main():
    parser = argparse.ArgumentParser(
        description="Train LoRA with different initialization methods"
    )

    # æ¨¡å‹å’Œæ•°æ®
    parser.add_argument("--base-model", type=str, required=True,
                       help="Base model path")
    parser.add_argument("--train-data", type=str, required=True,
                       help="Training data path (JSON)")
    parser.add_argument("--val-data", type=str, default=None,
                       help="Validation data path (optional)")

    # LoRA é…ç½®
    parser.add_argument("--lora-rank", type=int, default=16,
                       help="LoRA rank (default: 16)")
    parser.add_argument("--lora-alpha", type=int, default=16,
                       help="LoRA alpha (default: 16, same as rank)")

    # åˆå§‹åŒ–æ–¹æ³•
    parser.add_argument("--init", type=str, default="random",
                       choices=["random", "svd"],
                       help="Initialization method")
    parser.add_argument("--svd-factors", type=str, default=None,
                       help="Path to SVD factors (required for --init svd)")

    # è®­ç»ƒé…ç½®
    parser.add_argument("--epochs", type=int, default=3,
                       help="Number of epochs")
    parser.add_argument("--batch-size", type=int, default=4,
                       help="Batch size")
    parser.add_argument("--learning-rate", type=float, default=2e-4,
                       help="Learning rate")
    parser.add_argument("--max-length", type=int, default=512,
                       help="Max sequence length")

    # è¾“å‡º
    parser.add_argument("--output-dir", type=str,
                       default="./experiments/svd_lora/training_results",
                       help="Output directory")

    args = parser.parse_args()

    # éªŒè¯ SVD åˆå§‹åŒ–å‚æ•°
    if args.init == "svd" and args.svd_factors is None:
        raise ValueError("--svd-factors is required when --init svd")

    # ç›®æ ‡æ¨¡å—ï¼ˆæ ¹æ®æ¨¡å‹è°ƒæ•´ï¼‰
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                     "gate_proj", "up_proj", "down_proj"]

    # åŠ è½½ tokenizer
    print(f"\nLoading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("âœ“ Tokenizer loaded")

    # åŠ è½½æ•°æ®
    print(f"\nLoading data...")
    train_loader, val_loader = create_dataloaders(
        train_path=args.train_data,
        tokenizer=tokenizer,
        val_path=args.val_data,
        batch_size=args.batch_size,
        max_length=args.max_length
    )
    print(f"âœ“ Data loaded: {len(train_loader)} train batches")
    if val_loader:
        print(f"  {len(val_loader)} validation batches")

    # åˆ›å»º LoRA æ¨¡å‹
    lora_model = create_lora_model(
        base_model_path=args.base_model,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        target_modules=target_modules,
        init_method=args.init,
        svd_factors_path=args.svd_factors,
        device="auto"
    )

    # è®­ç»ƒ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    trainer, callback = train_lora(
        model=lora_model,
        tokenizer=tokenizer,
        train_loader=train_loader,
        val_loader=val_loader,
        output_dir=str(output_dir),
        init_method=args.init,
        num_epochs=args.epochs,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size
    )

    print(f"\n{'='*70}")
    print("âœ… Training Complete!")
    print(f"{'='*70}")
    print(f"\nOutput directory: {output_dir}")
    print(f"  - final_model_{args.init}/")
    print(f"  - training_log_{args.init}.csv")

    print("\nğŸ’¡ Next steps:")
    if args.init == "random":
        print("  1. Train with SVD init:")
        print(f"     python {__file__} --base-model {args.base_model} \\")
        print(f"       --train-data {args.train_data} --init svd \\")
        print(f"       --svd-factors <path_to_svd_factors.pth>")
        print("  2. Compare results:")
        print(f"     Will automatically compare when both logs are present")
    else:
        print("  1. Compare with random init results")
        print(f"  2. Analyze convergence speed difference")


if __name__ == "__main__":
    main()
