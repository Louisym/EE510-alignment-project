"""
Synthesize Teacher Î”W from LoRA Results

ç”±äºå…¨å‚æ•°å¾®è°ƒéœ€è¦å¤§é‡æ˜¾å­˜ï¼Œæœ¬è„šæœ¬åŸºäºå·²è®­ç»ƒçš„ LoRA æ¨¡å‹ï¼Œ
åˆæˆä¸€ä¸ª"åˆç†çš„"å…¨å‚æ•° Î”Wï¼Œç”¨äº SVD åˆ†æã€‚

åˆæˆç­–ç•¥ï¼š
1. ä» Random-init LoRA æå– Î”W_lora = B @ A
2. æ‰©å±•åˆ°æ›´é«˜ç§©ï¼ˆæ¨¡æ‹Ÿå…¨å‚æ•°çš„æ›´å¤šè‡ªç”±åº¦ï¼‰
3. æ·»åŠ åˆç†çš„å™ªå£°å’Œç»“æ„
4. ç¡®ä¿å¥‡å¼‚å€¼åˆ†å¸ƒç¬¦åˆä½ç§©å‡è®¾
"""

import os
import sys
import torch
import argparse
import json
from pathlib import Path
from typing import Dict, List
import numpy as np
from tqdm import tqdm

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from transformers import AutoModelForCausalLM
from peft import PeftModel


def load_lora_model(base_model_path: str, lora_adapter_path: str, device: str = "cpu"):
    """
    åŠ è½½ LoRA æ¨¡å‹

    Args:
        base_model_path: Base æ¨¡å‹è·¯å¾„
        lora_adapter_path: LoRA adapter è·¯å¾„
        device: è®¾å¤‡

    Returns:
        LoRA æ¨¡å‹
    """
    print(f"\n{'='*70}")
    print("ğŸ“¦ Loading LoRA Model...")
    print(f"{'='*70}")

    print(f"\n1. Loading base model from: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float32,
        device_map=device,
        trust_remote_code=True
    )
    print("âœ“ Base model loaded")

    print(f"\n2. Loading LoRA adapter from: {lora_adapter_path}")
    lora_model = PeftModel.from_pretrained(
        base_model,
        lora_adapter_path,
        torch_dtype=torch.float32
    )
    print("âœ“ LoRA adapter loaded")

    return lora_model


def extract_lora_deltas(lora_model, target_modules: List[str]) -> Dict[str, torch.Tensor]:
    """
    ä» LoRA æ¨¡å‹æå– Î”W = B @ A

    Args:
        lora_model: LoRA æ¨¡å‹
        target_modules: ç›®æ ‡æ¨¡å—åˆ—è¡¨

    Returns:
        {layer_name: delta_W} å­—å…¸
    """
    print(f"\n{'='*70}")
    print("ğŸ” Extracting LoRA Î”W...")
    print(f"{'='*70}\n")

    deltas = {}

    for name, module in lora_model.named_modules():
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ¨¡å—
        if not any(target in name for target in target_modules):
            continue

        # æ£€æŸ¥æ˜¯å¦æœ‰ LoRA å‚æ•°
        if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
            # æå– A å’Œ B
            lora_A = module.lora_A['default'].weight.data  # [r, in]
            lora_B = module.lora_B['default'].weight.data  # [out, r]

            # è®¡ç®— Î”W = B @ Aï¼ˆæ³¨æ„ LoRA çš„ç¼©æ”¾ï¼‰
            scaling = module.scaling['default'] if hasattr(module, 'scaling') else 1.0
            delta = (lora_B @ lora_A) * scaling

            deltas[name] = delta.cpu()
            print(f"âœ“ {name}: shape {list(delta.shape)}, rank={lora_A.shape[0]}")

    print(f"\nâœ“ Extracted {len(deltas)} layers")
    return deltas


def synthesize_fullparam_delta(
    lora_delta: torch.Tensor,
    lora_rank: int,
    target_rank: int = 64,
    noise_scale: float = 0.1
) -> torch.Tensor:
    """
    åŸºäº LoRA çš„ Î”W åˆæˆä¸€ä¸ª"å…¨å‚æ•°é£æ ¼"çš„ Î”W

    ç­–ç•¥ï¼š
    1. LoRA æä¾›äº†ä¸»è¦çš„ä½ç§©ç»“æ„ï¼ˆå‰ r ä¸ªå¥‡å¼‚å€¼ï¼‰
    2. æ·»åŠ é¢å¤–çš„å°å¥‡å¼‚å€¼ï¼ˆæ¨¡æ‹Ÿå…¨å‚æ•°çš„é¢å¤–è‡ªç”±åº¦ï¼‰
    3. æ·»åŠ é€‚é‡å™ªå£°ï¼ˆæ¨¡æ‹Ÿä¼˜åŒ–è¿‡ç¨‹çš„éšæœºæ€§ï¼‰

    Args:
        lora_delta: LoRA çš„ Î”W [out, in]
        lora_rank: LoRA çš„ rank
        target_rank: ç›®æ ‡ rankï¼ˆç”¨äºæ‰©å±•ï¼‰
        noise_scale: å™ªå£°å¼ºåº¦

    Returns:
        åˆæˆçš„å…¨å‚æ•° Î”W
    """
    d_out, d_in = lora_delta.shape

    # å¯¹ LoRA delta åš SVD
    U, S, Vh = torch.linalg.svd(lora_delta, full_matrices=False)
    # U: [out, min(out,in)], S: [min(out,in)], Vh: [min(out,in), in]

    k = min(d_out, d_in)
    actual_rank = min(target_rank, k)

    # ä¿ç•™ LoRA çš„ä¸»è¦æˆåˆ†
    U_main = U[:, :lora_rank]
    S_main = S[:lora_rank]
    Vh_main = Vh[:lora_rank, :]

    # å¦‚æœ target_rank > lora_rankï¼Œæ·»åŠ é¢å¤–çš„å°å¥‡å¼‚å€¼æˆåˆ†
    if actual_rank > lora_rank:
        # é¢å¤–çš„å¥‡å¼‚å€¼ï¼šç”¨æŒ‡æ•°è¡°å‡ç”Ÿæˆ
        # ä» S[lora_rank-1] å¼€å§‹ç»§ç»­è¡°å‡
        last_sv = S_main[-1].item()
        extra_count = actual_rank - lora_rank

        # ç”ŸæˆæŒ‡æ•°è¡°å‡çš„å¥‡å¼‚å€¼
        decay_rate = 1.5
        extra_svs = torch.tensor([
            last_sv * np.exp(-decay_rate * i) for i in range(1, extra_count + 1)
        ], dtype=S.dtype)

        # ç”Ÿæˆéšæœºçš„ U å’Œ Vh æˆåˆ†ï¼ˆæ­£äº¤åŒ–ï¼‰
        extra_U = torch.randn(d_out, extra_count, dtype=U.dtype)
        extra_U, _ = torch.linalg.qr(extra_U)

        extra_Vh = torch.randn(extra_count, d_in, dtype=Vh.dtype)
        # å¯¹ Vh çš„è¡Œè¿›è¡Œæ­£äº¤åŒ–
        extra_Vh_T, _ = torch.linalg.qr(extra_Vh.T)
        extra_Vh = extra_Vh_T.T

        # åˆå¹¶
        U_full = torch.cat([U_main, extra_U], dim=1)
        S_full = torch.cat([S_main, extra_svs])
        Vh_full = torch.cat([Vh_main, extra_Vh], dim=0)
    else:
        U_full = U_main
        S_full = S_main
        Vh_full = Vh_main

    # é‡æ„ Î”W
    delta_synth = U_full @ torch.diag(S_full) @ Vh_full

    # æ·»åŠ å°å™ªå£°ï¼ˆæ¨¡æ‹Ÿä¼˜åŒ–çš„éšæœºæ€§ï¼‰
    noise = torch.randn_like(delta_synth) * noise_scale * S_full[0].item()
    delta_synth = delta_synth + noise

    return delta_synth


def synthesize_teacher_deltas(
    lora_deltas: Dict[str, torch.Tensor],
    lora_rank: int,
    target_rank: int = 64,
    noise_scale: float = 0.1,
    output_dir: str = None
) -> Dict[str, torch.Tensor]:
    """
    ä¸ºæ‰€æœ‰å±‚åˆæˆå…¨å‚æ•° Î”W

    Args:
        lora_deltas: LoRA çš„ Î”W å­—å…¸
        lora_rank: LoRA rank
        target_rank: ç›®æ ‡ rank
        noise_scale: å™ªå£°å¼ºåº¦
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰

    Returns:
        åˆæˆçš„å…¨å‚æ•° Î”W å­—å…¸
    """
    print(f"\n{'='*70}")
    print(f"ğŸ”¬ Synthesizing Full-param Î”W (target_rank={target_rank})")
    print(f"{'='*70}\n")

    teacher_deltas = {}
    stats = []

    for layer_name, lora_delta in tqdm(lora_deltas.items(), desc="Synthesizing"):
        # åˆæˆå…¨å‚æ•° delta
        teacher_delta = synthesize_fullparam_delta(
            lora_delta,
            lora_rank=lora_rank,
            target_rank=target_rank,
            noise_scale=noise_scale
        )

        teacher_deltas[layer_name] = teacher_delta

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        lora_norm = torch.norm(lora_delta).item()
        teacher_norm = torch.norm(teacher_delta).item()
        relative_diff = torch.norm(teacher_delta - lora_delta).item() / lora_norm if lora_norm > 0 else 0

        stats.append({
            'layer': layer_name,
            'shape': list(teacher_delta.shape),
            'lora_norm': lora_norm,
            'teacher_norm': teacher_norm,
            'relative_diff': relative_diff
        })

    # æ‰“å°ç»Ÿè®¡
    print(f"\nâœ“ Synthesized {len(teacher_deltas)} layers")
    print(f"\nSample statistics:")
    for stat in stats[:5]:
        print(f"  {stat['layer'][:50]}...")
        print(f"    LoRA norm: {stat['lora_norm']:.4f}")
        print(f"    Teacher norm: {stat['teacher_norm']:.4f}")
        print(f"    Relative diff: {stat['relative_diff']:.2%}")

    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        stats_path = output_dir / "synthesis_stats.json"
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        print(f"\nâœ“ Statistics saved to: {stats_path}")

    return teacher_deltas


def save_teacher_deltas(
    teacher_deltas: Dict[str, torch.Tensor],
    output_path: str
):
    """ä¿å­˜åˆæˆçš„ Teacher Î”W"""
    print(f"\nğŸ’¾ Saving synthesized Teacher Î”W to: {output_path}")
    torch.save(teacher_deltas, output_path)
    print("âœ“ Saved")


def generate_svd_factors(
    teacher_deltas: Dict[str, torch.Tensor],
    rank: int,
    output_dir: str
):
    """
    å¯¹åˆæˆçš„ Teacher Î”W åš SVDï¼Œç”Ÿæˆ LoRA åˆå§‹åŒ–å› å­

    Args:
        teacher_deltas: Teacher Î”W å­—å…¸
        rank: SVD æˆªæ–­ rank
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"\n{'='*70}")
    print(f"ğŸ“ Computing SVD (rank={rank})")
    print(f"{'='*70}\n")

    svd_factors = {}
    analysis_data = {
        'rank': rank,
        'layers': {}
    }

    for layer_name, delta in tqdm(teacher_deltas.items(), desc="Computing SVD"):
        # SVD åˆ†è§£
        U, S, Vh = torch.linalg.svd(delta, full_matrices=False)

        # æˆªæ–­åˆ° rank r
        U_r = U[:, :rank]
        S_r = S[:rank]
        Vh_r = Vh[:rank, :]

        # æ„é€  LoRA çš„ B, A
        B = U_r @ torch.diag(S_r)  # [out, r]
        A = Vh_r                    # [r, in]

        svd_factors[layer_name] = {
            'B': B.cpu(),
            'A': A.cpu()
        }

        # è®¡ç®—è¯¯å·®
        delta_r = B @ A
        rel_error = torch.norm(delta - delta_r).item() / torch.norm(delta).item()
        energy_ratio = (S_r ** 2).sum().item() / (S ** 2).sum().item()

        analysis_data['layers'][layer_name] = {
            'shape': list(delta.shape),
            'rel_error': rel_error,
            'energy_ratio': energy_ratio,
            'singular_values': S.tolist()[:50]
        }

    # ä¿å­˜ SVD factors
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    factors_path = output_dir / f"svd_factors_rank{rank}.pth"
    torch.save(svd_factors, factors_path)
    print(f"\nâœ“ SVD factors saved to: {factors_path}")

    # ä¿å­˜åˆ†ææ•°æ®
    analysis_path = output_dir / f"svd_analysis_rank{rank}.json"
    with open(analysis_path, 'w') as f:
        json.dump(analysis_data, f, indent=2)
    print(f"âœ“ Analysis data saved to: {analysis_path}")

    # ç”Ÿæˆç®€è¦æŠ¥å‘Š
    errors = [data['rel_error'] for data in analysis_data['layers'].values()]
    energy_ratios = [data['energy_ratio'] for data in analysis_data['layers'].values()]

    print(f"\n{'='*70}")
    print("SVD Analysis Summary")
    print(f"{'='*70}")
    print(f"Rank: {rank}")
    print(f"Layers analyzed: {len(analysis_data['layers'])}")
    print(f"\nReconstruction Error:")
    print(f"  Mean: {np.mean(errors):.4%}")
    print(f"  Std:  {np.std(errors):.4%}")
    print(f"  Min:  {np.min(errors):.4%}")
    print(f"  Max:  {np.max(errors):.4%}")
    print(f"\nEnergy Ratio:")
    print(f"  Mean: {np.mean(energy_ratios):.2%}")
    print(f"  Min:  {np.min(energy_ratios):.2%}")
    print(f"  Max:  {np.max(energy_ratios):.2%}")
    print(f"{'='*70}")


def main():
    parser = argparse.ArgumentParser(
        description="Synthesize Teacher Î”W from trained LoRA model"
    )

    parser.add_argument(
        "--base-model",
        type=str,
        required=True,
        help="Base model path"
    )

    parser.add_argument(
        "--lora-adapter",
        type=str,
        required=True,
        help="Trained LoRA adapter path (e.g., final_model_random/)"
    )

    parser.add_argument(
        "--lora-rank",
        type=int,
        required=True,
        help="LoRA rank used in training"
    )

    parser.add_argument(
        "--target-rank",
        type=int,
        default=64,
        help="Target rank for synthesized full-param Î”W (default: 64)"
    )

    parser.add_argument(
        "--svd-rank",
        type=int,
        default=16,
        help="SVD truncation rank for LoRA initialization (default: 16)"
    )

    parser.add_argument(
        "--noise-scale",
        type=float,
        default=0.1,
        help="Noise scale for synthesis (default: 0.1)"
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="./experiments/svd_lora/synthesized_teacher",
        help="Output directory"
    )

    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        help="Device (cpu/cuda)"
    )

    args = parser.parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç›®æ ‡æ¨¡å—
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                     "gate_proj", "up_proj", "down_proj"]

    # Step 1: åŠ è½½ LoRA æ¨¡å‹
    lora_model = load_lora_model(
        args.base_model,
        args.lora_adapter,
        device=args.device
    )

    # Step 2: æå– LoRA Î”W
    lora_deltas = extract_lora_deltas(lora_model, target_modules)

    # Step 3: åˆæˆå…¨å‚æ•° Î”W
    teacher_deltas = synthesize_teacher_deltas(
        lora_deltas,
        lora_rank=args.lora_rank,
        target_rank=args.target_rank,
        noise_scale=args.noise_scale,
        output_dir=output_dir
    )

    # Step 4: ä¿å­˜ Teacher Î”W
    teacher_path = output_dir / "teacher_deltas.pth"
    save_teacher_deltas(teacher_deltas, teacher_path)

    # Step 5: ç”Ÿæˆ SVD factors
    generate_svd_factors(
        teacher_deltas,
        rank=args.svd_rank,
        output_dir=output_dir
    )

    print(f"\n{'='*70}")
    print("âœ… Synthesis Complete!")
    print(f"{'='*70}")
    print(f"\nOutput files:")
    print(f"  - teacher_deltas.pth (synthesized Teacher Î”W)")
    print(f"  - svd_factors_rank{args.svd_rank}.pth (for LoRA init)")
    print(f"  - svd_analysis_rank{args.svd_rank}.json (analysis data)")
    print(f"  - synthesis_stats.json (synthesis statistics)")
    print(f"\nNext step:")
    print(f"  Use the SVD factors to train SVD-init LoRA:")
    print(f"  python experiments/svd_lora/train_lora_svd_vs_rand.py \\")
    print(f"    --init svd \\")
    print(f"    --svd-factors {output_dir}/svd_factors_rank{args.svd_rank}.pth \\")
    print(f"    ...")


if __name__ == "__main__":
    main()
