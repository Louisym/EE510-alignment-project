"""
Model Evaluation and Comparison Script
è¯„ä¼°å’Œå¯¹æ¯” Base/SFT/GRPO æ¨¡å‹çš„æ€§èƒ½

ç”¨äºç”Ÿæˆ Presentation å’Œ Report çš„å¯¹æ¯”æ•°æ®
"""

import os
import sys
import json
import torch
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from training.visualization import ModelComparator


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(
        self,
        base_model_name: str = "deepseek-ai/deepseek-math-7b-instruct",
        sft_model_path: str = None,
        grpo_model_path: str = None,
        device: str = "auto"
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            base_model_name: åŸºç¡€æ¨¡å‹åç§°
            sft_model_path: SFT æ¨¡å‹è·¯å¾„
            grpo_model_path: GRPO æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡
        """
        self.base_model_name = base_model_name
        self.sft_model_path = sft_model_path
        self.grpo_model_path = grpo_model_path
        self.device = device

        self.tokenizer = None
        self.base_model = None
        self.sft_model = None
        self.grpo_model = None

        print("ğŸš€ Initializing Model Evaluator...")

    def load_models(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        print("\n" + "="*70)
        print("ğŸ“¦ Loading Models...")
        print("="*70)

        # åŠ è½½ tokenizer
        print(f"\n1. Loading tokenizer from {self.base_model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_name,
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        print("âœ“ Tokenizer loaded")

        # åŠ è½½åŸºç¡€æ¨¡å‹
        print(f"\n2. Loading base model: {self.base_model_name}...")
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            device_map=self.device,
            torch_dtype=torch.float16,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        self.base_model.eval()
        print("âœ“ Base model loaded")

        # åŠ è½½ SFT æ¨¡å‹
        if self.sft_model_path and os.path.exists(self.sft_model_path):
            print(f"\n3. Loading SFT model from {self.sft_model_path}...")
            self.sft_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                device_map=self.device,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            self.sft_model = PeftModel.from_pretrained(
                self.sft_model,
                self.sft_model_path
            )
            self.sft_model.eval()
            print("âœ“ SFT model loaded")
        else:
            print(f"\n3. âš  SFT model not found at {self.sft_model_path}, skipping")

        # åŠ è½½ GRPO æ¨¡å‹
        if self.grpo_model_path and os.path.exists(self.grpo_model_path):
            print(f"\n4. Loading GRPO model from {self.grpo_model_path}...")
            self.grpo_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                device_map=self.device,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            self.grpo_model = PeftModel.from_pretrained(
                self.grpo_model,
                self.grpo_model_path
            )
            self.grpo_model.eval()
            print("âœ“ GRPO model loaded")
        else:
            print(f"\n4. âš  GRPO model not found at {self.grpo_model_path}, skipping")

        print("\nâœ“ All available models loaded successfully!\n")

    @torch.no_grad()
    def generate_response(
        self,
        model,
        question: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9
    ) -> str:
        """
        ç”Ÿæˆæ¨¡å‹å›ç­”

        Args:
            model: æ¨¡å‹
            question: é—®é¢˜
            max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦
            top_p: top-p é‡‡æ ·

        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        # æ„å»º prompt
        prompt = f"You are a mathematics expert specializing in probability theory. Please answer the following question accurately and clearly.\n\nQuestion: {question}\n\nAnswer:"

        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt")
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        # ç”Ÿæˆ
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id
        )

        # è§£ç 
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–ç­”æ¡ˆéƒ¨åˆ†
        if "Answer:" in full_response:
            answer = full_response.split("Answer:")[-1].strip()
        else:
            answer = full_response[len(prompt):].strip()

        return answer

    def evaluate_on_questions(
        self,
        questions: List[str],
        output_dir: str = "./evaluation_results"
    ) -> Dict:
        """
        åœ¨é—®é¢˜åˆ—è¡¨ä¸Šè¯„ä¼°æ‰€æœ‰æ¨¡å‹

        Args:
            questions: é—®é¢˜åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            è¯„ä¼°ç»“æœ
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        print("\n" + "="*70)
        print("ğŸ”¬ Evaluating Models on Test Questions...")
        print("="*70)
        print(f"Total questions: {len(questions)}\n")

        results = {
            'questions': questions,
            'base_outputs': [],
            'sft_outputs': [],
            'grpo_outputs': []
        }

        # åˆ›å»ºå¯¹æ¯”å™¨
        comparator = ModelComparator(output_dir)

        # å¯¹æ¯ä¸ªé—®é¢˜ç”Ÿæˆå›ç­”
        for i, question in enumerate(tqdm(questions, desc="Generating responses")):
            print(f"\n{'='*70}")
            print(f"Question {i+1}/{len(questions)}:")
            print(f"{question[:100]}..." if len(question) > 100 else question)
            print('='*70)

            # Base æ¨¡å‹
            print("\nğŸ“Œ Base Model:")
            base_output = self.generate_response(self.base_model, question)
            results['base_outputs'].append(base_output)
            print(f"{base_output[:200]}..." if len(base_output) > 200 else base_output)

            # SFT æ¨¡å‹
            sft_output = ""
            if self.sft_model:
                print("\nğŸ“Œ SFT Model:")
                sft_output = self.generate_response(self.sft_model, question)
                results['sft_outputs'].append(sft_output)
                print(f"{sft_output[:200]}..." if len(sft_output) > 200 else sft_output)
            else:
                results['sft_outputs'].append("N/A")

            # GRPO æ¨¡å‹
            grpo_output = ""
            if self.grpo_model:
                print("\nğŸ“Œ GRPO Model:")
                grpo_output = self.generate_response(self.grpo_model, question)
                results['grpo_outputs'].append(grpo_output)
                print(f"{grpo_output[:200]}..." if len(grpo_output) > 200 else grpo_output)
            else:
                results['grpo_outputs'].append("N/A")

            # æ·»åŠ åˆ°å¯¹æ¯”å™¨
            comparator.add_comparison(question, base_output, sft_output, grpo_output)

        # ä¿å­˜ç»“æœ
        print("\n" + "="*70)
        print("ğŸ’¾ Saving Results...")
        print("="*70)

        results_path = output_dir / "evaluation_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"âœ“ Results saved to {results_path}")

        # ä¿å­˜å¯¹æ¯”è¡¨æ ¼
        comparator.save_comparison_table()

        return results

    def compute_metrics(self, results: Dict, references: List[str] = None) -> Dict:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡

        Args:
            results: è¯„ä¼°ç»“æœ
            references: å‚è€ƒç­”æ¡ˆï¼ˆå¯é€‰ï¼‰

        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        metrics = {
            'base': {},
            'sft': {},
            'grpo': {}
        }

        # è®¡ç®—å¹³å‡ç­”æ¡ˆé•¿åº¦
        for model_name in ['base', 'sft', 'grpo']:
            outputs_key = f"{model_name}_outputs"
            if outputs_key in results:
                outputs = [o for o in results[outputs_key] if o and o != "N/A"]
                if outputs:
                    lengths = [len(o) for o in outputs]
                    metrics[model_name]['avg_length'] = np.mean(lengths)
                    metrics[model_name]['num_responses'] = len(outputs)

        # å¦‚æœæœ‰å‚è€ƒç­”æ¡ˆï¼Œå¯ä»¥è®¡ç®—æ›´å¤šæŒ‡æ ‡ï¼ˆå¦‚ ROUGE, BLEU ç­‰ï¼‰
        # è¿™é‡Œæš‚æ—¶åªè®¡ç®—åŸºæœ¬ç»Ÿè®¡

        return metrics


def main():
    parser = argparse.ArgumentParser(description="Evaluate and compare models")
    parser.add_argument("--base-model", type=str,
                       default="deepseek-ai/deepseek-math-7b-instruct",
                       help="Base model name")
    parser.add_argument("--sft-model", type=str,
                       default="outputs/sft/final_model",
                       help="SFT model path")
    parser.add_argument("--grpo-model", type=str,
                       default="outputs/grpo/final_model",
                       help="GRPO model path")
    parser.add_argument("--test-data", type=str,
                       default="data/training_data/train_flattened.json",
                       help="Test data path")
    parser.add_argument("--num-samples", type=int, default=5,
                       help="Number of test samples to evaluate")
    parser.add_argument("--output-dir", type=str,
                       default="evaluation_results",
                       help="Output directory")

    args = parser.parse_args()

    # åŠ è½½æµ‹è¯•æ•°æ®
    print(f"ğŸ“‚ Loading test data from {args.test_data}...")
    with open(args.test_data, 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    # éšæœºé€‰æ‹©æµ‹è¯•æ ·æœ¬
    np.random.seed(42)
    if len(test_data) > args.num_samples:
        indices = np.random.choice(len(test_data), args.num_samples, replace=False)
        test_samples = [test_data[i] for i in indices]
    else:
        test_samples = test_data

    questions = [sample['question'] for sample in test_samples]
    references = [sample['answer'] for sample in test_samples]

    print(f"âœ“ Loaded {len(questions)} test questions\n")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(
        base_model_name=args.base_model,
        sft_model_path=args.sft_model,
        grpo_model_path=args.grpo_model
    )

    # åŠ è½½æ¨¡å‹
    evaluator.load_models()

    # è¯„ä¼°
    results = evaluator.evaluate_on_questions(questions, args.output_dir)

    # è®¡ç®—æŒ‡æ ‡
    print("\n" + "="*70)
    print("ğŸ“Š Computing Metrics...")
    print("="*70)
    metrics = evaluator.compute_metrics(results, references)

    # æ‰“å°æŒ‡æ ‡
    for model_name, model_metrics in metrics.items():
        if model_metrics:
            print(f"\n{model_name.upper()} Model:")
            for metric_name, value in model_metrics.items():
                print(f"  {metric_name}: {value}")

    # ä¿å­˜æŒ‡æ ‡
    metrics_path = Path(args.output_dir) / "metrics.json"
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"\nâœ“ Metrics saved to {metrics_path}")

    # ç”Ÿæˆå¯¹æ¯”å›¾
    comparator = ModelComparator(args.output_dir)
    comparator.plot_comparison_metrics(metrics, save=True, show=False)

    print("\n" + "="*70)
    print("âœ… Evaluation Complete!")
    print("="*70)
    print(f"ğŸ“ Results saved to: {args.output_dir}")
    print(f"   - evaluation_results.json: å®Œæ•´çš„æ¨¡å‹è¾“å‡º")
    print(f"   - model_comparison.csv: å¯¹æ¯”è¡¨æ ¼")
    print(f"   - model_comparison.md: Markdown æ ¼å¼å¯¹æ¯”")
    print(f"   - metrics.json: è¯„ä¼°æŒ‡æ ‡")
    print(f"   - metrics_comparison.png: æŒ‡æ ‡å¯¹æ¯”å›¾")


if __name__ == "__main__":
    main()
